{
    "docs": [
        {
            "location": "/aboutme/", 
            "text": "My current work involves big data analysis and large scale data science applications utilizing geospatial technology. I work with different imagery types and I am currently involved with looking at time series data \n analysis of natural systems coupled with system modeling and periodicity. Most of my work includes but is not limited to remote sensing applications, large scale data processing and management, API support along with network analysis and geostatistical methods.\n\n\n\n\nContact me:\nroysam[at]indiana.edu", 
            "title": "About me"
        }, 
        {
            "location": "/cv/", 
            "text": "Research Specialization and Interest\n\n\n\n\nRemote Sensing and GIS, Urban systems,patterns and hydrology,Land Change Science\n\n\n\n\nEducation\n\n\n\n\n\n\n\n\nDegree\n\n\nUniversity\n\n\nYear\n\n\nResearch Interest or Thesis\n\n\n\n\n\n\n\n\n\n\nPhD Candidate\n\n\nIndiana University\n\n\nExpected 2018\n\n\nLand cover change modeling and system dynamics using large scale spatio-temporal data analysis\n\n\n\n\n\n\nMS Earth Sciencs\n\n\nIndiana University\n\n\n2013\n\n\nThesis: Remote sensing \n GIS applications for drainage detection and Modeling in agricultural watersheds\n\n\n\n\n\n\nB.Tech\n\n\nVNIT\n\n\n2010\n\n\nThesis: Social Network Analysis for Mapping Segmented Growth in Urban Cities in India.\n\n\n\n\n\n\n\n\nTechnical\n\n\n\n\nIntermediate experience with Javascript, Python, Shell Scripts\n\n\nExtensive experience with Google Earth Engine, ESRI ArcGIS, ERDAS IMAGINE, ENVI.\n\n\nExperience with QGIS, eCognition, MATLAB, Fragstats, Patch Analyst, SPSS, QUAL2Kw, SocNetV, Expert Choice 2000, GRASS, ILWIS.\n\n\n\n\nEmployment\n\n\n\n\n2015-2016: Research Assistant, Indiana University funded by Deltas Belmont Forum and National Science Foundation(NSF)\n\n\n\n\n\n\nCatalyzing action towards sustainability of deltaic systems with an integrated modeling framework for risk assessment (DELTAS: 1342946). Partner-PI. National Science Foundation, Belmont Forum. Partner PI [International collaborative network of 24 institutions] (2015-2016) Responsible for looking at physical models of vulnerability and risk assessment in deltaic areas and among populated regions within Brazil. Includes model building and assessment along with hydrological model based vulnerability assessment of same area.\n\n\n\n\n\n\n2013-2014: Reseach Assistant , Indiana University funded by National Aeronautics and Space Administration(NASA)\n\n\n\n\n\n\nNational Aeronautics and Space Administration(NASA: NNX11AF50G) (01/01/2013\u201312/31/2014) Carbon Dynamics, Land Cover Change, and Vulnerability of the World's Largest Coastal Mangrove Ecosystem to Climate Change, with Dr. Rinku Roy Chowdhury. Responsibilities includes review of existing datasets and literature. Evaluation of existing LULC classes using Landsat ETM+, and Geoeye classified raster datasets. Assessment of existing census and socio ecological data.\n\n\n\n\n\n\n2013-2015: Research Assistant, Indiana University funded by National Science Foundation(NSF)\n\n\n\n\n\n\nNational Science Foundation (NSF: 1065785) (07/01/2013\u201306/30/2015).  Collaborative  Research: Ecological Homogenization of Urban America, with Dr. Rinku Roy Chowdhury Responsibilities include analysis of land cover data at census block group and parcel levels using landscape metrics and analysis of land cover heterogeneity or homogeneity at both levels. This includes developing batch models for running large datasets and creating consistent data and analytical output for all 6 urban sites in the project.\n\n\n\n\n\n\n2011-2012: Research Assistant, Indiana University Purdue University at Indinapolis funded by United States Department of Agriculture \n Natural Resource Conservation(USDA \n NRCS)\n\n\n\n\n\n\nUnited States Department of Agriculture \n Natural Resource Conservation (USDA \n NRCS: 68-52KY-11-058) project for Creating and Automating Potential Polygon location and Site Characterization for Upland Storage in Watersheds, using Arc Map. A CTI (Compound Topographic Index) based Toolbox was developed using Model Builder (\u00ae ESRI) as a part of project deliverables to automate location of these potential sites.\n(September 2011- August 2012)\n\n\n\n\n\n\nJanuary 2011-June 2011: Research Assistant, Indian Institute of Technology (IIT) funder by Ministry of Environment and Forests\n\n\n\n\n\n\nResearch Associate with Water Resource Management Group at IIT Kanpur, for the Ganga River Basin Management Plan (GRBMP), under Ministry of Environment and Forests Responsibilities: Included assessment of the hydrology of the Upper Ganga Basin and working on flow simulation and scenario development for flow calculation. Organization: Indian Institute of Technology (IIT), Kanpur, January to June 2011.\n\n\n\n\n\n\nJune 2010- August 2010: Research Assistant, Indiana University funded by National Science Foundation(NSF)\n\n\n\n\n\n\nResearch Intern at the Core Geospatial and Utilities(CGO) business Unit Responsibilities at Risk Management Solution in India (RMSI), Dehradun: Created a characteristic model called SMCPM (Scenario Modeling with Catchment Priority Model) which utilizes the SCN method accompanied with SMART and MAUT utilization for user based criterion input and priority output. 22nd June to 21st August 2010\n\\newpage\n\n\n\n\nPeer-Reviewed Journal Articles \n Products\n\n\n\n\n\n\nRoy, Samapriya., Robeson, Scott., Mansur, A. V., Brondizio, Eduardo., Spatial Clustering using Multiplex Geo-constrained Networks in Amazon River Delta Under Preparation\n\n\n\n\n\n\nRoy, Samapriya., Robeson, Scott., Ortiz, Alejandra., Edmonds, Douglas Edmonds \u201cDecadal Changes in Mississippi Delta Morphology: Analyzing Landscape Patterns using Satellite Time Series Data\u201d Under preparation\n\n\n\n\n\n\nOrtiz, A. C., S. Roy, and D. A. Edmonds (2017), Land loss by pond expansion on the Mississippi River Delta Plain, Geophys. Res. Lett., 44, doi:10.1002/2017GL073079.\n\n\n\n\n\n\nSamapriya Roy. (2017, August 16). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon. Zenodo. http://doi.org/10.5281/zenodo.844018\n\n\n\n\n\n\nSamapriya Roy. (2017, August 17). samapriya/Planet-Pipeline-GUI: Planet-Pipeline-GUI. Zenodo. http://doi.org/10.5281/zenodo.844149\n\n\n\n\n\n\nSamapriya Roy. (2017, August 12). samapriya/ArcticDEM-Batch-Pipeline: ArcticDEM-Batch-Pipeline. Zenodo. http://doi.org/10.5281/zenodo.842056\n\n\n\n\n\n\nSamapriya Roy. (2017). samapriya/Planet-Pipeline: Planet-Pipeline [Dataset]. Zenodo. http://doi.org/10.5281/zenodo.344532\n\n\n\n\n\n\nSamapriya Roy. (2017, June 19). samapriya/Planet-GEE-Pipeline-CLI: Planet-GEE-Pipeline-CLI. Zenodo. http://doi.org/10.5281/zenodo.810548\n\n\n\n\n\n\nSamapriya Roy. (2017, June 19). samapriya/gee_asset_manager_addon: GEE Asset Manager with Addons. Zenodo. http://doi.org/10.5281/zenodo.810566\n\n\n\n\n\n\nMansur A.V.,E.S.Brondizio,S. Roy, PPM Soares, Newton A Adapting to urban challenges in the Amazon: Flood risk and infrastructure deficiencies in Bel\u00e9m, Brazil Submitted to Regional Environmental Change 2016\n\n\n\n\n\n\nMansur, Andressa V., Eduardo S. Brond\u00edzio, Samapriya Roy, Scott Hetrick, Nathan D. Vogt, and Alice Newton. \"An assessment of urban vulnerability in the Amazon Delta and Estuary: a multi-criterion index of flood exposure, socio-economic conditions and infrastructure.\" Sustainability Science (2016): 1-19.\n\n\n\n\n\n\nRoy, Samapriya, and Katpatal,Y.B (2011) Cyclical Hierarchical Modeling for Water Quality Model based DSS Module in an urban river system, Journal of Environmental Engineering, ASCE. Vol. 137, Number 12, 1176-1184.\n\n\n\n\n\n\nConference presentations, Trainings and Attendance\n\n\n\n\n\n\nPolar Geospatial BootCamp 2017: Focus on looking at high resolution satellite data along with using stereoscopic method to create high resolution digital elevation model. August 7-10th 2017, Minneapolis, Minnesota\n\n\n\n\n\n\nGoogle Earth Engine Summit 2017: Focus on methodology and large scale data analysis. Mountain View, California. June 12-14th 2017.\n\n\n\n\n\n\nEdmonds,Douglas A., Caldwell,Rebecca L., Baumgardner,Sarah, Paola,Chris, Roy ,Samapriya, Nelson,Amelia: A global analysis of human habitation on river deltas. European Geosciences Union General Assembly, Vienna, 23-28th April 2017\n\n\n\n\n\n\nRoy, Samapriya., Edmonds, Douglas., Visualizing land loss across high spatio-temporal scales across Louisiana Coast, Planet Labs Headquarters December 2016.\n\n\n\n\n\n\nDeltas Belmont Forum End of Project Meeting August 2016: Large Scale data analysis and visualization application to specific delta cases along with urban land cover change.\n\n\n\n\n\n\nGoogle Earth Engine Summit 2016: Focus on methodology and large scale data analysis. Mountain View, California. June 13-16th 2016.\n\n\n\n\n\n\nCommunity Surface Dynamics Modeling System (CSDMS) Annual Meeting, A Composite Vulnerability for Urban Areas in Deltaic Regions: An application in the Amazon Delta, Colorado. 17-19th May 2016.\n\n\n\n\n\n\nIndiana Geographic Information Council Conference 2016.  Locally \n Globally Applied Classification Algorithms for Urban Land Cover Detection using Earth Engine. May 9-11th May 2016.\n\n\n\n\n\n\nSummer Training at the United States Army Core of Engineers (USACE) and South Florida Water Management District (SFWMD) joint Modeling Group (2015): South Florida Water Management Model, West Palm Beach, 26th July to 31st July 2015.\n\n\n\n\n\n\nRoy, Samapriya (2015) Hydrological Modeling for studying impacts of Urbanization: A cross site assessment at Community Surface Dynamics Modeling System (CSDMS) Annual Meeting, Colorado. 26-28nd May 2015\n\n\n\n\n\n\nRoy, Samapriya et.al (2015) Urban Watersheds and urbanizing hydrology: Assessment through dynamic modeling, Oral presentation at the Association of American Geographers(AAG) Annual Meeting, April 21-April 25th, 2015\n\n\n\n\n\n\nRoy, Samapriya, Roy Chowdhury, Rinku and Ficklin, Darren (2015) Dynamic Hydrological Modeling using SWAT within Florida Coastal Everglades. Florida Coastal Everglades LTER Mid Term Review Meeting, March 11-13, 2015.\n\n\n\n\n\n\nRoy, Samapriya, Darren L. Ficklin, Rinku Roy Chowdhury, Scott Robeson, Jarlath O Neil Dunn, James B Heffernan, Meredith K Steele, Peter M Groffman (2014) Dynamic Assessment of Urban Hydrologic Component using SWAT. International Long Term Ecological Research (ILTER): All Scientist Meeting of the Americas, Valdivia, Chile, December 1-3 , 2014\n\n\n\n\n\n\nField Research Experience\n\n\n\n\nSummer 2014 (05/15/2014-06/18/2014). Field research in Bangladesh: Conducted 140 household surveys, with data collection aimed at assessing socio-economic-political drivers of land use and livelihood vulnerability in 4 villages of the Sundarbans mangrove forest region.\n\n\n\n\nGrants and Fellowship\n\n\n\n\nCollege of Arts and Sciences Graduate Student Travel Award for attending Google Earth Engine Summit 2017 $200 April 2017.\n\n\nAwarded the Graduate \n Professional Student Government Travel Award for attending Google Earth Engine Summit 2017 $500 April 2017.\n\n\nAwarded the John Odland Graduate Research Fellowship to support graduate research $500 March 2017\n\n\nAwarded the Lester Spicer Department of Geography Poster Award Fellowship to support graduate research $250 March 2017\n\n\nAwarded the William R. Black Leadership Memorial Fellowship for $500 March 2017\n\n\nCo-PI on Extreme Science and Engineering Discovery Environment (XSEDE) Allocation Grant along with Douglas Edmonds for 50,000 Super-computing Units and 500 GB storage volume for project \"Automated API based Pipeline for high volume satellite data\", grant number TG-GEO160014 at Indiana University.\n\n\nCo PI on the Planet Labs Ambassador program providing us unprecedented daily satellite data for entire Louisiana coast an estimated value of over 40,000 square kilometer and estimated access to over 500,000 square kilometer daily (approximately) maximum access value considering RapidEye is (approx. 500,000 x $1.28/sqkm = $640,000) \n\n\nDigital Globe Imagery Grant for an area of over 1500 square kilometer (estimated value at $25,500) April 2016.\n\n\nAwarded the John Odland Graduate Research Fellowship to support graduate research $500 March 2016.\n\n\nAwarded the IndianaView Student Scholarship Program to participate and conduct research $750 March 2016\n\n\nAwarded the Partners of America (POA) 100,000 Strong award, for $1000 towards travel to the ILTER All Scientist Meeting of the Americas, Valdivia, Chile December 2014\n\n\nAwarded the ILTER Travel Award, for $500 towards travel to the ILTER All Scientist Meeting of the Americas, Valdivia, Chile December 2014\n\n\n\n\nTeaching Experience\n\n\n\n\nLead Instructor, G338: Introduction to Geographic Information Systems, Summer 2017, Indiana University, Bloomington\n\n\nLead  Instructor,  G237: Mapping  our  World, Spring  2017, Indiana University, Bloomington\n\n\nLead  Instructor,  G237: Mapping  our  World, Fall  2016, Indiana University, Bloomington\n\n\nTeaching Assistant, G237: Mapping  our  World, Spring 2016, Indiana University, Bloomington\n\n\nLead  Instructor,  G237: Mapping  our  World, Fall 2015, Indiana University, Bloomington\n\n\nGuest Lecture I202 Lecture Topic: Spatial Epidemiology September 25th 2014 at Indiana University, Bloomington\n\n\nTeaching Assistant: Environmental Geology Course, G117 Spring 2013, Indiana University Purdue University, Indianapolis\n\n\nTeaching Assistant: Environmental Geology Course, G117 Fall 2012, Indiana University Purdue University, Indianapolis\n\n\nTeaching Assistant: Environmental Geology Course, G117 Spring 2012, Indiana University Purdue University, Indianapolis\n\n\n\n\nMemberships and committees\n\n\n\n\nCo-Chair for GIS Day Day at Bloomington, Indiana University, 2016\n\n\nCollege Committee on Graduate Education, Graduate Student Representative (2015-2016)\n\n\nPlanning Committee for GIS Day at Bloomington, Indiana University 2015.\n\n\nStudent Member, American Society of Civil Engineers (ASCE)    2008-present\n\n\nEnvironmental and Water Resources Institute (EWRI)    2010-present\n\n\nStudent Member, American Association of Geographer (AAG)  2014-present\n\n\n\n\nCertifications and Trainings\n\n\n\n\nCollaborative Institutional Training Initiative (CITI) Human Research 2014\n\n\nTrimble Geospatial Training: eCognition- analysis strategies August 14th- 15th, 2014\n\n\n\n\nReferences\n\n\nAvailable on request.", 
            "title": "Current CV"
        }, 
        {
            "location": "/cv/#research-specialization-and-interest", 
            "text": "Remote Sensing and GIS, Urban systems,patterns and hydrology,Land Change Science", 
            "title": "Research Specialization and Interest"
        }, 
        {
            "location": "/cv/#education", 
            "text": "Degree  University  Year  Research Interest or Thesis      PhD Candidate  Indiana University  Expected 2018  Land cover change modeling and system dynamics using large scale spatio-temporal data analysis    MS Earth Sciencs  Indiana University  2013  Thesis: Remote sensing   GIS applications for drainage detection and Modeling in agricultural watersheds    B.Tech  VNIT  2010  Thesis: Social Network Analysis for Mapping Segmented Growth in Urban Cities in India.", 
            "title": "Education"
        }, 
        {
            "location": "/cv/#technical", 
            "text": "Intermediate experience with Javascript, Python, Shell Scripts  Extensive experience with Google Earth Engine, ESRI ArcGIS, ERDAS IMAGINE, ENVI.  Experience with QGIS, eCognition, MATLAB, Fragstats, Patch Analyst, SPSS, QUAL2Kw, SocNetV, Expert Choice 2000, GRASS, ILWIS.", 
            "title": "Technical"
        }, 
        {
            "location": "/cv/#employment", 
            "text": "2015-2016: Research Assistant, Indiana University funded by Deltas Belmont Forum and National Science Foundation(NSF)    Catalyzing action towards sustainability of deltaic systems with an integrated modeling framework for risk assessment (DELTAS: 1342946). Partner-PI. National Science Foundation, Belmont Forum. Partner PI [International collaborative network of 24 institutions] (2015-2016) Responsible for looking at physical models of vulnerability and risk assessment in deltaic areas and among populated regions within Brazil. Includes model building and assessment along with hydrological model based vulnerability assessment of same area.    2013-2014: Reseach Assistant , Indiana University funded by National Aeronautics and Space Administration(NASA)    National Aeronautics and Space Administration(NASA: NNX11AF50G) (01/01/2013\u201312/31/2014) Carbon Dynamics, Land Cover Change, and Vulnerability of the World's Largest Coastal Mangrove Ecosystem to Climate Change, with Dr. Rinku Roy Chowdhury. Responsibilities includes review of existing datasets and literature. Evaluation of existing LULC classes using Landsat ETM+, and Geoeye classified raster datasets. Assessment of existing census and socio ecological data.    2013-2015: Research Assistant, Indiana University funded by National Science Foundation(NSF)    National Science Foundation (NSF: 1065785) (07/01/2013\u201306/30/2015).  Collaborative  Research: Ecological Homogenization of Urban America, with Dr. Rinku Roy Chowdhury Responsibilities include analysis of land cover data at census block group and parcel levels using landscape metrics and analysis of land cover heterogeneity or homogeneity at both levels. This includes developing batch models for running large datasets and creating consistent data and analytical output for all 6 urban sites in the project.    2011-2012: Research Assistant, Indiana University Purdue University at Indinapolis funded by United States Department of Agriculture   Natural Resource Conservation(USDA   NRCS)    United States Department of Agriculture   Natural Resource Conservation (USDA   NRCS: 68-52KY-11-058) project for Creating and Automating Potential Polygon location and Site Characterization for Upland Storage in Watersheds, using Arc Map. A CTI (Compound Topographic Index) based Toolbox was developed using Model Builder (\u00ae ESRI) as a part of project deliverables to automate location of these potential sites.\n(September 2011- August 2012)    January 2011-June 2011: Research Assistant, Indian Institute of Technology (IIT) funder by Ministry of Environment and Forests    Research Associate with Water Resource Management Group at IIT Kanpur, for the Ganga River Basin Management Plan (GRBMP), under Ministry of Environment and Forests Responsibilities: Included assessment of the hydrology of the Upper Ganga Basin and working on flow simulation and scenario development for flow calculation. Organization: Indian Institute of Technology (IIT), Kanpur, January to June 2011.    June 2010- August 2010: Research Assistant, Indiana University funded by National Science Foundation(NSF)    Research Intern at the Core Geospatial and Utilities(CGO) business Unit Responsibilities at Risk Management Solution in India (RMSI), Dehradun: Created a characteristic model called SMCPM (Scenario Modeling with Catchment Priority Model) which utilizes the SCN method accompanied with SMART and MAUT utilization for user based criterion input and priority output. 22nd June to 21st August 2010\n\\newpage", 
            "title": "Employment"
        }, 
        {
            "location": "/cv/#peer-reviewed-journal-articles-products", 
            "text": "Roy, Samapriya., Robeson, Scott., Mansur, A. V., Brondizio, Eduardo., Spatial Clustering using Multiplex Geo-constrained Networks in Amazon River Delta Under Preparation    Roy, Samapriya., Robeson, Scott., Ortiz, Alejandra., Edmonds, Douglas Edmonds \u201cDecadal Changes in Mississippi Delta Morphology: Analyzing Landscape Patterns using Satellite Time Series Data\u201d Under preparation    Ortiz, A. C., S. Roy, and D. A. Edmonds (2017), Land loss by pond expansion on the Mississippi River Delta Plain, Geophys. Res. Lett., 44, doi:10.1002/2017GL073079.    Samapriya Roy. (2017, August 16). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon. Zenodo. http://doi.org/10.5281/zenodo.844018    Samapriya Roy. (2017, August 17). samapriya/Planet-Pipeline-GUI: Planet-Pipeline-GUI. Zenodo. http://doi.org/10.5281/zenodo.844149    Samapriya Roy. (2017, August 12). samapriya/ArcticDEM-Batch-Pipeline: ArcticDEM-Batch-Pipeline. Zenodo. http://doi.org/10.5281/zenodo.842056    Samapriya Roy. (2017). samapriya/Planet-Pipeline: Planet-Pipeline [Dataset]. Zenodo. http://doi.org/10.5281/zenodo.344532    Samapriya Roy. (2017, June 19). samapriya/Planet-GEE-Pipeline-CLI: Planet-GEE-Pipeline-CLI. Zenodo. http://doi.org/10.5281/zenodo.810548    Samapriya Roy. (2017, June 19). samapriya/gee_asset_manager_addon: GEE Asset Manager with Addons. Zenodo. http://doi.org/10.5281/zenodo.810566    Mansur A.V.,E.S.Brondizio,S. Roy, PPM Soares, Newton A Adapting to urban challenges in the Amazon: Flood risk and infrastructure deficiencies in Bel\u00e9m, Brazil Submitted to Regional Environmental Change 2016    Mansur, Andressa V., Eduardo S. Brond\u00edzio, Samapriya Roy, Scott Hetrick, Nathan D. Vogt, and Alice Newton. \"An assessment of urban vulnerability in the Amazon Delta and Estuary: a multi-criterion index of flood exposure, socio-economic conditions and infrastructure.\" Sustainability Science (2016): 1-19.    Roy, Samapriya, and Katpatal,Y.B (2011) Cyclical Hierarchical Modeling for Water Quality Model based DSS Module in an urban river system, Journal of Environmental Engineering, ASCE. Vol. 137, Number 12, 1176-1184.", 
            "title": "Peer-Reviewed Journal Articles &amp; Products"
        }, 
        {
            "location": "/cv/#conference-presentations-trainings-and-attendance", 
            "text": "Polar Geospatial BootCamp 2017: Focus on looking at high resolution satellite data along with using stereoscopic method to create high resolution digital elevation model. August 7-10th 2017, Minneapolis, Minnesota    Google Earth Engine Summit 2017: Focus on methodology and large scale data analysis. Mountain View, California. June 12-14th 2017.    Edmonds,Douglas A., Caldwell,Rebecca L., Baumgardner,Sarah, Paola,Chris, Roy ,Samapriya, Nelson,Amelia: A global analysis of human habitation on river deltas. European Geosciences Union General Assembly, Vienna, 23-28th April 2017    Roy, Samapriya., Edmonds, Douglas., Visualizing land loss across high spatio-temporal scales across Louisiana Coast, Planet Labs Headquarters December 2016.    Deltas Belmont Forum End of Project Meeting August 2016: Large Scale data analysis and visualization application to specific delta cases along with urban land cover change.    Google Earth Engine Summit 2016: Focus on methodology and large scale data analysis. Mountain View, California. June 13-16th 2016.    Community Surface Dynamics Modeling System (CSDMS) Annual Meeting, A Composite Vulnerability for Urban Areas in Deltaic Regions: An application in the Amazon Delta, Colorado. 17-19th May 2016.    Indiana Geographic Information Council Conference 2016.  Locally   Globally Applied Classification Algorithms for Urban Land Cover Detection using Earth Engine. May 9-11th May 2016.    Summer Training at the United States Army Core of Engineers (USACE) and South Florida Water Management District (SFWMD) joint Modeling Group (2015): South Florida Water Management Model, West Palm Beach, 26th July to 31st July 2015.    Roy, Samapriya (2015) Hydrological Modeling for studying impacts of Urbanization: A cross site assessment at Community Surface Dynamics Modeling System (CSDMS) Annual Meeting, Colorado. 26-28nd May 2015    Roy, Samapriya et.al (2015) Urban Watersheds and urbanizing hydrology: Assessment through dynamic modeling, Oral presentation at the Association of American Geographers(AAG) Annual Meeting, April 21-April 25th, 2015    Roy, Samapriya, Roy Chowdhury, Rinku and Ficklin, Darren (2015) Dynamic Hydrological Modeling using SWAT within Florida Coastal Everglades. Florida Coastal Everglades LTER Mid Term Review Meeting, March 11-13, 2015.    Roy, Samapriya, Darren L. Ficklin, Rinku Roy Chowdhury, Scott Robeson, Jarlath O Neil Dunn, James B Heffernan, Meredith K Steele, Peter M Groffman (2014) Dynamic Assessment of Urban Hydrologic Component using SWAT. International Long Term Ecological Research (ILTER): All Scientist Meeting of the Americas, Valdivia, Chile, December 1-3 , 2014", 
            "title": "Conference presentations, Trainings and Attendance"
        }, 
        {
            "location": "/cv/#field-research-experience", 
            "text": "Summer 2014 (05/15/2014-06/18/2014). Field research in Bangladesh: Conducted 140 household surveys, with data collection aimed at assessing socio-economic-political drivers of land use and livelihood vulnerability in 4 villages of the Sundarbans mangrove forest region.", 
            "title": "Field Research Experience"
        }, 
        {
            "location": "/cv/#grants-and-fellowship", 
            "text": "College of Arts and Sciences Graduate Student Travel Award for attending Google Earth Engine Summit 2017 $200 April 2017.  Awarded the Graduate   Professional Student Government Travel Award for attending Google Earth Engine Summit 2017 $500 April 2017.  Awarded the John Odland Graduate Research Fellowship to support graduate research $500 March 2017  Awarded the Lester Spicer Department of Geography Poster Award Fellowship to support graduate research $250 March 2017  Awarded the William R. Black Leadership Memorial Fellowship for $500 March 2017  Co-PI on Extreme Science and Engineering Discovery Environment (XSEDE) Allocation Grant along with Douglas Edmonds for 50,000 Super-computing Units and 500 GB storage volume for project \"Automated API based Pipeline for high volume satellite data\", grant number TG-GEO160014 at Indiana University.  Co PI on the Planet Labs Ambassador program providing us unprecedented daily satellite data for entire Louisiana coast an estimated value of over 40,000 square kilometer and estimated access to over 500,000 square kilometer daily (approximately) maximum access value considering RapidEye is (approx. 500,000 x $1.28/sqkm = $640,000)   Digital Globe Imagery Grant for an area of over 1500 square kilometer (estimated value at $25,500) April 2016.  Awarded the John Odland Graduate Research Fellowship to support graduate research $500 March 2016.  Awarded the IndianaView Student Scholarship Program to participate and conduct research $750 March 2016  Awarded the Partners of America (POA) 100,000 Strong award, for $1000 towards travel to the ILTER All Scientist Meeting of the Americas, Valdivia, Chile December 2014  Awarded the ILTER Travel Award, for $500 towards travel to the ILTER All Scientist Meeting of the Americas, Valdivia, Chile December 2014", 
            "title": "Grants and Fellowship"
        }, 
        {
            "location": "/cv/#teaching-experience", 
            "text": "Lead Instructor, G338: Introduction to Geographic Information Systems, Summer 2017, Indiana University, Bloomington  Lead  Instructor,  G237: Mapping  our  World, Spring  2017, Indiana University, Bloomington  Lead  Instructor,  G237: Mapping  our  World, Fall  2016, Indiana University, Bloomington  Teaching Assistant, G237: Mapping  our  World, Spring 2016, Indiana University, Bloomington  Lead  Instructor,  G237: Mapping  our  World, Fall 2015, Indiana University, Bloomington  Guest Lecture I202 Lecture Topic: Spatial Epidemiology September 25th 2014 at Indiana University, Bloomington  Teaching Assistant: Environmental Geology Course, G117 Spring 2013, Indiana University Purdue University, Indianapolis  Teaching Assistant: Environmental Geology Course, G117 Fall 2012, Indiana University Purdue University, Indianapolis  Teaching Assistant: Environmental Geology Course, G117 Spring 2012, Indiana University Purdue University, Indianapolis", 
            "title": "Teaching Experience"
        }, 
        {
            "location": "/cv/#memberships-and-committees", 
            "text": "Co-Chair for GIS Day Day at Bloomington, Indiana University, 2016  College Committee on Graduate Education, Graduate Student Representative (2015-2016)  Planning Committee for GIS Day at Bloomington, Indiana University 2015.  Student Member, American Society of Civil Engineers (ASCE)    2008-present  Environmental and Water Resources Institute (EWRI)    2010-present  Student Member, American Association of Geographer (AAG)  2014-present", 
            "title": "Memberships and committees"
        }, 
        {
            "location": "/cv/#certifications-and-trainings", 
            "text": "Collaborative Institutional Training Initiative (CITI) Human Research 2014  Trimble Geospatial Training: eCognition- analysis strategies August 14th- 15th, 2014", 
            "title": "Certifications and Trainings"
        }, 
        {
            "location": "/cv/#references", 
            "text": "Available on request.", 
            "title": "References"
        }, 
        {
            "location": "/", 
            "text": "Introduction\n\n\nThese github projects are a collection of tools and improvements made to accomplish tasks as well as potential tools and applications designed for future applications. While moving between assets from Planet Inc and Google Earth Engine it was imperative to create a pipeline that allows for easy transitions between the two service end points and hence tools were designed to link these two nodes and create effective pipelines. Since this required me to interact with Earth Engine I further developed addon tools the ambition of which is to helping users with batch actions on assets along with interacting and extending capabilities of existing GEE CLI.\n\n\nTwo projects included looking at different aspects of raster analysis and landscape modeling and hence I decided to create an ArcMap toolbox with tools that I have developed and continue to develop further. The toolbox can be downloaded and added to existing toolbox to create additional functionalities. The synthetic landscape models are designed to try different randomizing and clustering algorithms. An additional project was completed during the Polar Geospatial Bootcamp which allows users to bulk download and process 2m high resolution ArcticDEM provided by the Polar Geospatial Center.\n\n\n\n\n\n\n\n\nProject Name\n\n\nDigital Object Identification Number(DOI)\n\n\n\n\n\n\n\n\n\n\nPlanet-GEE-Pipeline-CLI\n\n\n\n\n\n\n\n\nPlanet-GEE-Pipeline-GUI\n\n\n\n\n\n\n\n\nPlanet-Pipeline-GUI\n\n\n\n\n\n\n\n\nGEE Asset Manager Addons\n\n\n\n\n\n\n\n\nSynthetic LandScape Models\n\n\n\n\n\n\n\n\nArcMap Addons\n\n\n\n\n\n\n\n\nArcticDEM-Batch-Pipeline\n\n\n\n\n\n\n\n\njetstream-unofficial-addon", 
            "title": "Introduction"
        }, 
        {
            "location": "/#introduction", 
            "text": "These github projects are a collection of tools and improvements made to accomplish tasks as well as potential tools and applications designed for future applications. While moving between assets from Planet Inc and Google Earth Engine it was imperative to create a pipeline that allows for easy transitions between the two service end points and hence tools were designed to link these two nodes and create effective pipelines. Since this required me to interact with Earth Engine I further developed addon tools the ambition of which is to helping users with batch actions on assets along with interacting and extending capabilities of existing GEE CLI.  Two projects included looking at different aspects of raster analysis and landscape modeling and hence I decided to create an ArcMap toolbox with tools that I have developed and continue to develop further. The toolbox can be downloaded and added to existing toolbox to create additional functionalities. The synthetic landscape models are designed to try different randomizing and clustering algorithms. An additional project was completed during the Polar Geospatial Bootcamp which allows users to bulk download and process 2m high resolution ArcticDEM provided by the Polar Geospatial Center.     Project Name  Digital Object Identification Number(DOI)      Planet-GEE-Pipeline-CLI     Planet-GEE-Pipeline-GUI     Planet-Pipeline-GUI     GEE Asset Manager Addons     Synthetic LandScape Models     ArcMap Addons     ArcticDEM-Batch-Pipeline     jetstream-unofficial-addon", 
            "title": "Introduction"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/", 
            "text": "Planet Pipeline GUI\n\n\nThe Planet Pipeline GUI came from the actual CLI (command line interface tools) to enable the use of tools required to access control and download planet labs assets as well as parse metadata in a tabular form which maybe required by other applications.\n\n\n\n\n\n\nTable of contents\n\n\n\n\nInstallation\n\n\nGetting started\n\n\nPlanet Key\n\n\nAOI JSON\n\n\nActivate or Check Asset\n\n\nDownload Size\n\n\nDownload Asset\n\n\nMetadata Parser\n\n\n\n\n\n\nCredits\n\n\n\n\nInstallation\n\n\nWe assume Planet Python API is installed you can install by simply running \n\n\npip install planet\n\n\n\n\nFurther instructions can be found \nhere\n \n\n\nTo install the tool:\n\n\ngit clone https://github.com/samapriya/Planet-Pipeline-GUI.git\ncd Planet-Pipeline-GUI \n pip install .\n\n\n\n\nThe application can be also run directly by executing PlanetPipe_GUI.pyc script. \nYou require two important packages for this to run\n\n\nWxPython(which is what the GUI is built on)\nfor windows(Tested in Windows 10)\nhttps://wxpython.org/download.php\n\nfor linux(Tested in Ubuntu 16)\nsudo add-apt-repository \ndeb http://archive.ubuntu.com/ubuntu utopic main restricted universe\n  \nsudo apt-get update\napt-cache search python-wxgtk3.0\nsudo apt-get install python-wxgtk3.0\n\n\n\n\nGetting started\n\n\nThis should be pretty simple on windows systems with python \n=2.7.13 you can just double click the PlanetPipe_GUI.pyc for linux user python PlanetPipe_GUI.pyc\n\n\nPlanet Key\n\n\nThis tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools\n\n\n\n\nIf using on a private machine the Key is saved as a csv file for all future runs of the tool.\n\n\nAOI JSON\n\n\nThe aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.\n\n\n\n\nActivate or Check Asset\n\n\nThe activatepl tab allows the users to either check or activate planet assets. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier\n\n\n\n\nDownload Size\n\n\nThe space tool allows the user to check the size of the activated assets before downloading while also estimating the available space on the local drive. The overall size and space is printed in MB and GB\n\n\n\n\nDownload Asset\n\n\nThe downloadpl tab allows the users to download assets that have been activated. \n\n\n\n\nCredits\n\n\nJetStream\n A portion of the work is suported by JetStream Grant TG-GEO160014.\n\n\nAlso supported by \nPlanet Labs Ambassador Program", 
            "title": "Planet Bulk Data Downloader GUI"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#planet-pipeline-gui", 
            "text": "The Planet Pipeline GUI came from the actual CLI (command line interface tools) to enable the use of tools required to access control and download planet labs assets as well as parse metadata in a tabular form which maybe required by other applications.", 
            "title": "Planet Pipeline GUI"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#table-of-contents", 
            "text": "Installation  Getting started  Planet Key  AOI JSON  Activate or Check Asset  Download Size  Download Asset  Metadata Parser    Credits", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#installation", 
            "text": "We assume Planet Python API is installed you can install by simply running   pip install planet  Further instructions can be found  here    To install the tool:  git clone https://github.com/samapriya/Planet-Pipeline-GUI.git\ncd Planet-Pipeline-GUI   pip install .  The application can be also run directly by executing PlanetPipe_GUI.pyc script. \nYou require two important packages for this to run  WxPython(which is what the GUI is built on)\nfor windows(Tested in Windows 10)\nhttps://wxpython.org/download.php\n\nfor linux(Tested in Ubuntu 16)\nsudo add-apt-repository  deb http://archive.ubuntu.com/ubuntu utopic main restricted universe   \nsudo apt-get update\napt-cache search python-wxgtk3.0\nsudo apt-get install python-wxgtk3.0", 
            "title": "Installation"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#getting-started", 
            "text": "This should be pretty simple on windows systems with python  =2.7.13 you can just double click the PlanetPipe_GUI.pyc for linux user python PlanetPipe_GUI.pyc", 
            "title": "Getting started"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#planet-key", 
            "text": "This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools   If using on a private machine the Key is saved as a csv file for all future runs of the tool.", 
            "title": "Planet Key"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#aoi-json", 
            "text": "The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.", 
            "title": "AOI JSON"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#activate-or-check-asset", 
            "text": "The activatepl tab allows the users to either check or activate planet assets. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier", 
            "title": "Activate or Check Asset"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#download-size", 
            "text": "The space tool allows the user to check the size of the activated assets before downloading while also estimating the available space on the local drive. The overall size and space is printed in MB and GB", 
            "title": "Download Size"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#download-asset", 
            "text": "The downloadpl tab allows the users to download assets that have been activated.", 
            "title": "Download Asset"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#credits", 
            "text": "JetStream  A portion of the work is suported by JetStream Grant TG-GEO160014.  Also supported by  Planet Labs Ambassador Program", 
            "title": "Credits"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/", 
            "text": "Google Earth Engine Batch Asset Manager with Addons\n\n\n\n\nGoogle Earth Engine Batch Asset Manager with Addons is an extension of the one developed by Lukasz \nhere\n and additional tools were added to include functionality for moving assets, conversion of objects to fusion table, cleaning folders, querying tasks. The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. tab.\n\n\n\n\nTable of contents\n\n\n\n\nInstallation\n\n\nGetting started\n\n\nBatch uploader\n\n\nParsing metadata\n\n\n\n\n\n\nUsage examples\n\n\nEE User\n\n\nUpload a directory with images and associate properties with each image:\n\n\nUpload a directory with images with specific NoData value to a selected destination:\n\n\nAsset List\n\n\nTask Query\n\n\nTask Query during ingestion\n\n\nTask Report\n\n\nDelete a collection with content:\n\n\nAssets Move\n\n\nAssets Copy\n\n\nAssets Access\n\n\nSet Collection Property\n\n\nConvert to Fusion Table\n\n\nCleanup Utility\n\n\nCancel all tasks\n\n\n\n\n\n\n\n\nInstallation\n\n\nWe assume Earth Engine Python API is installed and EE authorised as desribed \nhere\n. This toolbox also uses some functionality from GDAL\nFor installing GDAL in Ubuntu\n\n\nsudo add-apt-repository ppa:ubuntugis/ppa \n sudo apt-get update\nsudo apt-get install gdal-bin\n\n\n\n\nFor Windows I found this \nguide\n from UCLA\n\n\nTo install toolbox:\n\n\ngit clone https://github.com/samapriya/gee_asset_manager_addon\ncd gee_asset_manager \n pip install .\n\n\n\n\nInstallation is an optional step; the application can be also run\ndirectly by executing geeadd.py script. The advantage of having it\ninstalled is being able to execute geeadd as any command line tool. I\nrecommend installation within virtual environment. To install run\n\n\npython setup.py develop or python setup.py install\n\nIn a linux distribution\nsudo python setup.py develop or sudo python setup.py install\n\n\n\n\nGetting started\n\n\nAs usual, to print help:\n\n\nGoogle Earth Engine Batch Asset Manager with Addons\n\npositional arguments:\n  {ee_user,upload,lst,tasks,taskquery,report,delete,mover,copy,access,collprop,convert2ft,cleanout,cancel}\n    ee_user             Allows you to associate/change GEE account to system\n    upload              Batch Asset Uploader.\n    lst                 List assets in a folder/collection or write as text\n                        file\n    tasks               Queries currently running, enqued,failed\n    taskquery           Queries currently running, enqued,failed ingestions\n                        and uploaded assets\n    report              Create a report of all tasks and exports to a CSV file\n    delete              Deletes collection and all items inside. Supports\n                        Unix-like wildcards.\n    mover               Moves all assets from one collection to another\n    copy                Copies all assets from one collection to another:\n                        Including copying from other users if you have read\n                        permission to their assets\n    access              Sets Permissions for Images, Collection or all assets\n                        in EE Folder Example: python ee_permissions.py --mode\n                        \nfolder\n --asset \nusers/john/doe\n --user\n                        \njimmy@doe.com:R\n\n    collprop            Sets Overall Properties for Image Collection\n    convert2ft          Uploads a given feature collection to Google Fusion\n                        Table.\n    cleanout            Clear folders with datasets from earlier downloaded\n    cancel              Cancel all running tasks\n\noptional arguments:\n  -h, --help            show this help message and exit\n\n\n\n\nTo obtain help for a specific functionality, simply call it with \nhelp\n\nswitch, e.g.: \ngeeadd upload -h\n. If you didn't install geeadd, then you\ncan run it just by going to \ngeeadd\n directory and running \npython\ngeeadd.py [arguments go here]\n\n\nBatch uploader\n\n\nThe script creates an Image Collection from GeoTIFFs in your local\ndirectory. By default, the collection name is the same as the local\ndirectory name; with optional parameter you can provide a different\nname. Another optional parameter is a path to a CSV file with metadata\nfor images, which is covered in the next section:\n\nParsing metadata\n.\n\n\nusage: geeadd.py upload [-h] -u USER --source SOURCE --dest DEST [-m METADATA]\n                        [--large] [--nodata NODATA]\n\noptional arguments:\n  -h, --help            show this help message and exit\n\nRequired named arguments.:\n  -u USER, --user USER  Google account name (gmail address).\n  --source SOURCE       Path to the directory with images for upload.\n  --dest DEST           Destination. Full path for upload to Google Earth\n                        Engine, e.g. users/johndoe/myponycollection\n\nOptional named arguments:\n  -m METADATA, --metadata METADATA\n                        Path to CSV with metadata.\n  --large               (Advanced) Use multipart upload. Might help if upload\n                        of large files is failing on some systems. Might cause\n                        other issues.\n  --nodata NODATA       The value to burn into the raster as NoData (missing\n                        data)\n\n\n\n\nParsing metadata\n\n\nBy metadata we understand here the properties associated with each image. Thanks to these, GEE user can easily filter collection based on specified criteria. The file with metadata should be organised as follows:\n\n\n\n\n\n\n\n\nfilename (without extension)\n\n\nproperty1 header\n\n\nproperty2 header\n\n\n\n\n\n\n\n\n\n\nfile1\n\n\nvalue1\n\n\nvalue2\n\n\n\n\n\n\nfile2\n\n\nvalue3\n\n\nvalue4\n\n\n\n\n\n\n\n\nNote that header can contain only letters, digits and underscores. \n\n\nExample:\n\n\n\n\n\n\n\n\nid_no\n\n\nclass\n\n\ncategory\n\n\nbinomial\n\n\nsystem:time_start\n\n\n\n\n\n\n\n\n\n\nmy_file_1\n\n\nGASTROPODA\n\n\nEN\n\n\nAaadonta constricta\n\n\n1478943081000\n\n\n\n\n\n\nmy_file_2\n\n\nGASTROPODA\n\n\nCR\n\n\nAaadonta irregularis\n\n\n1478943081000\n\n\n\n\n\n\n\n\nThe corresponding files are my_file_1.tif and my_file_2.tif. With each of the files five properties are associated: id_no, class, category, binomial and system:time_start. The latter is time in Unix epoch format, in milliseconds, as documented in GEE glosary. The program will match the file names from the upload directory with ones provided in the CSV and pass the metadata in JSON format:\n\n\n{ id_no: my_file_1, class: GASTROPODA, category: EN, binomial: Aaadonta constricta, system:time_start: 1478943081000}\n\n\n\n\nThe program will report any illegal fields, it will also complain if not all of the images passed for upload have metadata associated. User can opt to ignore it, in which case some assets will have no properties.\n\n\nHaving metadata helps in organising your asstets, but is not mandatory - you can skip it.\n\n\nUsage examples\n\n\nEE User\n\n\nThis tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back\n\n\nUpload a directory with images to your myfolder/mycollection and associate properties with each image:\n\n\ngeeadd upload -u johndoe@gmail.com --source path_to_directory_with_tif -m path_to_metadata.csv --dest users/johndoe/myfolder/myponycollection\n\n\n\n\nThe script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.\n\n\nUpload a directory with images with specific NoData value to a selected destination\n\n\ngeeadd upload -u johndoe@gmail.com --source path_to_directory_with_tif --dest users/johndoe/myfolder/myponycollection --nodata 222\n\n\n\n\nIn this case we need to supply full path to the destination, which is helpful when we upload to a shared folder. In the provided example we also burn value 222 into all rasters for missing data (NoData).\n\n\nAsset List\n\n\nThis tool is designed to either print or output asset lists within folders or collections using earthengine ls tool functions.\n\n\nusage: geeadd lst [-h] --location LOCATION --type TYPE [--items ITEMS]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --location LOCATION  This it the location of your folder/collection\n  --type TYPE          Whether you want the list to be printed or output as\n                       text(print/report)\n  --items ITEMS        Number of items to list\n\n\n\n\nTask Query\n\n\nThis script counts all currently running and ready tasks along with failed tasks.\n\n\nusage: geeadd.py tasks [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\ngeeadd.py tasks\n\n\n\n\nTask Query during ingestion\n\n\nThis script can be used intermittently to look at running, failed and ready(waiting) tasks during ingestion. This script is a special case using query tasks only when uploading assets to collection by providing collection pathway to see how collection size increases.\n\n\nusage: geeadd.py taskquery [-h] [--destination DESTINATION]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --destination DESTINATION\n                        Full path to asset where you are uploading files\n\ngeeadd.py taskquery \nusers/johndoe/myfolder/myponycollection\n                       \n\n\n\n\nTask Report\n\n\nSometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type\n\n\nusage: geeadd.py report [-h] [--r R] [--e E]\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --r R       Path \n CSV filename where the report will be saved\n  --e E       Path \n CSV filename where the errorlog will be saved\n\ngeeadd.py report --r \nreport.csv\n --e \nerrorlog.csv\n\n\n\n\n\nDelete a collection with content:\n\n\nThe delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution!\n\n\ngeeadd delete users/johndoe/test\n\n\n\n\nConsole output:\n\n\n2016-07-17 16:14:09,212 :: oauth2client.client :: INFO :: Attempting refresh to obtain initial access_token\n2016-07-17 16:14:09,213 :: oauth2client.client :: INFO :: Refreshing access_token\n2016-07-17 16:14:10,842 :: root :: INFO :: Attempting to delete collection test\n2016-07-17 16:14:16,898 :: root :: INFO :: Collection users/johndoe/test removed\n\n\n\n\nDelete all directories / collections based on a Unix-like pattern\n\n\ngeeadd delete users/johndoe/*weird[0-9]?name*\n\n\n\n\nAssets Move\n\n\nThis script allows us to recursively move assets from one collection to the other.\n\n\nusage: geeadd.py mover [-h] [--assetpath ASSETPATH] [--finalpath FINALPATH]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --assetpath ASSETPATH\n                        Existing path of assets\n  --finalpath FINALPATH\n                        New path for assets\ngeeadd.py mover --assetpath \nusers/johndoe/myfolder/myponycollection\n --destination \nusers/johndoe/myfolder/myotherponycollection\n                  \n\n\n\n\nAssets Copy\n\n\nThis script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections.\n\n\nusage: geeadd.py copy [-h] [--initial INITIAL] [--final FINAL]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --initial INITIAL  Existing path of assets\n  --final FINAL      New path for assets\ngeeadd.py mover --initial \nusers/johndoe/myfolder/myponycollection\n --final \nusers/johndoe/myfolder/myotherponycollection\n                  \n\n\n\n\nAssets Access\n\n\nThis tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time.\n\n\nusage: geeadd access [-h] --mode MODE --asset ASSET --user USER\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --mode MODE    This lets you select if you want to change permission or\n                 folder/collection/image\n  --asset ASSET  This is the path to the earth engine asset whose permission\n                 you are changing folder/collection/image\n  --user USER    This is the email address to whom you want to give read or\n                 write permission Usage: \njohn@doe.com:R\n or \njohn@doe.com:W\n\n                 R/W refers to read or write permission\ngeeadd.py access --mode folder --asset \nfolder/collection/image\n --user \njohn@doe.com:R\n\n\n\n\n\nSet Collection Property\n\n\nThis script is derived from the ee tool to set collection properties and will set overall properties for collection. \n\n\nusage: geeadd.py collprop [-h] [--coll COLL] [--p P]\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --coll COLL  Path of Image Collection\n  --p P        \nsystem:description=Description\n/\nsystem:provider_url=url\n/\nsys\n               tem:tags=tags\n/\nsystem:title=title\n\n\n\n\nConvert to Fusion Table\n\n\nOnce validated with gdal and google fusion table it can be used to convert any geoObject to google fusion table. Forked and contributed by Gennadii \nhere\n. The scripts can be used only with a specific google account\n\n\nusage: geeadd.py convert2ft [-h] --i I --o O [--add_missing]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --i I          input feature source (KML, SHP, SpatiLite, etc.)\n  --o O          output Fusion Table name\n  --add_missing  add missing features from the last inserted feature index\n\ngeeadd.py convert2ft --i \n./aoi.kml\n --o \nconverted_aoi\n\n\n\n\n\nCleanup Utility\n\n\nThis script is used to clean folders once all processes have been completed. In short this is a function to clear folder on local machine.\n\n\nusage: geeadd.py cleanout [-h] [--dirpath DIRPATH]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --dirpath DIRPATH  Folder you want to delete after all processes have been\n                     completed\ngeeadd.py cleanout --dirpath \n./folder\n\n\n\n\n\nCancel all tasks\n\n\nThis is a simpler tool, can be called directly from the earthengine cli as well\n\n\nearthengine cli command\nearthengine task cancel all\n\nusage: geeadd.py cancel [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit", 
            "title": "Google Earth Engine Asset Manager Addon"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#google-earth-engine-batch-asset-manager-with-addons", 
            "text": "Google Earth Engine Batch Asset Manager with Addons is an extension of the one developed by Lukasz  here  and additional tools were added to include functionality for moving assets, conversion of objects to fusion table, cleaning folders, querying tasks. The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. tab.", 
            "title": "Google Earth Engine Batch Asset Manager with Addons"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#table-of-contents", 
            "text": "Installation  Getting started  Batch uploader  Parsing metadata    Usage examples  EE User  Upload a directory with images and associate properties with each image:  Upload a directory with images with specific NoData value to a selected destination:  Asset List  Task Query  Task Query during ingestion  Task Report  Delete a collection with content:  Assets Move  Assets Copy  Assets Access  Set Collection Property  Convert to Fusion Table  Cleanup Utility  Cancel all tasks", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#installation", 
            "text": "We assume Earth Engine Python API is installed and EE authorised as desribed  here . This toolbox also uses some functionality from GDAL\nFor installing GDAL in Ubuntu  sudo add-apt-repository ppa:ubuntugis/ppa   sudo apt-get update\nsudo apt-get install gdal-bin  For Windows I found this  guide  from UCLA  To install toolbox:  git clone https://github.com/samapriya/gee_asset_manager_addon\ncd gee_asset_manager   pip install .  Installation is an optional step; the application can be also run\ndirectly by executing geeadd.py script. The advantage of having it\ninstalled is being able to execute geeadd as any command line tool. I\nrecommend installation within virtual environment. To install run  python setup.py develop or python setup.py install\n\nIn a linux distribution\nsudo python setup.py develop or sudo python setup.py install", 
            "title": "Installation"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#getting-started", 
            "text": "As usual, to print help:  Google Earth Engine Batch Asset Manager with Addons\n\npositional arguments:\n  {ee_user,upload,lst,tasks,taskquery,report,delete,mover,copy,access,collprop,convert2ft,cleanout,cancel}\n    ee_user             Allows you to associate/change GEE account to system\n    upload              Batch Asset Uploader.\n    lst                 List assets in a folder/collection or write as text\n                        file\n    tasks               Queries currently running, enqued,failed\n    taskquery           Queries currently running, enqued,failed ingestions\n                        and uploaded assets\n    report              Create a report of all tasks and exports to a CSV file\n    delete              Deletes collection and all items inside. Supports\n                        Unix-like wildcards.\n    mover               Moves all assets from one collection to another\n    copy                Copies all assets from one collection to another:\n                        Including copying from other users if you have read\n                        permission to their assets\n    access              Sets Permissions for Images, Collection or all assets\n                        in EE Folder Example: python ee_permissions.py --mode\n                         folder  --asset  users/john/doe  --user\n                         jimmy@doe.com:R \n    collprop            Sets Overall Properties for Image Collection\n    convert2ft          Uploads a given feature collection to Google Fusion\n                        Table.\n    cleanout            Clear folders with datasets from earlier downloaded\n    cancel              Cancel all running tasks\n\noptional arguments:\n  -h, --help            show this help message and exit  To obtain help for a specific functionality, simply call it with  help \nswitch, e.g.:  geeadd upload -h . If you didn't install geeadd, then you\ncan run it just by going to  geeadd  directory and running  python\ngeeadd.py [arguments go here]", 
            "title": "Getting started"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#batch-uploader", 
            "text": "The script creates an Image Collection from GeoTIFFs in your local\ndirectory. By default, the collection name is the same as the local\ndirectory name; with optional parameter you can provide a different\nname. Another optional parameter is a path to a CSV file with metadata\nfor images, which is covered in the next section: Parsing metadata .  usage: geeadd.py upload [-h] -u USER --source SOURCE --dest DEST [-m METADATA]\n                        [--large] [--nodata NODATA]\n\noptional arguments:\n  -h, --help            show this help message and exit\n\nRequired named arguments.:\n  -u USER, --user USER  Google account name (gmail address).\n  --source SOURCE       Path to the directory with images for upload.\n  --dest DEST           Destination. Full path for upload to Google Earth\n                        Engine, e.g. users/johndoe/myponycollection\n\nOptional named arguments:\n  -m METADATA, --metadata METADATA\n                        Path to CSV with metadata.\n  --large               (Advanced) Use multipart upload. Might help if upload\n                        of large files is failing on some systems. Might cause\n                        other issues.\n  --nodata NODATA       The value to burn into the raster as NoData (missing\n                        data)", 
            "title": "Batch uploader"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#parsing-metadata", 
            "text": "By metadata we understand here the properties associated with each image. Thanks to these, GEE user can easily filter collection based on specified criteria. The file with metadata should be organised as follows:     filename (without extension)  property1 header  property2 header      file1  value1  value2    file2  value3  value4     Note that header can contain only letters, digits and underscores.   Example:     id_no  class  category  binomial  system:time_start      my_file_1  GASTROPODA  EN  Aaadonta constricta  1478943081000    my_file_2  GASTROPODA  CR  Aaadonta irregularis  1478943081000     The corresponding files are my_file_1.tif and my_file_2.tif. With each of the files five properties are associated: id_no, class, category, binomial and system:time_start. The latter is time in Unix epoch format, in milliseconds, as documented in GEE glosary. The program will match the file names from the upload directory with ones provided in the CSV and pass the metadata in JSON format:  { id_no: my_file_1, class: GASTROPODA, category: EN, binomial: Aaadonta constricta, system:time_start: 1478943081000}  The program will report any illegal fields, it will also complain if not all of the images passed for upload have metadata associated. User can opt to ignore it, in which case some assets will have no properties.  Having metadata helps in organising your asstets, but is not mandatory - you can skip it.", 
            "title": "Parsing metadata"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#usage-examples", 
            "text": "", 
            "title": "Usage examples"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#ee-user", 
            "text": "This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back", 
            "title": "EE User"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#upload-a-directory-with-images-to-your-myfoldermycollection-and-associate-properties-with-each-image", 
            "text": "geeadd upload -u johndoe@gmail.com --source path_to_directory_with_tif -m path_to_metadata.csv --dest users/johndoe/myfolder/myponycollection  The script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.", 
            "title": "Upload a directory with images to your myfolder/mycollection and associate properties with each image:"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#upload-a-directory-with-images-with-specific-nodata-value-to-a-selected-destination", 
            "text": "geeadd upload -u johndoe@gmail.com --source path_to_directory_with_tif --dest users/johndoe/myfolder/myponycollection --nodata 222  In this case we need to supply full path to the destination, which is helpful when we upload to a shared folder. In the provided example we also burn value 222 into all rasters for missing data (NoData).", 
            "title": "Upload a directory with images with specific NoData value to a selected destination"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#asset-list", 
            "text": "This tool is designed to either print or output asset lists within folders or collections using earthengine ls tool functions.  usage: geeadd lst [-h] --location LOCATION --type TYPE [--items ITEMS]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --location LOCATION  This it the location of your folder/collection\n  --type TYPE          Whether you want the list to be printed or output as\n                       text(print/report)\n  --items ITEMS        Number of items to list", 
            "title": "Asset List"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#task-query", 
            "text": "This script counts all currently running and ready tasks along with failed tasks.  usage: geeadd.py tasks [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\ngeeadd.py tasks", 
            "title": "Task Query"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#task-query-during-ingestion", 
            "text": "This script can be used intermittently to look at running, failed and ready(waiting) tasks during ingestion. This script is a special case using query tasks only when uploading assets to collection by providing collection pathway to see how collection size increases.  usage: geeadd.py taskquery [-h] [--destination DESTINATION]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --destination DESTINATION\n                        Full path to asset where you are uploading files\n\ngeeadd.py taskquery  users/johndoe/myfolder/myponycollection", 
            "title": "Task Query during ingestion"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#task-report", 
            "text": "Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type  usage: geeadd.py report [-h] [--r R] [--e E]\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --r R       Path   CSV filename where the report will be saved\n  --e E       Path   CSV filename where the errorlog will be saved\n\ngeeadd.py report --r  report.csv  --e  errorlog.csv", 
            "title": "Task Report"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#delete-a-collection-with-content", 
            "text": "The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution!  geeadd delete users/johndoe/test  Console output:  2016-07-17 16:14:09,212 :: oauth2client.client :: INFO :: Attempting refresh to obtain initial access_token\n2016-07-17 16:14:09,213 :: oauth2client.client :: INFO :: Refreshing access_token\n2016-07-17 16:14:10,842 :: root :: INFO :: Attempting to delete collection test\n2016-07-17 16:14:16,898 :: root :: INFO :: Collection users/johndoe/test removed", 
            "title": "Delete a collection with content:"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#delete-all-directories-collections-based-on-a-unix-like-pattern", 
            "text": "geeadd delete users/johndoe/*weird[0-9]?name*", 
            "title": "Delete all directories / collections based on a Unix-like pattern"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#assets-move", 
            "text": "This script allows us to recursively move assets from one collection to the other.  usage: geeadd.py mover [-h] [--assetpath ASSETPATH] [--finalpath FINALPATH]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --assetpath ASSETPATH\n                        Existing path of assets\n  --finalpath FINALPATH\n                        New path for assets\ngeeadd.py mover --assetpath  users/johndoe/myfolder/myponycollection  --destination  users/johndoe/myfolder/myotherponycollection", 
            "title": "Assets Move"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#assets-copy", 
            "text": "This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections.  usage: geeadd.py copy [-h] [--initial INITIAL] [--final FINAL]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --initial INITIAL  Existing path of assets\n  --final FINAL      New path for assets\ngeeadd.py mover --initial  users/johndoe/myfolder/myponycollection  --final  users/johndoe/myfolder/myotherponycollection", 
            "title": "Assets Copy"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#assets-access", 
            "text": "This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time.  usage: geeadd access [-h] --mode MODE --asset ASSET --user USER\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --mode MODE    This lets you select if you want to change permission or\n                 folder/collection/image\n  --asset ASSET  This is the path to the earth engine asset whose permission\n                 you are changing folder/collection/image\n  --user USER    This is the email address to whom you want to give read or\n                 write permission Usage:  john@doe.com:R  or  john@doe.com:W \n                 R/W refers to read or write permission\ngeeadd.py access --mode folder --asset  folder/collection/image  --user  john@doe.com:R", 
            "title": "Assets Access"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#set-collection-property", 
            "text": "This script is derived from the ee tool to set collection properties and will set overall properties for collection.   usage: geeadd.py collprop [-h] [--coll COLL] [--p P]\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --coll COLL  Path of Image Collection\n  --p P         system:description=Description / system:provider_url=url / sys\n               tem:tags=tags / system:title=title", 
            "title": "Set Collection Property"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#convert-to-fusion-table", 
            "text": "Once validated with gdal and google fusion table it can be used to convert any geoObject to google fusion table. Forked and contributed by Gennadii  here . The scripts can be used only with a specific google account  usage: geeadd.py convert2ft [-h] --i I --o O [--add_missing]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --i I          input feature source (KML, SHP, SpatiLite, etc.)\n  --o O          output Fusion Table name\n  --add_missing  add missing features from the last inserted feature index\n\ngeeadd.py convert2ft --i  ./aoi.kml  --o  converted_aoi", 
            "title": "Convert to Fusion Table"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#cleanup-utility", 
            "text": "This script is used to clean folders once all processes have been completed. In short this is a function to clear folder on local machine.  usage: geeadd.py cleanout [-h] [--dirpath DIRPATH]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --dirpath DIRPATH  Folder you want to delete after all processes have been\n                     completed\ngeeadd.py cleanout --dirpath  ./folder", 
            "title": "Cleanup Utility"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#cancel-all-tasks", 
            "text": "This is a simpler tool, can be called directly from the earthengine cli as well  earthengine cli command\nearthengine task cancel all\n\nusage: geeadd.py cancel [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit", 
            "title": "Cancel all tasks"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/", 
            "text": "Planet GEE Pipeline CLI\n\n\n\n\n\n\n\n\nWhile moving between assets from Planet Inc and Google Earth Engine it was imperative to create a pipeline that allows for easy transitions between the two service end points and this tool is designed to act as a step by step process chain from Planet Assets to batch upload and modification within the Google Earth Engine environment. The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. tab.\n\n\n\n\nTable of contents\n\n\n\n\nInstallation\n\n\nGetting started\n\n\nBatch uploader\n\n\nParsing metadata\n\n\n\n\n\n\nUsage examples\n\n\nPlanet Tools\n\n\nPlanet Key\n\n\nAOI JSON\n\n\nActivate or Check Asset\n\n\nDownload Asset\n\n\nMetadata Parser\n\n\n\n\n\n\nEarth Engine Tools\n\n\nEE User\n\n\nUpload a directory with images and associate properties with each image:\n\n\nUpload a directory with images with specific NoData value to a selected destination:\n\n\nTask Query\n\n\nTask Query during ingestion\n\n\nTask Report\n\n\nDelete a collection with content:\n\n\nAssets Move\n\n\nAssets Copy\n\n\nAssets Access\n\n\nSet Collection Property\n\n\nConvert to Fusion Table\n\n\nCleanup Utility\n\n\nCancel all tasks\n\n\n\n\n\n\n\n\nInstallation\n\n\nWe assume Earth Engine Python API is installed and EE authorised as desribed \nhere\n. We also assume Planet Python API is installed you can install by simply running \n\n\npip install planet\n\n\n\n\nFurther instructions can be found \nhere\n \n\n\nThis toolbox also uses some functionality from GDAL\nFor installing GDAL in Ubuntu\n\n\nsudo add-apt-repository ppa:ubuntugis/ppa \n sudo apt-get update\nsudo apt-get install gdal-bin\n\n\n\n\nFor Windows I found this \nguide\n from UCLA\n\n\nTo install toolbox:\n\n\ngit clone https://github.com/samapriya/Planet-GEE-Pipeline-CLI.git\ncd Planet-GEE-Pipeline-CLI \n pip install .\n\n\n\n\nInstallation is an optional step; the application can be also run\ndirectly by executing geeadd.py script. The advantage of having it\ninstalled is being able to execute geeadd as any command line tool. I\nrecommend installation within virtual environment. To install run\n\n\npython setup.py develop or python setup.py install\n\nIn a linux distribution\nsudo python setup.py develop or sudo python setup.py install\n\n\n\n\nGetting started\n\n\nAs usual, to print help:\n\n\nPlanet Pipeline with Google Earth Engine Batch Addons\n\npositional arguments:\n  { ,planetkey,aoijson,activatepl,downloadpl,metadata,ee_user,upload,delete,tasks,taskquery,report,cancel,mover,copy,access,collprop,convert2ft,cleanout}\n                        ---------------------------------------\n                        -----Choose from Planet Tools Below-----\n                        ---------------------------------------\n    planetkey           Enter your planet API Key\n    aoijson             Tool to convert KML, Shapefile,WKT,GeoJSON or Landsat\n                        WRS PathRow file to AreaOfInterest.JSON file with\n                        structured query for use with Planet API 1.0\n    activatepl          Tool to query and/or activate Planet Assets\n    downloadpl          Tool to download Planet Assets\n    metadata            Tool to tabulate and convert all metadata files from\n                        Planet or Digital Globe Assets\n                        -------------------------------------------\n                        ----Choose from Earth Engine Tools Below----\n                        -------------------------------------------\n    ee_user             Get Earth Engine API Key \n Paste it back to Command\n                        line/shell to change user\n    upload              Batch Asset Uploader to Earth Engine.\n    delete              Deletes collection and all items inside. Supports\n                        Unix-like wildcards.\n    tasks               Queries currently running, enqued,failed\n    taskquery           Queries currently running, enqued,failed ingestions\n                        and uploaded assets\n    report              Create a report of all tasks and exports to a CSV file\n    cancel              Cancel all running tasks\n    mover               Moves all assets from one collection to another\n    copy                Copies all assets from one collection to another:\n                        Including copying from other users if you have read\n                        permission to their assets\n    access              Sets Permissions for Images, Collection or all assets\n                        in EE Folder Example: python ee_permissions.py --mode\n                        \nfolder\n --asset \nusers/john/doe\n --user\n                        \njimmy@doe.com:R\n\n    collprop            Sets Overall Properties for Image Collection\n    convert2ft          Uploads a given feature collection to Google Fusion\n                        Table.\n    cleanout            Clear folders with datasets from earlier downloaded\n\noptional arguments:\n  -h, --help            show this help message and exit\n\n\n\n\nTo obtain help for a specific functionality, simply call it with \nhelp\n\nswitch, e.g.: \ngeeadd upload -h\n. If you didn't install geeadd, then you\ncan run it just by going to \ngeeadd\n directory and running \npython\ngeeadd.py [arguments go here]\n\n\nBatch uploader\n\n\nThe script creates an Image Collection from GeoTIFFs in your local\ndirectory. By default, the collection name is the same as the local\ndirectory name; with optional parameter you can provide a different\nname. Another optional parameter is a path to a CSV file with metadata\nfor images, which is covered in the next section:\n\nParsing metadata\n.\n\n\nusage: geeadd.py upload [-h] -u USER --source SOURCE --dest DEST [-m METADATA]\n                        [--large] [--nodata NODATA]\n\noptional arguments:\n  -h, --help            show this help message and exit\n\nRequired named arguments.:\n  -u USER, --user USER  Google account name (gmail address).\n  --source SOURCE       Path to the directory with images for upload.\n  --dest DEST           Destination. Full path for upload to Google Earth\n                        Engine, e.g. users/johndoe/myponycollection\n\nOptional named arguments:\n  -m METADATA, --metadata METADATA\n                        Path to CSV with metadata.\n  --large               (Advanced) Use multipart upload. Might help if upload\n                        of large files is failing on some systems. Might cause\n                        other issues.\n  --nodata NODATA       The value to burn into the raster as NoData (missing\n                        data)\n\n\n\n\nParsing metadata\n\n\nBy metadata we understand here the properties associated with each image. Thanks to these, GEE user can easily filter collection based on specified criteria. The file with metadata should be organised as follows:\n\n\n\n\n\n\n\n\nfilename (without extension)\n\n\nproperty1 header\n\n\nproperty2 header\n\n\n\n\n\n\n\n\n\n\nfile1\n\n\nvalue1\n\n\nvalue2\n\n\n\n\n\n\nfile2\n\n\nvalue3\n\n\nvalue4\n\n\n\n\n\n\n\n\nNote that header can contain only letters, digits and underscores. \n\n\nExample:\n\n\n\n\n\n\n\n\nid_no\n\n\nclass\n\n\ncategory\n\n\nbinomial\n\n\nsystem:time_start\n\n\n\n\n\n\n\n\n\n\nmy_file_1\n\n\nGASTROPODA\n\n\nEN\n\n\nAaadonta constricta\n\n\n1478943081000\n\n\n\n\n\n\nmy_file_2\n\n\nGASTROPODA\n\n\nCR\n\n\nAaadonta irregularis\n\n\n1478943081000\n\n\n\n\n\n\n\n\nThe corresponding files are my_file_1.tif and my_file_2.tif. With each of the files five properties are associated: id_no, class, category, binomial and system:time_start. The latter is time in Unix epoch format, in milliseconds, as documented in GEE glosary. The program will match the file names from the upload directory with ones provided in the CSV and pass the metadata in JSON format:\n\n\n{ id_no: my_file_1, class: GASTROPODA, category: EN, binomial: Aaadonta constricta, system:time_start: 1478943081000}\n\n\n\n\nThe program will report any illegal fields, it will also complain if not all of the images passed for upload have metadata associated. User can opt to ignore it, in which case some assets will have no properties.\n\n\nHaving metadata helps in organising your asstets, but is not mandatory - you can skip it.\n\n\nUsage examples\n\n\nUsage examples have been segmented into two parts focusing on both planet tools as well as earth engine tools, earth engine tools include additional developments in CLI which allows you to recursively interact with their python API\n\n\nPlanet Tools\n\n\nThe Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.\n\n\nPlanet Key\n\n\nThis tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools\n\n\nusage: ppipe.py planetkey [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\n\n\n\nIf using on a private machine the Key is saved as a csv file for all future runs of the tool.\n\n\nAOI JSON\n\n\nThe aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.\n\n\nusage: ppipe.py aoijson [-h] [--start START] [--end END] [--cloud CLOUD]\n                        [--inputfile INPUTFILE] [--geo GEO]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --start START         Start date in YYYY-MM-DD?\n  --end END             End date in YYYY-MM-DD?\n  --cloud CLOUD         Maximum Cloud Cover(0-1) representing 0-100\n  --inputfile INPUTFILE\n                        Choose a kml/shapefile/geojson or WKT file for\n                        AOI(KML/SHP/GJSON/WKT) or WRS (6 digit RowPath\n                        Example: 023042)\n  --geo GEO             map.geojson/aoi.kml/aoi.shp/aoi.wkt file\n\n\n\n\nActivate or Check Asset\n\n\nThe activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier\n\n\nusage: ppipe.py activatepl [-h] [--aoi AOI] [--action ACTION] [--asst ASST]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --aoi AOI        Choose aoi.json file created earlier\n  --action ACTION  choose between check/activate\n  --asst ASST      Choose between planet asset types (PSOrthoTile\n                   analytic/REOrthoTile analytic/PSOrthoTile\n                   analytic_xml/REOrthoTile analytic_xml\n\n\n\n\n\nDownload Asset\n\n\nHaving metadata helps in organising your asstets, but is not mandatory - you can skip it.\nThe downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types.\n\n\nusage: ppipe.py downloadpl [-h] [--aoi AOI] [--action ACTION] [--asst ASST]\n                           [--pathway PATHWAY]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --aoi AOI          Choose aoi.json file created earlier\n  --action ACTION    choose download\n  --asst ASST        Choose between planet asset types (PSOrthoTile\n                     analytic/REOrthoTile analytic/PSOrthoTile\n                     analytic_xml/REOrthoTile analytic_xml\n  --pathway PATHWAY  Folder Pathways where PlanetAssets are saved exampled\n                     ./PlanetScope ./RapidEye\n\n\n\n\nMetadata Parser\n\n\nThe metadata tab is a more powerful tool and consists of metadata parsing for PlanetScope OrthoTile RapiEye OrthoTile along with Digital Globe MultiSpectral and DigitalGlobe PanChromatic datasets. This was developed as a standalone to process xml metadata files from multiple sources and is important step is the user plans to upload these assets to Google Earth Engine. The combine Planet-GEE Pipeline tool will be made available soon for testing.\n\n\nusage: ppipe.py metadata [-h] [--asset ASSET] [--mf MF] [--mfile MFILE]\n                         [--errorlog ERRORLOG]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --asset ASSET        Choose RapidEye/PlantScope/DigitalGlobe\n                       MultiSpectral/DigitalGlobe Panchromatic\n                       (RE/PS/DGMS/DGP)?\n  --mf MF              Metadata folder?\n  --mfile MFILE        Metadata filename to be exported along with Path.csv\n  --errorlog ERRORLOG  Errorlog to be exported along with Path.csv\n\n\n\n\nEarth Engine Tools\n\n\nThe ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. This is also a seperate package for earth engine users to use and can be downloaded \nhere\n\n\nEE User\n\n\nThis tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back\n\n\nUpload a directory with images to your myfolder/mycollection and associate properties with each image:\n\n\ngeeadd upload -u johndoe@gmail.com --source path_to_directory_with_tif -m path_to_metadata.csv --dest users/johndoe/myfolder/myponycollection\n\n\n\n\nThe script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.\n\n\nUpload a directory with images with specific NoData value to a selected destination\n\n\ngeeadd upload -u johndoe@gmail.com --source path_to_directory_with_tif --dest users/johndoe/myfolder/myponycollection --nodata 222\n\n\n\n\nIn this case we need to supply full path to the destination, which is helpful when we upload to a shared folder. In the provided example we also burn value 222 into all rasters for missing data (NoData).\n\n\nTask Query\n\n\nThis script counts all currently running and ready tasks along with failed tasks.\n\n\nusage: geeadd.py tasks [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\ngeeadd.py tasks\n\n\n\n\nTask Query during ingestion\n\n\nThis script can be used intermittently to look at running, failed and ready(waiting) tasks during ingestion. This script is a special case using query tasks only when uploading assets to collection by providing collection pathway to see how collection size increases.\n\n\nusage: geeadd.py taskquery [-h] [--destination DESTINATION]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --destination DESTINATION\n                        Full path to asset where you are uploading files\n\ngeeadd.py taskquery \nusers/johndoe/myfolder/myponycollection\n                       \n\n\n\n\nTask Report\n\n\nSometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type\n\n\nusage: geeadd.py report [-h] [--r R] [--e E]\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --r R       Path \n CSV filename where the report will be saved\n  --e E       Path \n CSV filename where the errorlog will be saved\n\ngeeadd.py report --r \nreport.csv\n --e \nerrorlog.csv\n\n\n\n\n\nDelete a collection with content:\n\n\nThe delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution!\n\n\ngeeadd delete users/johndoe/test\n\n\n\n\nConsole output:\n\n\n2016-07-17 16:14:09,212 :: oauth2client.client :: INFO :: Attempting refresh to obtain initial access_token\n2016-07-17 16:14:09,213 :: oauth2client.client :: INFO :: Refreshing access_token\n2016-07-17 16:14:10,842 :: root :: INFO :: Attempting to delete collection test\n2016-07-17 16:14:16,898 :: root :: INFO :: Collection users/johndoe/test removed\n\n\n\n\nDelete all directories / collections based on a Unix-like pattern\n\n\ngeeadd delete users/johndoe/*weird[0-9]?name*\n\n\n\n\nAssets Move\n\n\nThis script allows us to recursively move assets from one collection to the other.\n\n\nusage: geeadd.py mover [-h] [--assetpath ASSETPATH] [--finalpath FINALPATH]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --assetpath ASSETPATH\n                        Existing path of assets\n  --finalpath FINALPATH\n                        New path for assets\ngeeadd.py mover --assetpath \nusers/johndoe/myfolder/myponycollection\n --destination \nusers/johndoe/myfolder/myotherponycollection\n                  \n\n\n\n\nAssets Copy\n\n\nThis script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections.\n\n\nusage: geeadd.py copy [-h] [--initial INITIAL] [--final FINAL]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --initial INITIAL  Existing path of assets\n  --final FINAL      New path for assets\ngeeadd.py mover --initial \nusers/johndoe/myfolder/myponycollection\n --final \nusers/johndoe/myfolder/myotherponycollection\n                  \n\n\n\n\nAssets Access\n\n\nThis tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time.\n\n\nusage: geeadd access [-h] --mode MODE --asset ASSET --user USER\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --mode MODE    This lets you select if you want to change permission or\n                 folder/collection/image\n  --asset ASSET  This is the path to the earth engine asset whose permission\n                 you are changing folder/collection/image\n  --user USER    This is the email address to whom you want to give read or\n                 write permission Usage: \njohn@doe.com:R\n or \njohn@doe.com:W\n\n                 R/W refers to read or write permission\ngeeadd.py access --mode folder --asset \nfolder/collection/image\n --user \njohn@doe.com:R\n\n\n\n\n\nSet Collection Property\n\n\nThis script is derived from the ee tool to set collection properties and will set overall properties for collection. \n\n\nusage: geeadd.py collprop [-h] [--coll COLL] [--p P]\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --coll COLL  Path of Image Collection\n  --p P        \nsystem:description=Description\n/\nsystem:provider_url=url\n/\nsys\n               tem:tags=tags\n/\nsystem:title=title\n\n\n\n\nConvert to Fusion Table\n\n\nOnce validated with gdal and google fusion table it can be used to convert any geoObject to google fusion table. Forked and contributed by Gennadii \nhere\n. The scripts can be used only with a specific google account\n\n\nusage: geeadd.py convert2ft [-h] --i I --o O [--add_missing]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --i I          input feature source (KML, SHP, SpatiLite, etc.)\n  --o O          output Fusion Table name\n  --add_missing  add missing features from the last inserted feature index\n\ngeeadd.py convert2ft --i \n./aoi.kml\n --o \nconverted_aoi\n\n\n\n\n\nCleanup Utility\n\n\nThis script is used to clean folders once all processes have been completed. In short this is a function to clear folder on local machine.\n\n\nusage: geeadd.py cleanout [-h] [--dirpath DIRPATH]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --dirpath DIRPATH  Folder you want to delete after all processes have been\n                     completed\ngeeadd.py cleanout --dirpath \n./folder\n\n\n\n\n\nCancel all tasks\n\n\nThis is a simpler tool, can be called directly from the earthengine cli as well\n\n\nearthengine cli command\nearthengine task cancel all\n\nusage: geeadd.py cancel [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit", 
            "title": "Planet-Google Earth Engine Pipeline CLI"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#planet-gee-pipeline-cli", 
            "text": "While moving between assets from Planet Inc and Google Earth Engine it was imperative to create a pipeline that allows for easy transitions between the two service end points and this tool is designed to act as a step by step process chain from Planet Assets to batch upload and modification within the Google Earth Engine environment. The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. tab.", 
            "title": "Planet GEE Pipeline CLI"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#table-of-contents", 
            "text": "Installation  Getting started  Batch uploader  Parsing metadata    Usage examples  Planet Tools  Planet Key  AOI JSON  Activate or Check Asset  Download Asset  Metadata Parser    Earth Engine Tools  EE User  Upload a directory with images and associate properties with each image:  Upload a directory with images with specific NoData value to a selected destination:  Task Query  Task Query during ingestion  Task Report  Delete a collection with content:  Assets Move  Assets Copy  Assets Access  Set Collection Property  Convert to Fusion Table  Cleanup Utility  Cancel all tasks", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#installation", 
            "text": "We assume Earth Engine Python API is installed and EE authorised as desribed  here . We also assume Planet Python API is installed you can install by simply running   pip install planet  Further instructions can be found  here    This toolbox also uses some functionality from GDAL\nFor installing GDAL in Ubuntu  sudo add-apt-repository ppa:ubuntugis/ppa   sudo apt-get update\nsudo apt-get install gdal-bin  For Windows I found this  guide  from UCLA  To install toolbox:  git clone https://github.com/samapriya/Planet-GEE-Pipeline-CLI.git\ncd Planet-GEE-Pipeline-CLI   pip install .  Installation is an optional step; the application can be also run\ndirectly by executing geeadd.py script. The advantage of having it\ninstalled is being able to execute geeadd as any command line tool. I\nrecommend installation within virtual environment. To install run  python setup.py develop or python setup.py install\n\nIn a linux distribution\nsudo python setup.py develop or sudo python setup.py install", 
            "title": "Installation"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#getting-started", 
            "text": "As usual, to print help:  Planet Pipeline with Google Earth Engine Batch Addons\n\npositional arguments:\n  { ,planetkey,aoijson,activatepl,downloadpl,metadata,ee_user,upload,delete,tasks,taskquery,report,cancel,mover,copy,access,collprop,convert2ft,cleanout}\n                        ---------------------------------------\n                        -----Choose from Planet Tools Below-----\n                        ---------------------------------------\n    planetkey           Enter your planet API Key\n    aoijson             Tool to convert KML, Shapefile,WKT,GeoJSON or Landsat\n                        WRS PathRow file to AreaOfInterest.JSON file with\n                        structured query for use with Planet API 1.0\n    activatepl          Tool to query and/or activate Planet Assets\n    downloadpl          Tool to download Planet Assets\n    metadata            Tool to tabulate and convert all metadata files from\n                        Planet or Digital Globe Assets\n                        -------------------------------------------\n                        ----Choose from Earth Engine Tools Below----\n                        -------------------------------------------\n    ee_user             Get Earth Engine API Key   Paste it back to Command\n                        line/shell to change user\n    upload              Batch Asset Uploader to Earth Engine.\n    delete              Deletes collection and all items inside. Supports\n                        Unix-like wildcards.\n    tasks               Queries currently running, enqued,failed\n    taskquery           Queries currently running, enqued,failed ingestions\n                        and uploaded assets\n    report              Create a report of all tasks and exports to a CSV file\n    cancel              Cancel all running tasks\n    mover               Moves all assets from one collection to another\n    copy                Copies all assets from one collection to another:\n                        Including copying from other users if you have read\n                        permission to their assets\n    access              Sets Permissions for Images, Collection or all assets\n                        in EE Folder Example: python ee_permissions.py --mode\n                         folder  --asset  users/john/doe  --user\n                         jimmy@doe.com:R \n    collprop            Sets Overall Properties for Image Collection\n    convert2ft          Uploads a given feature collection to Google Fusion\n                        Table.\n    cleanout            Clear folders with datasets from earlier downloaded\n\noptional arguments:\n  -h, --help            show this help message and exit  To obtain help for a specific functionality, simply call it with  help \nswitch, e.g.:  geeadd upload -h . If you didn't install geeadd, then you\ncan run it just by going to  geeadd  directory and running  python\ngeeadd.py [arguments go here]", 
            "title": "Getting started"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#batch-uploader", 
            "text": "The script creates an Image Collection from GeoTIFFs in your local\ndirectory. By default, the collection name is the same as the local\ndirectory name; with optional parameter you can provide a different\nname. Another optional parameter is a path to a CSV file with metadata\nfor images, which is covered in the next section: Parsing metadata .  usage: geeadd.py upload [-h] -u USER --source SOURCE --dest DEST [-m METADATA]\n                        [--large] [--nodata NODATA]\n\noptional arguments:\n  -h, --help            show this help message and exit\n\nRequired named arguments.:\n  -u USER, --user USER  Google account name (gmail address).\n  --source SOURCE       Path to the directory with images for upload.\n  --dest DEST           Destination. Full path for upload to Google Earth\n                        Engine, e.g. users/johndoe/myponycollection\n\nOptional named arguments:\n  -m METADATA, --metadata METADATA\n                        Path to CSV with metadata.\n  --large               (Advanced) Use multipart upload. Might help if upload\n                        of large files is failing on some systems. Might cause\n                        other issues.\n  --nodata NODATA       The value to burn into the raster as NoData (missing\n                        data)", 
            "title": "Batch uploader"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#parsing-metadata", 
            "text": "By metadata we understand here the properties associated with each image. Thanks to these, GEE user can easily filter collection based on specified criteria. The file with metadata should be organised as follows:     filename (without extension)  property1 header  property2 header      file1  value1  value2    file2  value3  value4     Note that header can contain only letters, digits and underscores.   Example:     id_no  class  category  binomial  system:time_start      my_file_1  GASTROPODA  EN  Aaadonta constricta  1478943081000    my_file_2  GASTROPODA  CR  Aaadonta irregularis  1478943081000     The corresponding files are my_file_1.tif and my_file_2.tif. With each of the files five properties are associated: id_no, class, category, binomial and system:time_start. The latter is time in Unix epoch format, in milliseconds, as documented in GEE glosary. The program will match the file names from the upload directory with ones provided in the CSV and pass the metadata in JSON format:  { id_no: my_file_1, class: GASTROPODA, category: EN, binomial: Aaadonta constricta, system:time_start: 1478943081000}  The program will report any illegal fields, it will also complain if not all of the images passed for upload have metadata associated. User can opt to ignore it, in which case some assets will have no properties.  Having metadata helps in organising your asstets, but is not mandatory - you can skip it.", 
            "title": "Parsing metadata"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#usage-examples", 
            "text": "Usage examples have been segmented into two parts focusing on both planet tools as well as earth engine tools, earth engine tools include additional developments in CLI which allows you to recursively interact with their python API", 
            "title": "Usage examples"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#planet-tools", 
            "text": "The Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.", 
            "title": "Planet Tools"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#planet-key", 
            "text": "This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools  usage: ppipe.py planetkey [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit  If using on a private machine the Key is saved as a csv file for all future runs of the tool.", 
            "title": "Planet Key"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#aoi-json", 
            "text": "The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.  usage: ppipe.py aoijson [-h] [--start START] [--end END] [--cloud CLOUD]\n                        [--inputfile INPUTFILE] [--geo GEO]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --start START         Start date in YYYY-MM-DD?\n  --end END             End date in YYYY-MM-DD?\n  --cloud CLOUD         Maximum Cloud Cover(0-1) representing 0-100\n  --inputfile INPUTFILE\n                        Choose a kml/shapefile/geojson or WKT file for\n                        AOI(KML/SHP/GJSON/WKT) or WRS (6 digit RowPath\n                        Example: 023042)\n  --geo GEO             map.geojson/aoi.kml/aoi.shp/aoi.wkt file", 
            "title": "AOI JSON"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#activate-or-check-asset", 
            "text": "The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier  usage: ppipe.py activatepl [-h] [--aoi AOI] [--action ACTION] [--asst ASST]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --aoi AOI        Choose aoi.json file created earlier\n  --action ACTION  choose between check/activate\n  --asst ASST      Choose between planet asset types (PSOrthoTile\n                   analytic/REOrthoTile analytic/PSOrthoTile\n                   analytic_xml/REOrthoTile analytic_xml", 
            "title": "Activate or Check Asset"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#download-asset", 
            "text": "Having metadata helps in organising your asstets, but is not mandatory - you can skip it.\nThe downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types.  usage: ppipe.py downloadpl [-h] [--aoi AOI] [--action ACTION] [--asst ASST]\n                           [--pathway PATHWAY]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --aoi AOI          Choose aoi.json file created earlier\n  --action ACTION    choose download\n  --asst ASST        Choose between planet asset types (PSOrthoTile\n                     analytic/REOrthoTile analytic/PSOrthoTile\n                     analytic_xml/REOrthoTile analytic_xml\n  --pathway PATHWAY  Folder Pathways where PlanetAssets are saved exampled\n                     ./PlanetScope ./RapidEye", 
            "title": "Download Asset"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#metadata-parser", 
            "text": "The metadata tab is a more powerful tool and consists of metadata parsing for PlanetScope OrthoTile RapiEye OrthoTile along with Digital Globe MultiSpectral and DigitalGlobe PanChromatic datasets. This was developed as a standalone to process xml metadata files from multiple sources and is important step is the user plans to upload these assets to Google Earth Engine. The combine Planet-GEE Pipeline tool will be made available soon for testing.  usage: ppipe.py metadata [-h] [--asset ASSET] [--mf MF] [--mfile MFILE]\n                         [--errorlog ERRORLOG]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --asset ASSET        Choose RapidEye/PlantScope/DigitalGlobe\n                       MultiSpectral/DigitalGlobe Panchromatic\n                       (RE/PS/DGMS/DGP)?\n  --mf MF              Metadata folder?\n  --mfile MFILE        Metadata filename to be exported along with Path.csv\n  --errorlog ERRORLOG  Errorlog to be exported along with Path.csv", 
            "title": "Metadata Parser"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#earth-engine-tools", 
            "text": "The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. This is also a seperate package for earth engine users to use and can be downloaded  here", 
            "title": "Earth Engine Tools"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#ee-user", 
            "text": "This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back", 
            "title": "EE User"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#upload-a-directory-with-images-to-your-myfoldermycollection-and-associate-properties-with-each-image", 
            "text": "geeadd upload -u johndoe@gmail.com --source path_to_directory_with_tif -m path_to_metadata.csv --dest users/johndoe/myfolder/myponycollection  The script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.", 
            "title": "Upload a directory with images to your myfolder/mycollection and associate properties with each image:"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#upload-a-directory-with-images-with-specific-nodata-value-to-a-selected-destination", 
            "text": "geeadd upload -u johndoe@gmail.com --source path_to_directory_with_tif --dest users/johndoe/myfolder/myponycollection --nodata 222  In this case we need to supply full path to the destination, which is helpful when we upload to a shared folder. In the provided example we also burn value 222 into all rasters for missing data (NoData).", 
            "title": "Upload a directory with images with specific NoData value to a selected destination"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#task-query", 
            "text": "This script counts all currently running and ready tasks along with failed tasks.  usage: geeadd.py tasks [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\ngeeadd.py tasks", 
            "title": "Task Query"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#task-query-during-ingestion", 
            "text": "This script can be used intermittently to look at running, failed and ready(waiting) tasks during ingestion. This script is a special case using query tasks only when uploading assets to collection by providing collection pathway to see how collection size increases.  usage: geeadd.py taskquery [-h] [--destination DESTINATION]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --destination DESTINATION\n                        Full path to asset where you are uploading files\n\ngeeadd.py taskquery  users/johndoe/myfolder/myponycollection", 
            "title": "Task Query during ingestion"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#task-report", 
            "text": "Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type  usage: geeadd.py report [-h] [--r R] [--e E]\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --r R       Path   CSV filename where the report will be saved\n  --e E       Path   CSV filename where the errorlog will be saved\n\ngeeadd.py report --r  report.csv  --e  errorlog.csv", 
            "title": "Task Report"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#delete-a-collection-with-content", 
            "text": "The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution!  geeadd delete users/johndoe/test  Console output:  2016-07-17 16:14:09,212 :: oauth2client.client :: INFO :: Attempting refresh to obtain initial access_token\n2016-07-17 16:14:09,213 :: oauth2client.client :: INFO :: Refreshing access_token\n2016-07-17 16:14:10,842 :: root :: INFO :: Attempting to delete collection test\n2016-07-17 16:14:16,898 :: root :: INFO :: Collection users/johndoe/test removed", 
            "title": "Delete a collection with content:"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#delete-all-directories-collections-based-on-a-unix-like-pattern", 
            "text": "geeadd delete users/johndoe/*weird[0-9]?name*", 
            "title": "Delete all directories / collections based on a Unix-like pattern"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#assets-move", 
            "text": "This script allows us to recursively move assets from one collection to the other.  usage: geeadd.py mover [-h] [--assetpath ASSETPATH] [--finalpath FINALPATH]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --assetpath ASSETPATH\n                        Existing path of assets\n  --finalpath FINALPATH\n                        New path for assets\ngeeadd.py mover --assetpath  users/johndoe/myfolder/myponycollection  --destination  users/johndoe/myfolder/myotherponycollection", 
            "title": "Assets Move"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#assets-copy", 
            "text": "This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections.  usage: geeadd.py copy [-h] [--initial INITIAL] [--final FINAL]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --initial INITIAL  Existing path of assets\n  --final FINAL      New path for assets\ngeeadd.py mover --initial  users/johndoe/myfolder/myponycollection  --final  users/johndoe/myfolder/myotherponycollection", 
            "title": "Assets Copy"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#assets-access", 
            "text": "This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time.  usage: geeadd access [-h] --mode MODE --asset ASSET --user USER\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --mode MODE    This lets you select if you want to change permission or\n                 folder/collection/image\n  --asset ASSET  This is the path to the earth engine asset whose permission\n                 you are changing folder/collection/image\n  --user USER    This is the email address to whom you want to give read or\n                 write permission Usage:  john@doe.com:R  or  john@doe.com:W \n                 R/W refers to read or write permission\ngeeadd.py access --mode folder --asset  folder/collection/image  --user  john@doe.com:R", 
            "title": "Assets Access"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#set-collection-property", 
            "text": "This script is derived from the ee tool to set collection properties and will set overall properties for collection.   usage: geeadd.py collprop [-h] [--coll COLL] [--p P]\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --coll COLL  Path of Image Collection\n  --p P         system:description=Description / system:provider_url=url / sys\n               tem:tags=tags / system:title=title", 
            "title": "Set Collection Property"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#convert-to-fusion-table", 
            "text": "Once validated with gdal and google fusion table it can be used to convert any geoObject to google fusion table. Forked and contributed by Gennadii  here . The scripts can be used only with a specific google account  usage: geeadd.py convert2ft [-h] --i I --o O [--add_missing]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --i I          input feature source (KML, SHP, SpatiLite, etc.)\n  --o O          output Fusion Table name\n  --add_missing  add missing features from the last inserted feature index\n\ngeeadd.py convert2ft --i  ./aoi.kml  --o  converted_aoi", 
            "title": "Convert to Fusion Table"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#cleanup-utility", 
            "text": "This script is used to clean folders once all processes have been completed. In short this is a function to clear folder on local machine.  usage: geeadd.py cleanout [-h] [--dirpath DIRPATH]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --dirpath DIRPATH  Folder you want to delete after all processes have been\n                     completed\ngeeadd.py cleanout --dirpath  ./folder", 
            "title": "Cleanup Utility"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#cancel-all-tasks", 
            "text": "This is a simpler tool, can be called directly from the earthengine cli as well  earthengine cli command\nearthengine task cancel all\n\nusage: geeadd.py cancel [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit", 
            "title": "Cancel all tasks"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/", 
            "text": "Planet GEE Pipeline GUI\n\n\n\n\n\n\n\n\nThe Planet Pipeline GUI came from the actual CLI (command line interface tools) to enable the use of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.\n\n\n\n\nTable of contents\n\n\n\n\nInstallation\n\n\nUsage examples\n\n\nPlanet Tools\n\n\nPlanet Key\n\n\nAOI JSON\n\n\nActivate or Check Asset\n\n\nDownload Asset\n\n\nMetadata Parser\n\n\n\n\n\n\nEarth Engine Tools\n\n\nEE User\n\n\nUpload a directory with images and associate properties with each image:\n\n\nUpload a directory with images with specific NoData value to a selected destination:\n\n\nTask Query\n\n\nTask Query during ingestion\n\n\nTask Report\n\n\nDelete a collection with content:\n\n\nAssets Move\n\n\nAssets Copy\n\n\nAssets Access\n\n\nSet Collection Property\n\n\nConvert to Fusion Table\n\n\nCleanup Utility\n\n\nCancel all tasks\n\n\n\n\n\n\nCredits\n\n\n\n\nInstallation\n\n\nWe assume Earth Engine Python API is installed and EE authorised as desribed \nhere\n. We also assume Planet Python API is installed you can install by simply running \n\n\npip install planet\n\n\n\n\nFurther instructions can be found \nhere\n \n\n\nYou require two important packages for this to run\n\n\nWxPython(which is what the GUI is built on)\nfor windows(Tested in Windows 10)\nhttps://wxpython.org/download.php\npip install wxPython\n\nfor linux(Tested in Ubuntu 16)\nsudo add-apt-repository \ndeb http://archive.ubuntu.com/ubuntu utopic main restricted universe\n  \nsudo apt-get update\napt-cache search python-wxgtk3.0\nsudo apt-get install python-wxgtk3.0\n\n\n\n\nThis toolbox also uses some functionality from GDAL\nFor installing GDAL in Ubuntu\n\n\nsudo add-apt-repository ppa:ubuntugis/ppa \n sudo apt-get update\nsudo apt-get install gdal-bin\n\n\n\n\nFor Windows I found this \nguide\n from UCLA\n\n\nUsage examples\n\n\nUsage examples have been segmented into two parts focusing on both planet tools as well as earth engine tools, earth engine tools include additional developments in CLI which allows you to recursively interact with their python API. To run the tool open a command prompt window or terminal and type\n\n\npython ee_ppipe.pyc\n\n\n\n\nPlanet Tools\n\n\nThe Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.\n\n\nPlanet Key\n\n\nThis tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools\n\n\n\n\nIf using on a private machine the Key is saved as a csv file for all future runs of the tool.\n\n\nAOI JSON\n\n\nThe aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.\n\n\n\n\nActivate or Check Asset\n\n\nThe activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier\n\n\n\n\nDownload Asset\n\n\nHaving metadata helps in organising your asstets, but is not mandatory - you can skip it.\nThe downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types.\n\n\n\n\nMetadata Parser\n\n\nThe metadata tab is a more powerful tool and consists of metadata parsing for PlanetScope OrthoTile RapiEye OrthoTile along with Digital Globe MultiSpectral and DigitalGlobe PanChromatic datasets. This was developed as a standalone to process xml metadata files from multiple sources and is important step is the user plans to upload these assets to Google Earth Engine. The combine Planet-GEE Pipeline tool will be made available soon for testing.\n\n\n\n\nEarth Engine Tools\n\n\nThe ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. This is also a seperate package for earth engine users to use and can be downloaded \nhere\n\n\nEE User\n\n\nThis tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back\n\n\n\nUpload a directory with images to your myfolder/mycollection and associate properties with each image:\n\n\n\n\nThe script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.\n\n\nTask Query\n\n\nThis script counts all currently running and ready tasks along with failed tasks.\n\n\n\n\nTask Query during ingestion\n\n\nThis script can be used intermittently to look at running, failed and ready(waiting) tasks during ingestion. This script is a special case using query tasks only when uploading assets to collection by providing collection pathway to see how collection size increases.\n\n\n\n\nTask Report\n\n\nSometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type\n\n\n\n\nDelete a collection with content:\n\n\nThe delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution!\n\n\n\n\nAssets Move\n\n\nThis script allows us to recursively move assets from one collection to the other.\n\n\n\n\nAssets Copy\n\n\nThis script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections.\n\n\n\n\nAssets Access\n\n\nThis tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time.\n\n\n\n\nSet Collection Property\n\n\nThis script is derived from the ee tool to set collection properties and will set overall properties for collection. \n\n\n\n\nConvert to Fusion Table\n\n\nOnce validated with gdal and google fusion table it can be used to convert any geoObject to google fusion table. Forked and contributed by Gennadii \nhere\n. The scripts can be used only with a specific google account\n\n\n\n\nCleanup Utility\n\n\nThis script is used to clean folders once all processes have been completed. In short this is a function to clear folder on local machine.\n\n\n\n\nCancel all tasks\n\n\nThis is a simpler tool, can be called directly from the earthengine cli as well\n\n\n\n\nCredits\n\n\nJetStream\n A portion of the work is suported by JetStream Grant TG-GEO160014.\n\n\nAlso supported by \nPlanet Labs Ambassador Program", 
            "title": "Planet-Google Earth Engine Pipeline GUI"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#planet-gee-pipeline-gui", 
            "text": "The Planet Pipeline GUI came from the actual CLI (command line interface tools) to enable the use of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.", 
            "title": "Planet GEE Pipeline GUI"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#table-of-contents", 
            "text": "Installation  Usage examples  Planet Tools  Planet Key  AOI JSON  Activate or Check Asset  Download Asset  Metadata Parser    Earth Engine Tools  EE User  Upload a directory with images and associate properties with each image:  Upload a directory with images with specific NoData value to a selected destination:  Task Query  Task Query during ingestion  Task Report  Delete a collection with content:  Assets Move  Assets Copy  Assets Access  Set Collection Property  Convert to Fusion Table  Cleanup Utility  Cancel all tasks    Credits", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#installation", 
            "text": "We assume Earth Engine Python API is installed and EE authorised as desribed  here . We also assume Planet Python API is installed you can install by simply running   pip install planet  Further instructions can be found  here    You require two important packages for this to run  WxPython(which is what the GUI is built on)\nfor windows(Tested in Windows 10)\nhttps://wxpython.org/download.php\npip install wxPython\n\nfor linux(Tested in Ubuntu 16)\nsudo add-apt-repository  deb http://archive.ubuntu.com/ubuntu utopic main restricted universe   \nsudo apt-get update\napt-cache search python-wxgtk3.0\nsudo apt-get install python-wxgtk3.0  This toolbox also uses some functionality from GDAL\nFor installing GDAL in Ubuntu  sudo add-apt-repository ppa:ubuntugis/ppa   sudo apt-get update\nsudo apt-get install gdal-bin  For Windows I found this  guide  from UCLA", 
            "title": "Installation"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#usage-examples", 
            "text": "Usage examples have been segmented into two parts focusing on both planet tools as well as earth engine tools, earth engine tools include additional developments in CLI which allows you to recursively interact with their python API. To run the tool open a command prompt window or terminal and type  python ee_ppipe.pyc", 
            "title": "Usage examples"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#planet-tools", 
            "text": "The Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.", 
            "title": "Planet Tools"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#planet-key", 
            "text": "This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools   If using on a private machine the Key is saved as a csv file for all future runs of the tool.", 
            "title": "Planet Key"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#aoi-json", 
            "text": "The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.", 
            "title": "AOI JSON"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#activate-or-check-asset", 
            "text": "The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier", 
            "title": "Activate or Check Asset"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#download-asset", 
            "text": "Having metadata helps in organising your asstets, but is not mandatory - you can skip it.\nThe downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types.", 
            "title": "Download Asset"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#metadata-parser", 
            "text": "The metadata tab is a more powerful tool and consists of metadata parsing for PlanetScope OrthoTile RapiEye OrthoTile along with Digital Globe MultiSpectral and DigitalGlobe PanChromatic datasets. This was developed as a standalone to process xml metadata files from multiple sources and is important step is the user plans to upload these assets to Google Earth Engine. The combine Planet-GEE Pipeline tool will be made available soon for testing.", 
            "title": "Metadata Parser"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#earth-engine-tools", 
            "text": "The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. This is also a seperate package for earth engine users to use and can be downloaded  here", 
            "title": "Earth Engine Tools"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#ee-user", 
            "text": "This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back", 
            "title": "EE User"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#upload-a-directory-with-images-to-your-myfoldermycollection-and-associate-properties-with-each-image", 
            "text": "The script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.", 
            "title": "Upload a directory with images to your myfolder/mycollection and associate properties with each image:"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#task-query", 
            "text": "This script counts all currently running and ready tasks along with failed tasks.", 
            "title": "Task Query"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#task-query-during-ingestion", 
            "text": "This script can be used intermittently to look at running, failed and ready(waiting) tasks during ingestion. This script is a special case using query tasks only when uploading assets to collection by providing collection pathway to see how collection size increases.", 
            "title": "Task Query during ingestion"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#task-report", 
            "text": "Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type", 
            "title": "Task Report"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#delete-a-collection-with-content", 
            "text": "The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution!", 
            "title": "Delete a collection with content:"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#assets-move", 
            "text": "This script allows us to recursively move assets from one collection to the other.", 
            "title": "Assets Move"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#assets-copy", 
            "text": "This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections.", 
            "title": "Assets Copy"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#assets-access", 
            "text": "This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time.", 
            "title": "Assets Access"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#set-collection-property", 
            "text": "This script is derived from the ee tool to set collection properties and will set overall properties for collection.", 
            "title": "Set Collection Property"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#convert-to-fusion-table", 
            "text": "Once validated with gdal and google fusion table it can be used to convert any geoObject to google fusion table. Forked and contributed by Gennadii  here . The scripts can be used only with a specific google account", 
            "title": "Convert to Fusion Table"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#cleanup-utility", 
            "text": "This script is used to clean folders once all processes have been completed. In short this is a function to clear folder on local machine.", 
            "title": "Cleanup Utility"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#cancel-all-tasks", 
            "text": "This is a simpler tool, can be called directly from the earthengine cli as well", 
            "title": "Cancel all tasks"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#credits", 
            "text": "JetStream  A portion of the work is suported by JetStream Grant TG-GEO160014.  Also supported by  Planet Labs Ambassador Program", 
            "title": "Credits"
        }, 
        {
            "location": "/projects/arcmap_addon/", 
            "text": "ArcMap Addons\n\n\n\n\nWhile working on different projects over sometime I had to create and use some tools which acted as valuable addons to arcmap toolbox. I am sharing a few and I will keep on updating these as I keep working on new models.This toolbox was created on ArcMap 10.4 and is not backward compatible. The toolbox can be downloaded and added to existing toolbox to create additional functionalities.\n\n\n\n\nTable of contents\n\n\n\n\nInstallation\n\n\nUsage examples\n\n\nEmail Notification\n\n\nIterative Clip\n\n\nMultiBand to Single Images\n\n\nRaster Properties as CSV\n\n\nRaster Copy Iterative\n\n\nFeature Select and Copy\n\n\nSelect and Calculate Field\n\n\n\n\n\n\nCredits\n\n\n\n\nInstallation\n\n\nWe assume that the user already has a copy of ArcMap \n=10.4. The toolbox can be downloaded along with adjoining script and added onto existing toolbox. You can keep this toolbox as part of the default toolboxes by clicking Save Settings\nSave as Default\n\n\nUsage examples\n\n\nUsage examples will vary and only continue to grow as new tools are added to the toolbox. \n\n\nEmail Notification\n\n\nI found this to be one of the most useful tools when running a long process and not having to wait for your results. The tool makes use of the smtp library and allows the user to set up an email service for an email to be sent out after a process has been completed. \n\n\n\n\nIterative Clip\n\n\nThis tool does exactl what the name suggests it uses an iterator tool to clip a raster to different boundaries using shapefiles. Requires a single raster file and a folder with multiple polygons for clips.\n\n\n\n\nIf using on a private machine the Key is saved as a csv file for all future runs of the tool.\n\n\nMultiBand to Single Images\n\n\nThe aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.\n\n\n\n\nRaster Properties as CSV\n\n\nThe activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier\n\n\n\n\nRaster Copy Iterative\n\n\nHaving metadata helps in organising your asstets, but is not mandatory - you can skip it.\nThe downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types.\n\n\n\n\nFeature Select and Copy\n\n\nThese tools allow the users to iteratively select features using a given SQL expression and then extract that as a new feature class. This can be done iteratively for all feature classes in the folder.\n\n\n\n\nSelect and Calculate Field\n\n\nThis tool allows you to select a field and then recalculate or calculate it based on expressions. The user has the option of simply filling empty fields based on selection of a different field. Meaning I can calculate a field B based on a selection of a seperate field A if I want.\n\n\n\n\nCredits\n\n\nI would like to thank all those who bring me interesting problems to solve, there is little that is needed to keep one inspired.", 
            "title": "ArcMap Addon Tools"
        }, 
        {
            "location": "/projects/arcmap_addon/#arcmap-addons", 
            "text": "While working on different projects over sometime I had to create and use some tools which acted as valuable addons to arcmap toolbox. I am sharing a few and I will keep on updating these as I keep working on new models.This toolbox was created on ArcMap 10.4 and is not backward compatible. The toolbox can be downloaded and added to existing toolbox to create additional functionalities.", 
            "title": "ArcMap Addons"
        }, 
        {
            "location": "/projects/arcmap_addon/#table-of-contents", 
            "text": "Installation  Usage examples  Email Notification  Iterative Clip  MultiBand to Single Images  Raster Properties as CSV  Raster Copy Iterative  Feature Select and Copy  Select and Calculate Field    Credits", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/arcmap_addon/#installation", 
            "text": "We assume that the user already has a copy of ArcMap  =10.4. The toolbox can be downloaded along with adjoining script and added onto existing toolbox. You can keep this toolbox as part of the default toolboxes by clicking Save Settings Save as Default", 
            "title": "Installation"
        }, 
        {
            "location": "/projects/arcmap_addon/#usage-examples", 
            "text": "Usage examples will vary and only continue to grow as new tools are added to the toolbox.", 
            "title": "Usage examples"
        }, 
        {
            "location": "/projects/arcmap_addon/#email-notification", 
            "text": "I found this to be one of the most useful tools when running a long process and not having to wait for your results. The tool makes use of the smtp library and allows the user to set up an email service for an email to be sent out after a process has been completed.", 
            "title": "Email Notification"
        }, 
        {
            "location": "/projects/arcmap_addon/#iterative-clip", 
            "text": "This tool does exactl what the name suggests it uses an iterator tool to clip a raster to different boundaries using shapefiles. Requires a single raster file and a folder with multiple polygons for clips.   If using on a private machine the Key is saved as a csv file for all future runs of the tool.", 
            "title": "Iterative Clip"
        }, 
        {
            "location": "/projects/arcmap_addon/#multiband-to-single-images", 
            "text": "The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.", 
            "title": "MultiBand to Single Images"
        }, 
        {
            "location": "/projects/arcmap_addon/#raster-properties-as-csv", 
            "text": "The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier", 
            "title": "Raster Properties as CSV"
        }, 
        {
            "location": "/projects/arcmap_addon/#raster-copy-iterative", 
            "text": "Having metadata helps in organising your asstets, but is not mandatory - you can skip it.\nThe downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types.", 
            "title": "Raster Copy Iterative"
        }, 
        {
            "location": "/projects/arcmap_addon/#feature-select-and-copy", 
            "text": "These tools allow the users to iteratively select features using a given SQL expression and then extract that as a new feature class. This can be done iteratively for all feature classes in the folder.", 
            "title": "Feature Select and Copy"
        }, 
        {
            "location": "/projects/arcmap_addon/#select-and-calculate-field", 
            "text": "This tool allows you to select a field and then recalculate or calculate it based on expressions. The user has the option of simply filling empty fields based on selection of a different field. Meaning I can calculate a field B based on a selection of a seperate field A if I want.", 
            "title": "Select and Calculate Field"
        }, 
        {
            "location": "/projects/arcmap_addon/#credits", 
            "text": "I would like to thank all those who bring me interesting problems to solve, there is little that is needed to keep one inspired.", 
            "title": "Credits"
        }, 
        {
            "location": "/projects/synthetic_models/", 
            "text": "Synthetic LandScape Generation\n\n\nWhile working on synthetic landscape generation or neutral landscape generation, it it interesting to try out different randomizing and clustering algorithms. The ones included here were part of the same experiments. The outputs are generally preceded by a header so that the ASCII files can be read easily in a GIS software such as ArcMap. The output file is ASCII type since this is commonly accepted by multiple tools. The number of rows and columns can be changed as can the no data value which for now is set to a 16 bit signed integer output. There are codes included to generate a video using landscapes as frames within the output file. Further work can include the nlmpy module \n\n\nTable of contents\n\n\n\n\nInstallation\n\n\nPackages\n\n\nClumped matrix algorithms\n\n\nFragmentation Aggregation Algorithms\n\n\nNlmPy applications\n\n\nNoise Function Terrain Generation\n\n\nRandom matrix algorithms\n\n\nRandom matrix to video\n\n\n\n\n\n\nCredits\n\n\n\n\nInstallation\n\n\nWe assume that the user already has a copy of Matlab and has installed necessary libraries in python such as nlmpy and numpy+mlk and scipy. The toolbox can be modified to accomodate varying sizes of images to be produced and varying number of classes. The codes are mostly written in matlab and some implementation in NlmPy produced by \n Etherington et al\n are included as well.\n\n\nPackages\n\n\nEach folder has a variety of packages and tools available to run different kinds of analysis. Tools and methods include and are not limited to the following\n\n\nClumped matrix algorithms\n\n\nThis allows for clumping to occur in random matrix algorithms such that there is an element of non random allocation of class\n\n\n\n\n\n\n\n\nfilename\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nclumped_randi_land.m\n\n\nclumping algorithm applied to Uniformly distributed pseudorandom integers\n\n\n\n\n\n\nclumped_sprand_land\n\n\nclumping algorithm applied to Sparse uniformly distributed random matrix\n\n\n\n\n\n\n\n\nFragmentation Aggregation Algorithms\n\n\nThis one is more specific looking at different fragmentation and aggregation algorithms and allows the user to set convergence time for solution.It includes Gibbs model and Metropolis-Hastings algorithm.\n\n\nNlmPy applications\n\n\nNlmPy was created as a python library which allows the user to user different algorithms \n Etherington et al\n. The output files as ASCII to allow for easy read.\n\n\nNoise Function Terrain Generation\n\n\nThese Matlab functions allows you to test noise functions to generate artificial terrains using noise function executables.Source codes for the c++ noise functions files are included in the adjoining folder and were provided kindly by Travis Archer for open use.The codes create a bmp output which are then modified to create binned responses and landscape classes as needed. Noise types include\n\n Cell Noise\n\n Diamond Square\n\n Erosion\n\n Midpoint Displacement\n\n Perline Noise\n\n Simplex Noise\n* Value Noise\n\n\nRandom matrix algorithms\n\n\nThis uses the random matrix functions within matlab to extract different landscape types. Once again they are exported as ASCII.\n\n\n\n\n\n\n\n\nfilename\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nrandi_land.m\n\n\nUniformly distributed pseudorandom integers\n\n\n\n\n\n\nsprand_land.m\n\n\nSparse uniformly distributed random matrix\n\n\n\n\n\n\nrand_land.m\n\n\nUniformly distributed random numbers\n\n\n\n\n\n\nrng_seed_land\n\n\nRandom Seed Generation with Random Number Generator\n\n\n\n\n\n\n\n\nRandom matrix to video\n\n\nThese are experiments within matlab to use the landscapes generated as frames in a video output file from the landscapes. The video output is generally in avi format and includes slices of the landscapes produced.\n\n\nCredits\n\n\nI would like to thank Dr. Scott Robeson my mentor among others who got me interested in some topics on Landscape ecology.", 
            "title": "Synthetic Landscape Generation Models"
        }, 
        {
            "location": "/projects/synthetic_models/#synthetic-landscape-generation", 
            "text": "While working on synthetic landscape generation or neutral landscape generation, it it interesting to try out different randomizing and clustering algorithms. The ones included here were part of the same experiments. The outputs are generally preceded by a header so that the ASCII files can be read easily in a GIS software such as ArcMap. The output file is ASCII type since this is commonly accepted by multiple tools. The number of rows and columns can be changed as can the no data value which for now is set to a 16 bit signed integer output. There are codes included to generate a video using landscapes as frames within the output file. Further work can include the nlmpy module", 
            "title": "Synthetic LandScape Generation"
        }, 
        {
            "location": "/projects/synthetic_models/#table-of-contents", 
            "text": "Installation  Packages  Clumped matrix algorithms  Fragmentation Aggregation Algorithms  NlmPy applications  Noise Function Terrain Generation  Random matrix algorithms  Random matrix to video    Credits", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/synthetic_models/#installation", 
            "text": "We assume that the user already has a copy of Matlab and has installed necessary libraries in python such as nlmpy and numpy+mlk and scipy. The toolbox can be modified to accomodate varying sizes of images to be produced and varying number of classes. The codes are mostly written in matlab and some implementation in NlmPy produced by   Etherington et al  are included as well.", 
            "title": "Installation"
        }, 
        {
            "location": "/projects/synthetic_models/#packages", 
            "text": "Each folder has a variety of packages and tools available to run different kinds of analysis. Tools and methods include and are not limited to the following", 
            "title": "Packages"
        }, 
        {
            "location": "/projects/synthetic_models/#clumped-matrix-algorithms", 
            "text": "This allows for clumping to occur in random matrix algorithms such that there is an element of non random allocation of class     filename  Description      clumped_randi_land.m  clumping algorithm applied to Uniformly distributed pseudorandom integers    clumped_sprand_land  clumping algorithm applied to Sparse uniformly distributed random matrix", 
            "title": "Clumped matrix algorithms"
        }, 
        {
            "location": "/projects/synthetic_models/#fragmentation-aggregation-algorithms", 
            "text": "This one is more specific looking at different fragmentation and aggregation algorithms and allows the user to set convergence time for solution.It includes Gibbs model and Metropolis-Hastings algorithm.", 
            "title": "Fragmentation Aggregation Algorithms"
        }, 
        {
            "location": "/projects/synthetic_models/#nlmpy-applications", 
            "text": "NlmPy was created as a python library which allows the user to user different algorithms   Etherington et al . The output files as ASCII to allow for easy read.", 
            "title": "NlmPy applications"
        }, 
        {
            "location": "/projects/synthetic_models/#noise-function-terrain-generation", 
            "text": "These Matlab functions allows you to test noise functions to generate artificial terrains using noise function executables.Source codes for the c++ noise functions files are included in the adjoining folder and were provided kindly by Travis Archer for open use.The codes create a bmp output which are then modified to create binned responses and landscape classes as needed. Noise types include  Cell Noise  Diamond Square  Erosion  Midpoint Displacement  Perline Noise  Simplex Noise\n* Value Noise", 
            "title": "Noise Function Terrain Generation"
        }, 
        {
            "location": "/projects/synthetic_models/#random-matrix-algorithms", 
            "text": "This uses the random matrix functions within matlab to extract different landscape types. Once again they are exported as ASCII.     filename  Description      randi_land.m  Uniformly distributed pseudorandom integers    sprand_land.m  Sparse uniformly distributed random matrix    rand_land.m  Uniformly distributed random numbers    rng_seed_land  Random Seed Generation with Random Number Generator", 
            "title": "Random matrix algorithms"
        }, 
        {
            "location": "/projects/synthetic_models/#random-matrix-to-video", 
            "text": "These are experiments within matlab to use the landscapes generated as frames in a video output file from the landscapes. The video output is generally in avi format and includes slices of the landscapes produced.", 
            "title": "Random matrix to video"
        }, 
        {
            "location": "/projects/synthetic_models/#credits", 
            "text": "I would like to thank Dr. Scott Robeson my mentor among others who got me interested in some topics on Landscape ecology.", 
            "title": "Credits"
        }, 
        {
            "location": "/projects/arcticdem_download/", 
            "text": "ArcticDEM Batch Download \n Processing Tools\n\n\n\n\n\n\nArcticDEM project was a joint project supported by both the National Geospatial-Intelligence Agency(NGA) and the National Science Foundation(NSF) with the idea of creating a high resolution and high quality digital surface model(DSM). The product is distributed free of cost as time-dependent DEM strips and is hosted as https links that a user can use to download each strip. As per their policy\n\n\nThe seamless terrain mosaic can be distributed without restriction.\n\n\nThe created product is a 2-by-2 meter elevation cells over an over of over 20 million square kilometers and uses digital globe stereo imagery to create these high resolution DSM. The method used for the 2m derivate is Surface Extraction with TIN-based Search-space Minimization(SETSM).\n\n\nBased on their acknowledgements requests you can use\n\nAcknowledging PGC services(including data access)\n\n\n\n\nGeospatial support for this work provided by the Polar Geospatial Center under NSF OPP awards 1043681 \n 1559691.\n\n\n\n\nAcknowledging DEMS created from the ArcticDEM project\n\n\n\n\nDEMs provided by the Polar Geospatial Center under NSF OPP awards 1043681, 1559691 and 1542736.\n\n\n\n\nYou can find details on the background, scope and methods among other details \nhere\n\nA detailed acknowledgement link can be found \nhere\n\n\nWith this in mind and with the potential applications of using these toolsets there was a need to batch download the DEM files for your area of interest and to be able to extract, clean and process metadata. In all fairness this tool has a motive of extending this as an input to Google Earth Engine and hence the last tool which is the metadata parser is designed to create a metadata manifest in a csv file which GEE can understand and associate during asset upload. \n\n\nTable of contents\n\n\n\n\nInstallation\n\n\nGetting started\n\n\nUsage examples\n\n\nSubset to AOI\n\n\nEstimate Download Size\n\n\nDownload DEM\n\n\nExtract DEM\n\n\nMetadata Parsing for GEE\n\n\n\n\n\n\n\n\nInstallation\n\n\nWe assume that you have installed the requirements files to install all the necessary packages and libraries required to use this tool. To install packages from the requirements.txt file you can simply use\n\npip install -r requirements.txt\n. Remember that installation is an optional step and you can run this program by simply browsing to the pgcdem-cli file and typing \npython arcticdem.py\n. One of the only other requirement for this tool is the Master Shapefile for all DEM footprints(make sure to use the most updated version which can be found \nhere\n)\n\n\nThis toolbox also uses some functionality from GDAL\n\nFor installing GDAL in Ubuntu\n\n\nsudo add-apt-repository ppa:ubuntugis/ppa \n sudo apt-get update\nsudo apt-get install gdal-bin\n\n\n\n\nFor Windows I found this \nguide\n from UCLA\n\n\nTo install \nArcticDEM Batch Download \n Processing Tools:\n\n\ngit clone https://github.com/samapriya/ArcticDEM-Batch-Pipeline.git\ncd ArcticDEM-Batch-Pipeline \n pip install .\n\n\n\n\nThis release also contains a windows installer which bypasses the need for you to have admin permission, it does however require you to have python in the system path meaning when you open up command prompt you should be able to type python and start it within the command prompt window. Post installation using the installer you can just call ppipe using the command prompt similar to calling python. Give it a go post installation type\n\n\narcticdem -h\n\n\n\n\nThe advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. As a extra addon feature I have wrapped the cli into a Graphical User Interface(GUI) so some people will find it easier to use.\n\n\nTo install run\n\n\npython setup.py develop or python setup.py install\n\nIn a linux distribution\nsudo python setup.py develop or sudo python setup.py install\n\n\n\n\nGetting started\n\n\nTo obtain help for a specific functionality, simply call it with \nhelp\n\nswitch, e.g.: \narcticdem demextract -h\n. If you didn't install arcticdem, then you\ncan run it just by going to \narcticdem-cli\n directory and running \npython\narcticdem.py [arguments go here]\n\n\nAs usual, to print help  \narcticdem -h\n:\n\n\nArcticDEM Batch Download \n Processing Tools\npositional arguments:\n  { ,demaoi,demsize,demdownload,demextract,demmeta}\n                        ---------------------------------------\n                        -----Choose from ArcticDEM-Download Tools Below-----\n                        ---------------------------------------\n    demaoi              Allows user to subset Master ArcticDEM to their AOI\n    demsize             Allows users to estimate total download size and space\n                        left in your destination folder\n    demdownload         Allows users to batch download ArcticDEM Strips using\n                        aoi shapefile\n    demextract          Allows users to extract both image and metadata files\n                        from the zipped tar files\n    demmeta             Tool to process metadata files into CSV for all\n                        strips[For use with Google Earth Engine]\n\noptional arguments:\n  -h, --help            show this help message and exit\n\n\n\n\nSubset to AOI\n\n\nThe script clips the master ArcticDEM strip file to a smaller subset usgin an area of interest shapefile. This allows to get the strip DEM(s) for only the area of interest and to use that to download these files. The subset allows the user to limit the total amount of strips to be downloaded and processed. The script will create a new shapefile with the clipped subset of the master ArcticDEM strip file. \n\n\nMake sure you reproject your aoi shapefile to the same projection as the ArcticDEM strip file\n \n\n\nusage: arcticdem.py demaoi [-h] [--source SOURCE] [--target TARGET]\n                           [--output OUTPUT]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --source SOURCE  Choose location of your AOI shapefile\n  --target TARGET  Choose the location of the master ArcticDEM strip file\n  --output OUTPUT  Choose the location of the output shapefile based on your\n                   AOI\n\n\n\n\nAn example setup would be\n\n\narcticdem demaoi --source \nC:\\users\\aoi.shp\n --target \nC:\\users\\masterdem.shp\n --output \nC:\\users\\master_aoi.shp\n\n\n\n\n\nEstimate Download Size\n\n\nOne of the most common things you want to do is to know if the destination where you want to save these files has enough space before you begin the download process. This script allows you to query the total download size for your area of interest and the destination drive where you want to save the compressed files. It also recursively updates overall download size on the screen and print total size needed along with total download size in GB.\n\n\nusage: arcticdem.py demsize [-h] [--infile INFILE] [--path PATH]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --infile INFILE  Choose the clipped aoi file you clipped from demaoi\n                   tool[This is the subset of the master ArcticDEM Strip]\n  --path PATH      Choose the destination folder where you want your dem files\n                   to be saved[This checks available disk space]\n\n\n\n\nAn example setup would be\n\n\narcticdem demsize --infile \nC:\\users\\master_aoi.shp\n --path \nC:\\users\\ArcticDEM\n\n\n\n\n\nThe program might misbehave if the area of interest is extremely large or be sluggish in nature.\n\n\nDownload DEM\n\n\nWhat we were mainly interested after we know that we have enough space to download is to download the files. The script used a multi part download library to download the files quicker and in a more managed style to the destination given by the user.\n\n\nusage: arcticdem.py demdownload [-h] [--subset SUBSET]\n                                [--desination DESINATION]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --subset SUBSET       Choose the location of the output shapefile based on\n                        your AOI[You got this from demaoi tool]\n  --desination DESINATION\n                        Choose the destination where you want to download your\n                        files\n\n\n\n\nAn example setup would be\n\n\narcticdem demdownload --subset \nC:\\users\\master_aoi.shp\n --destination \nC:\\users\\ArcticDEM\n\n\n\n\n\nExtract DEM\n\n\nThis downloaded DEM files are tar or tar gz files and need to be extracted. The important thing to note is that the script retains the dem file, the matchtag file and the metadata text files in separate directories within the destination directory. \n\n\nusage: arcticdem.py demextract [-h] [--folder FOLDER]\n                               [--destination DESTINATION] [--action ACTION]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --folder FOLDER       Choose the download file where you downloaded your tar\n                        zipped files\n  --destination DESTINATION\n                        Choose the destination folder where you want your\n                        images and metadata files to be extracted\n  --action ACTION       Choose if you want your zipped files to be deleted\n                        post extraction \nyes\n|\nno\n\n\n\n\n\nAn example setup would be\n\n\narcticdem demdextract --folder \nC:\\users\\ArcticDEM\n --destination \nC:\\users\\ArcticDEM\\Extract\n --action \nyes\n\n\n\n\n\nMetadata Parsing for GEE\n\n\nOne of my key interest in working on the ArcticDEM parsing was to be able to upload this to Google Earth Engine(GEE). The metadata parser script allows you to parse all metadata into a combined csv file to be used to upload images to GEE. The manifest and upload tools are separate from this package tool and included in my gee_asset_manager_addon.\n\n\nusage: arcticdem.py demmeta [-h] [--folder FOLDER] [--metadata METADATA]\n                            [--error ERROR]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --folder FOLDER      Choose where you unzipped and extracted your DEM and\n                       metadata files\n  --metadata METADATA  Choose a path to the metadata file \nexample:\n                       users/desktop/metadata.csv\n\n  --error ERROR        Choose a path to the errorlog file \nexample:\n                       users/desktop/errorlog.csv\n\n\n\n\n\nAn example setup would be\n\n\narcticdem demmeta --folder \nC:\\users\\ArcticDEM\\Extract\\pgcmeta\n --metadata \nC:\\users\\arcticdem_metadata.csv\n --error \nC:\\users\\arcticdem_errorlog.csv\n\n\n\n\n\nChangelog\n\n\n[0.1.1] - 2017-08-12\n\n\nAdded\n\n\n\n\nCan now handle ogr input and includes instruction to project aoi in same projection as DEM strip.\n\n\nAdded the capability of skipping over already downloaded files and continues with left over downloads.\n\n\nCompleted recompiling executable to include changes.", 
            "title": "ArcticDEM-Batch-Pipeline"
        }, 
        {
            "location": "/projects/arcticdem_download/#arcticdem-batch-download-processing-tools", 
            "text": "ArcticDEM project was a joint project supported by both the National Geospatial-Intelligence Agency(NGA) and the National Science Foundation(NSF) with the idea of creating a high resolution and high quality digital surface model(DSM). The product is distributed free of cost as time-dependent DEM strips and is hosted as https links that a user can use to download each strip. As per their policy  The seamless terrain mosaic can be distributed without restriction.  The created product is a 2-by-2 meter elevation cells over an over of over 20 million square kilometers and uses digital globe stereo imagery to create these high resolution DSM. The method used for the 2m derivate is Surface Extraction with TIN-based Search-space Minimization(SETSM).  Based on their acknowledgements requests you can use Acknowledging PGC services(including data access)   Geospatial support for this work provided by the Polar Geospatial Center under NSF OPP awards 1043681   1559691.   Acknowledging DEMS created from the ArcticDEM project   DEMs provided by the Polar Geospatial Center under NSF OPP awards 1043681, 1559691 and 1542736.   You can find details on the background, scope and methods among other details  here \nA detailed acknowledgement link can be found  here  With this in mind and with the potential applications of using these toolsets there was a need to batch download the DEM files for your area of interest and to be able to extract, clean and process metadata. In all fairness this tool has a motive of extending this as an input to Google Earth Engine and hence the last tool which is the metadata parser is designed to create a metadata manifest in a csv file which GEE can understand and associate during asset upload.", 
            "title": "ArcticDEM Batch Download &amp; Processing Tools"
        }, 
        {
            "location": "/projects/arcticdem_download/#table-of-contents", 
            "text": "Installation  Getting started  Usage examples  Subset to AOI  Estimate Download Size  Download DEM  Extract DEM  Metadata Parsing for GEE", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/arcticdem_download/#installation", 
            "text": "We assume that you have installed the requirements files to install all the necessary packages and libraries required to use this tool. To install packages from the requirements.txt file you can simply use pip install -r requirements.txt . Remember that installation is an optional step and you can run this program by simply browsing to the pgcdem-cli file and typing  python arcticdem.py . One of the only other requirement for this tool is the Master Shapefile for all DEM footprints(make sure to use the most updated version which can be found  here )  This toolbox also uses some functionality from GDAL \nFor installing GDAL in Ubuntu  sudo add-apt-repository ppa:ubuntugis/ppa   sudo apt-get update\nsudo apt-get install gdal-bin  For Windows I found this  guide  from UCLA  To install  ArcticDEM Batch Download   Processing Tools:  git clone https://github.com/samapriya/ArcticDEM-Batch-Pipeline.git\ncd ArcticDEM-Batch-Pipeline   pip install .  This release also contains a windows installer which bypasses the need for you to have admin permission, it does however require you to have python in the system path meaning when you open up command prompt you should be able to type python and start it within the command prompt window. Post installation using the installer you can just call ppipe using the command prompt similar to calling python. Give it a go post installation type  arcticdem -h  The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. As a extra addon feature I have wrapped the cli into a Graphical User Interface(GUI) so some people will find it easier to use.  To install run  python setup.py develop or python setup.py install\n\nIn a linux distribution\nsudo python setup.py develop or sudo python setup.py install", 
            "title": "Installation"
        }, 
        {
            "location": "/projects/arcticdem_download/#getting-started", 
            "text": "To obtain help for a specific functionality, simply call it with  help \nswitch, e.g.:  arcticdem demextract -h . If you didn't install arcticdem, then you\ncan run it just by going to  arcticdem-cli  directory and running  python\narcticdem.py [arguments go here]  As usual, to print help   arcticdem -h :  ArcticDEM Batch Download   Processing Tools\npositional arguments:\n  { ,demaoi,demsize,demdownload,demextract,demmeta}\n                        ---------------------------------------\n                        -----Choose from ArcticDEM-Download Tools Below-----\n                        ---------------------------------------\n    demaoi              Allows user to subset Master ArcticDEM to their AOI\n    demsize             Allows users to estimate total download size and space\n                        left in your destination folder\n    demdownload         Allows users to batch download ArcticDEM Strips using\n                        aoi shapefile\n    demextract          Allows users to extract both image and metadata files\n                        from the zipped tar files\n    demmeta             Tool to process metadata files into CSV for all\n                        strips[For use with Google Earth Engine]\n\noptional arguments:\n  -h, --help            show this help message and exit", 
            "title": "Getting started"
        }, 
        {
            "location": "/projects/arcticdem_download/#subset-to-aoi", 
            "text": "The script clips the master ArcticDEM strip file to a smaller subset usgin an area of interest shapefile. This allows to get the strip DEM(s) for only the area of interest and to use that to download these files. The subset allows the user to limit the total amount of strips to be downloaded and processed. The script will create a new shapefile with the clipped subset of the master ArcticDEM strip file.   Make sure you reproject your aoi shapefile to the same projection as the ArcticDEM strip file    usage: arcticdem.py demaoi [-h] [--source SOURCE] [--target TARGET]\n                           [--output OUTPUT]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --source SOURCE  Choose location of your AOI shapefile\n  --target TARGET  Choose the location of the master ArcticDEM strip file\n  --output OUTPUT  Choose the location of the output shapefile based on your\n                   AOI  An example setup would be  arcticdem demaoi --source  C:\\users\\aoi.shp  --target  C:\\users\\masterdem.shp  --output  C:\\users\\master_aoi.shp", 
            "title": "Subset to AOI"
        }, 
        {
            "location": "/projects/arcticdem_download/#estimate-download-size", 
            "text": "One of the most common things you want to do is to know if the destination where you want to save these files has enough space before you begin the download process. This script allows you to query the total download size for your area of interest and the destination drive where you want to save the compressed files. It also recursively updates overall download size on the screen and print total size needed along with total download size in GB.  usage: arcticdem.py demsize [-h] [--infile INFILE] [--path PATH]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --infile INFILE  Choose the clipped aoi file you clipped from demaoi\n                   tool[This is the subset of the master ArcticDEM Strip]\n  --path PATH      Choose the destination folder where you want your dem files\n                   to be saved[This checks available disk space]  An example setup would be  arcticdem demsize --infile  C:\\users\\master_aoi.shp  --path  C:\\users\\ArcticDEM   The program might misbehave if the area of interest is extremely large or be sluggish in nature.", 
            "title": "Estimate Download Size"
        }, 
        {
            "location": "/projects/arcticdem_download/#download-dem", 
            "text": "What we were mainly interested after we know that we have enough space to download is to download the files. The script used a multi part download library to download the files quicker and in a more managed style to the destination given by the user.  usage: arcticdem.py demdownload [-h] [--subset SUBSET]\n                                [--desination DESINATION]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --subset SUBSET       Choose the location of the output shapefile based on\n                        your AOI[You got this from demaoi tool]\n  --desination DESINATION\n                        Choose the destination where you want to download your\n                        files  An example setup would be  arcticdem demdownload --subset  C:\\users\\master_aoi.shp  --destination  C:\\users\\ArcticDEM", 
            "title": "Download DEM"
        }, 
        {
            "location": "/projects/arcticdem_download/#extract-dem", 
            "text": "This downloaded DEM files are tar or tar gz files and need to be extracted. The important thing to note is that the script retains the dem file, the matchtag file and the metadata text files in separate directories within the destination directory.   usage: arcticdem.py demextract [-h] [--folder FOLDER]\n                               [--destination DESTINATION] [--action ACTION]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --folder FOLDER       Choose the download file where you downloaded your tar\n                        zipped files\n  --destination DESTINATION\n                        Choose the destination folder where you want your\n                        images and metadata files to be extracted\n  --action ACTION       Choose if you want your zipped files to be deleted\n                        post extraction  yes | no   An example setup would be  arcticdem demdextract --folder  C:\\users\\ArcticDEM  --destination  C:\\users\\ArcticDEM\\Extract  --action  yes", 
            "title": "Extract DEM"
        }, 
        {
            "location": "/projects/arcticdem_download/#metadata-parsing-for-gee", 
            "text": "One of my key interest in working on the ArcticDEM parsing was to be able to upload this to Google Earth Engine(GEE). The metadata parser script allows you to parse all metadata into a combined csv file to be used to upload images to GEE. The manifest and upload tools are separate from this package tool and included in my gee_asset_manager_addon.  usage: arcticdem.py demmeta [-h] [--folder FOLDER] [--metadata METADATA]\n                            [--error ERROR]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --folder FOLDER      Choose where you unzipped and extracted your DEM and\n                       metadata files\n  --metadata METADATA  Choose a path to the metadata file  example:\n                       users/desktop/metadata.csv \n  --error ERROR        Choose a path to the errorlog file  example:\n                       users/desktop/errorlog.csv   An example setup would be  arcticdem demmeta --folder  C:\\users\\ArcticDEM\\Extract\\pgcmeta  --metadata  C:\\users\\arcticdem_metadata.csv  --error  C:\\users\\arcticdem_errorlog.csv", 
            "title": "Metadata Parsing for GEE"
        }, 
        {
            "location": "/projects/arcticdem_download/#changelog", 
            "text": "", 
            "title": "Changelog"
        }, 
        {
            "location": "/projects/arcticdem_download/#011-2017-08-12", 
            "text": "", 
            "title": "[0.1.1] - 2017-08-12"
        }, 
        {
            "location": "/projects/arcticdem_download/#added", 
            "text": "Can now handle ogr input and includes instruction to project aoi in same projection as DEM strip.  Added the capability of skipping over already downloaded files and continues with left over downloads.  Completed recompiling executable to include changes.", 
            "title": "Added"
        }, 
        {
            "location": "/projects/jetstream_unofficial_cli/", 
            "text": "Jetstream Unofficial Addon for Atmosphere VM(s)\n\n\n\n\n\n\nJetstream provides researchers and students with a unique approach to cloud computing and data analysis on demand. Moving away from a job submission model which is more pervasice on High Performance Cluster computing this allows users to create user-friendly installs of \u201cvirtual machines\u201d which can then be custom installed with softwares and packages and even imaged further to make science replicable. The system in theory also allows a BYoOS (Bring your own Operating System) designed to further allow more flexibility while sharing the backbone of high performance compute and cloud storage infrastructure. The idea of imaging a system allows the user to not only share their data or analysis but their entire setup that allows them to run the analysis making it a plug and play model in terms of research replication and transference. As the NSF project proceeds further it allows users to apply for grants or allocation time on the machines and indeed renew these free of cost for researchers and students. \n\n\nFor my work looking at developing a Satellite Imagery Input Output (IO) pipeline I was allocated a Jetstream grant and while using a couple of these systems I always thought it would be easier to query number of instances and volumes running on my account, my burn rate and left over allocation over sometime. And something useful was the possibility to perform system actions such as shutdown(stop), restart, start, reboot among others programatically. It is with this idea that I approach the API backend at JetStream and built just a few tools to allows users to perform these actions without the need to log into their web browsers and allows user to call upon instance actions through a rough command line interface (CLI).\n\n\nI would love to thank Steve Gregory who is getting the Official API ready for release and Jeremy Fischer who have helped me in more ways and allowed me to keep asking questions and learn from them to build the unofficial api for Jetstream.\n\n\nThe Jetstream's mission and funding is supported by NSF and any and all citations are useful so please make sure to cite and citation information can be found at the end of this readme or use the \nlink\n\n\nNote: I have used this on the Jetstream-IU system but it should work on the TACC end as well and the information I decided to show per instance is based on what I used, there are much more information generated but the tool prints a subset\n\n\n\n\nTable of contents\n\n\n\n\nGetting started\n\n\nSave API Password as Credential\n\n\nQuery Current Instances\n\n\nQuery Current Volumes\n\n\nPerform Instance Actions\n\n\n\n\n\n\n\n\nGetting started\n\n\nTo get started you need an XSEDE account of a Jetstream Rapid Access Account and you can create one using instructions \nhere\n. Once into the system you will still need an allocation and the wiki page setup by the project explains these along with everything you can do step by step \nhere\n. Once you have a project allocation and create some instances and volumes you can query and perform instance actions.\n\n\nJust browse to the folder and perform a \npython jetstream.py -h\n:\n\n\nusage: jetstream.py [-h] { ,jskey,instance,volume,action} ...\n\nJetStream API Unofficial\n\npositional arguments:\n  { ,jskey,instance,volume,action}\n                        -------------------------------------------\n                        -----Choose from JetStream Tools Below-----\n                        -------------------------------------------\n    jskey               Allows you to save your JetStream API Password\n    instance            Allows users to print out all instance information\n    volume              Allows users to print out all volume information\n    action              Allows user to start, suspend,resume,reboot instance\n\noptional arguments:\n  -h, --help            show this help message and exit\n\n\n\n\n\nSave API Password as Credential\n\n\nThis tool allows the user to save the credential or password file into \nusers/.config/jetstream\n making sure that they are user specific and are not shared on a system resource or location. This password is not your XSEDE or Jetstream Rapid Access account password but a API password which has to be requested atleast for now. It uses a getpass implementation and write the password as a csv and the other tools first tries to read this and if not present asks for your password.\n\n\nusage: jetstream.py jskey [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\n\n\n\n\nQuery Current Instances\n\n\nAs the name stated this allows the user to query instances related to your account. This queries all instances for now and prints certain key parameters(these are a subset that I selected for my use) but can be easily modified in the code to print other information.\n\n\nusage: jetstream.py instance [-h] [--username USERNAME] [--password PASSWORD]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --username USERNAME  Jetstream API username\n  --password PASSWORD  Jetstream API password: \nOptional if you already saved\n                       jetstream key\n\n\n\n\n\nIncase you have already saved your password a setup would be simply\n\n\npython jetstream.py instance --username \njohndoe\n\n\n\n\n\nif not\n\n\npython jetstream.py instance --username \njohndoe\n --password \npass\n\n\n\n\n\nQuery Current Volumes\n\n\nThe current volume options queries current volumes on your project. Note that the current info does not tell you which instance a volume is attached to which might be a nice endpoint readout to have.\n\n\nusage: jetstream.py volume [-h] [--username USERNAME] [--password PASSWORD]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --username USERNAME  Jetstream API username\n  --password PASSWORD  Jetstream API password: \nOptional if you already saved\n                       jetstream key\n\n\n\n\n\nIncase you have already saved your password a setup would be simply\n\n\npython jetstream.py volume --username \njohndoe\n\n\n\n\n\nif not\n\n\npython jetstream.py volume --username \njohndoe\n --password \npass\n\n\n\n\n\nPerform Instance Actions\n\n\nThis is perhaps one of the most important aspect of building this tool, the idea was to get a read out or a email message when a system has been idle for sometime and then auto shutdown or initiate a shutdown remotely. This command allows you to choose the instance you want to perform an instance action using the instance ID mentioned on your instance profile page. The next thing you need to know is what kind of action you want to perform. There are certain things that will create a conflict, for example: If you are trying to shutdown an already shutoff system or start an already active system. In that case it will print out a conflict message.\n\n\nusage: jetstream.py action [-h] [--username USERNAME] [--password PASSWORD]\n                           [--id ID] [--action ACTION]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --username USERNAME  Jetstream API username\n  --password PASSWORD  Jetstream API password: \nOptional if you already saved\n                       jetstream key\n\n  --id ID              Jetstream Instance ID on your Instance Detail Page\n  --action ACTION      Jetstream Instance Action,\n                       \nstart|stop|suspend|resume|reboot\n\n\n\n\n\nIncase you have already saved your password a setup would be simply\n\n\npython jetstream.py action --username \njohndoe\n --id \n00000\n --action \nstart\n\n\n\n\n\nif not \n\n\npython jetstream.py action --username \njohndoe\n --password \npass\n --id \n00000\n --action \nstart\n\n\n\n\n\nI would like to thank my grant from JetStream TG-GEO160014. You can cite this tool as \n\n\nSamapriya Roy. (2017, August 16). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon. Zenodo. http://doi.org/10.5281/zenodo.844018\n\n\nAnd I would like to include Jetstream citations for others to use\n\n\nStewart, C.A., Cockerill, T.M., Foster, I., Hancock, D., Merchant, N., Skidmore, E., Stanzione, D., Taylor, J., Tuecke, S., Turner, G., Vaughn, M., and Gaffney, N.I., Jetstream: a self-provisioned, scalable science and engineering cloud environment. 2015, In Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure. St. Louis, Missouri.  ACM: 2792774.  p. 1-8. http://dx.doi.org/10.1145/2792745.2792774\n\n\nand\n\n\nJohn Towns, Timothy Cockerill, Maytal Dahan, Ian Foster, Kelly Gaither, Andrew Grimshaw, Victor Hazlewood, Scott Lathrop, Dave Lifka, Gregory D. Peterson, Ralph Roskies, J. Ray Scott, Nancy Wilkins-Diehr, \"XSEDE: Accelerating Scientific Discovery\", Computing in Science \n Engineering, vol.16, no. 5, pp. 62-74, Sept.-Oct. 2014, doi:10.1109/MCSE.2014.80", 
            "title": "Jetstream Unofficial Addon"
        }, 
        {
            "location": "/projects/jetstream_unofficial_cli/#jetstream-unofficial-addon-for-atmosphere-vms", 
            "text": "Jetstream provides researchers and students with a unique approach to cloud computing and data analysis on demand. Moving away from a job submission model which is more pervasice on High Performance Cluster computing this allows users to create user-friendly installs of \u201cvirtual machines\u201d which can then be custom installed with softwares and packages and even imaged further to make science replicable. The system in theory also allows a BYoOS (Bring your own Operating System) designed to further allow more flexibility while sharing the backbone of high performance compute and cloud storage infrastructure. The idea of imaging a system allows the user to not only share their data or analysis but their entire setup that allows them to run the analysis making it a plug and play model in terms of research replication and transference. As the NSF project proceeds further it allows users to apply for grants or allocation time on the machines and indeed renew these free of cost for researchers and students.   For my work looking at developing a Satellite Imagery Input Output (IO) pipeline I was allocated a Jetstream grant and while using a couple of these systems I always thought it would be easier to query number of instances and volumes running on my account, my burn rate and left over allocation over sometime. And something useful was the possibility to perform system actions such as shutdown(stop), restart, start, reboot among others programatically. It is with this idea that I approach the API backend at JetStream and built just a few tools to allows users to perform these actions without the need to log into their web browsers and allows user to call upon instance actions through a rough command line interface (CLI).  I would love to thank Steve Gregory who is getting the Official API ready for release and Jeremy Fischer who have helped me in more ways and allowed me to keep asking questions and learn from them to build the unofficial api for Jetstream.  The Jetstream's mission and funding is supported by NSF and any and all citations are useful so please make sure to cite and citation information can be found at the end of this readme or use the  link  Note: I have used this on the Jetstream-IU system but it should work on the TACC end as well and the information I decided to show per instance is based on what I used, there are much more information generated but the tool prints a subset", 
            "title": "Jetstream Unofficial Addon for Atmosphere VM(s)"
        }, 
        {
            "location": "/projects/jetstream_unofficial_cli/#table-of-contents", 
            "text": "Getting started  Save API Password as Credential  Query Current Instances  Query Current Volumes  Perform Instance Actions", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/jetstream_unofficial_cli/#getting-started", 
            "text": "To get started you need an XSEDE account of a Jetstream Rapid Access Account and you can create one using instructions  here . Once into the system you will still need an allocation and the wiki page setup by the project explains these along with everything you can do step by step  here . Once you have a project allocation and create some instances and volumes you can query and perform instance actions.  Just browse to the folder and perform a  python jetstream.py -h :  usage: jetstream.py [-h] { ,jskey,instance,volume,action} ...\n\nJetStream API Unofficial\n\npositional arguments:\n  { ,jskey,instance,volume,action}\n                        -------------------------------------------\n                        -----Choose from JetStream Tools Below-----\n                        -------------------------------------------\n    jskey               Allows you to save your JetStream API Password\n    instance            Allows users to print out all instance information\n    volume              Allows users to print out all volume information\n    action              Allows user to start, suspend,resume,reboot instance\n\noptional arguments:\n  -h, --help            show this help message and exit", 
            "title": "Getting started"
        }, 
        {
            "location": "/projects/jetstream_unofficial_cli/#save-api-password-as-credential", 
            "text": "This tool allows the user to save the credential or password file into  users/.config/jetstream  making sure that they are user specific and are not shared on a system resource or location. This password is not your XSEDE or Jetstream Rapid Access account password but a API password which has to be requested atleast for now. It uses a getpass implementation and write the password as a csv and the other tools first tries to read this and if not present asks for your password.  usage: jetstream.py jskey [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit", 
            "title": "Save API Password as Credential"
        }, 
        {
            "location": "/projects/jetstream_unofficial_cli/#query-current-instances", 
            "text": "As the name stated this allows the user to query instances related to your account. This queries all instances for now and prints certain key parameters(these are a subset that I selected for my use) but can be easily modified in the code to print other information.  usage: jetstream.py instance [-h] [--username USERNAME] [--password PASSWORD]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --username USERNAME  Jetstream API username\n  --password PASSWORD  Jetstream API password:  Optional if you already saved\n                       jetstream key   Incase you have already saved your password a setup would be simply  python jetstream.py instance --username  johndoe   if not  python jetstream.py instance --username  johndoe  --password  pass", 
            "title": "Query Current Instances"
        }, 
        {
            "location": "/projects/jetstream_unofficial_cli/#query-current-volumes", 
            "text": "The current volume options queries current volumes on your project. Note that the current info does not tell you which instance a volume is attached to which might be a nice endpoint readout to have.  usage: jetstream.py volume [-h] [--username USERNAME] [--password PASSWORD]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --username USERNAME  Jetstream API username\n  --password PASSWORD  Jetstream API password:  Optional if you already saved\n                       jetstream key   Incase you have already saved your password a setup would be simply  python jetstream.py volume --username  johndoe   if not  python jetstream.py volume --username  johndoe  --password  pass", 
            "title": "Query Current Volumes"
        }, 
        {
            "location": "/projects/jetstream_unofficial_cli/#perform-instance-actions", 
            "text": "This is perhaps one of the most important aspect of building this tool, the idea was to get a read out or a email message when a system has been idle for sometime and then auto shutdown or initiate a shutdown remotely. This command allows you to choose the instance you want to perform an instance action using the instance ID mentioned on your instance profile page. The next thing you need to know is what kind of action you want to perform. There are certain things that will create a conflict, for example: If you are trying to shutdown an already shutoff system or start an already active system. In that case it will print out a conflict message.  usage: jetstream.py action [-h] [--username USERNAME] [--password PASSWORD]\n                           [--id ID] [--action ACTION]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --username USERNAME  Jetstream API username\n  --password PASSWORD  Jetstream API password:  Optional if you already saved\n                       jetstream key \n  --id ID              Jetstream Instance ID on your Instance Detail Page\n  --action ACTION      Jetstream Instance Action,\n                        start|stop|suspend|resume|reboot   Incase you have already saved your password a setup would be simply  python jetstream.py action --username  johndoe  --id  00000  --action  start   if not   python jetstream.py action --username  johndoe  --password  pass  --id  00000  --action  start   I would like to thank my grant from JetStream TG-GEO160014. You can cite this tool as   Samapriya Roy. (2017, August 16). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon. Zenodo. http://doi.org/10.5281/zenodo.844018  And I would like to include Jetstream citations for others to use  Stewart, C.A., Cockerill, T.M., Foster, I., Hancock, D., Merchant, N., Skidmore, E., Stanzione, D., Taylor, J., Tuecke, S., Turner, G., Vaughn, M., and Gaffney, N.I., Jetstream: a self-provisioned, scalable science and engineering cloud environment. 2015, In Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure. St. Louis, Missouri.  ACM: 2792774.  p. 1-8. http://dx.doi.org/10.1145/2792745.2792774  and  John Towns, Timothy Cockerill, Maytal Dahan, Ian Foster, Kelly Gaither, Andrew Grimshaw, Victor Hazlewood, Scott Lathrop, Dave Lifka, Gregory D. Peterson, Ralph Roskies, J. Ray Scott, Nancy Wilkins-Diehr, \"XSEDE: Accelerating Scientific Discovery\", Computing in Science   Engineering, vol.16, no. 5, pp. 62-74, Sept.-Oct. 2014, doi:10.1109/MCSE.2014.80", 
            "title": "Perform Instance Actions"
        }, 
        {
            "location": "/release-notes/", 
            "text": "Release notes\n\n\n\n\n\n\n\n\nProject Name\n\n\nDigital Object Identification Number(DOI)\n\n\nRelease\n\n\n\n\n\n\n\n\n\n\nPlanet-GEE-Pipeline-CLI\n\n\n\n\n0.1.5\n\n\n\n\n\n\nPlanet-GEE-Pipeline-GUI\n\n\n\n\n0.1.4\n\n\n\n\n\n\nPlanet-Pipeline-GUI\n\n\n\n\n0.3\n\n\n\n\n\n\nGEE Asset Manager Addons\n\n\n\n\n0.1.6\n\n\n\n\n\n\nSynthetic LandScape Models\n\n\n\n\n0.1.1\n\n\n\n\n\n\nArcMap Addons\n\n\n\n\n0.1\n\n\n\n\n\n\nArcticDEM-Batch-Pipeline\n\n\n\n\n0.1.1\n\n\n\n\n\n\nJetstream-Unofficial-addon\n\n\n\n\n0.1.1", 
            "title": "Release notes"
        }, 
        {
            "location": "/release-notes/#release-notes", 
            "text": "Project Name  Digital Object Identification Number(DOI)  Release      Planet-GEE-Pipeline-CLI   0.1.5    Planet-GEE-Pipeline-GUI   0.1.4    Planet-Pipeline-GUI   0.3    GEE Asset Manager Addons   0.1.6    Synthetic LandScape Models   0.1.1    ArcMap Addons   0.1    ArcticDEM-Batch-Pipeline   0.1.1    Jetstream-Unofficial-addon   0.1.1", 
            "title": "Release notes"
        }, 
        {
            "location": "/license/", 
            "text": "Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n\n\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n\n\n\n\n\nDefinitions.\n\n\n\"License\" shall mean the terms and conditions for use, reproduction,\n  and distribution as defined by Sections 1 through 9 of this document.\n\n\n\"Licensor\" shall mean the copyright owner or entity authorized by\n  the copyright owner that is granting the License.\n\n\n\"Legal Entity\" shall mean the union of the acting entity and all\n  other entities that control, are controlled by, or are under common\n  control with that entity. For the purposes of this definition,\n  \"control\" means (i) the power, direct or indirect, to cause the\n  direction or management of such entity, whether by contract or\n  otherwise, or (ii) ownership of fifty percent (50%) or more of the\n  outstanding shares, or (iii) beneficial ownership of such entity.\n\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity\n  exercising permissions granted by this License.\n\n\n\"Source\" form shall mean the preferred form for making modifications,\n  including but not limited to software source code, documentation\n  source, and configuration files.\n\n\n\"Object\" form shall mean any form resulting from mechanical\n  transformation or translation of a Source form, including but\n  not limited to compiled object code, generated documentation,\n  and conversions to other media types.\n\n\n\"Work\" shall mean the work of authorship, whether in Source or\n  Object form, made available under the License, as indicated by a\n  copyright notice that is included in or attached to the work\n  (an example is provided in the Appendix below).\n\n\n\"Derivative Works\" shall mean any work, whether in Source or Object\n  form, that is based on (or derived from) the Work and for which the\n  editorial revisions, annotations, elaborations, or other modifications\n  represent, as a whole, an original work of authorship. For the purposes\n  of this License, Derivative Works shall not include works that remain\n  separable from, or merely link (or bind by name) to the interfaces of,\n  the Work and Derivative Works thereof.\n\n\n\"Contribution\" shall mean any work of authorship, including\n  the original version of the Work and any modifications or additions\n  to that Work or Derivative Works thereof, that is intentionally\n  submitted to Licensor for inclusion in the Work by the copyright owner\n  or by an individual or Legal Entity authorized to submit on behalf of\n  the copyright owner. For the purposes of this definition, \"submitted\"\n  means any form of electronic, verbal, or written communication sent\n  to the Licensor or its representatives, including but not limited to\n  communication on electronic mailing lists, source code control systems,\n  and issue tracking systems that are managed by, or on behalf of, the\n  Licensor for the purpose of discussing and improving the Work, but\n  excluding communication that is conspicuously marked or otherwise\n  designated in writing by the copyright owner as \"Not a Contribution.\"\n\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity\n  on behalf of whom a Contribution has been received by Licensor and\n  subsequently incorporated within the Work.\n\n\n\n\n\n\nGrant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n\n\n\n\n\nGrant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n\n\n\n\n\nRedistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n\n(a) You must give any other recipients of the Work or\n      Derivative Works a copy of this License; and\n\n\n(b) You must cause any modified files to carry prominent notices\n      stating that You changed the files; and\n\n\n(c) You must retain, in the Source form of any Derivative Works\n      that You distribute, all copyright, patent, trademark, and\n      attribution notices from the Source form of the Work,\n      excluding those notices that do not pertain to any part of\n      the Derivative Works; and\n\n\n(d) If the Work includes a \"NOTICE\" text file as part of its\n      distribution, then any Derivative Works that You distribute must\n      include a readable copy of the attribution notices contained\n      within such NOTICE file, excluding those notices that do not\n      pertain to any part of the Derivative Works, in at least one\n      of the following places: within a NOTICE text file distributed\n      as part of the Derivative Works; within the Source form or\n      documentation, if provided along with the Derivative Works; or,\n      within a display generated by the Derivative Works, if and\n      wherever such third-party notices normally appear. The contents\n      of the NOTICE file are for informational purposes only and\n      do not modify the License. You may add Your own attribution\n      notices within Derivative Works that You distribute, alongside\n      or as an addendum to the NOTICE text from the Work, provided\n      that such additional attribution notices cannot be construed\n      as modifying the License.\n\n\nYou may add Your own copyright statement to Your modifications and\n  may provide additional or different license terms and conditions\n  for use, reproduction, or distribution of Your modifications, or\n  for any such Derivative Works as a whole, provided Your use,\n  reproduction, and distribution of the Work otherwise complies with\n  the conditions stated in this License.\n\n\n\n\n\n\nSubmission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n\n\n\n\n\nTrademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n\n\n\n\n\nDisclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n\n\n\n\n\nLimitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n\n\n\n\n\nAccepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n\n\n\n\n\nEND OF TERMS AND CONDITIONS\n\n\nAPPENDIX: How to apply the Apache License to your work.\n\n\n  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"{}\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n\n\n\nCopyright {2017} {Samapriya Roy}\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n\n\nUnless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.", 
            "title": "License"
        }, 
        {
            "location": "/credits/", 
            "text": "Credits\n\n\n\n\nJetStream\n A portion of the work is suported by JetStream Grant TG-GEO160014.\n\n\nArcticDEM data provided by Polar Geospatial Center \nPGC\n\n\nAlso supported by \nPlanet Labs Ambassador Program", 
            "title": "Credits"
        }, 
        {
            "location": "/credits/#credits", 
            "text": "JetStream  A portion of the work is suported by JetStream Grant TG-GEO160014.  ArcticDEM data provided by Polar Geospatial Center  PGC  Also supported by  Planet Labs Ambassador Program", 
            "title": "Credits"
        }
    ]
}