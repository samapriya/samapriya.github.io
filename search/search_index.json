{
    "docs": [
        {
            "location": "/aboutme/", 
            "text": "My current work involves big data analysis and large scale data science applications utilizing geospatial technology. I work with different imagery types and I am currently involved with looking at time series data \n analysis of natural systems coupled with system modeling and periodicity. Most of my work includes but is not limited to remote sensing applications, large scale data processing and management, API support along with network analysis and geostatistical methods.\n\n\n\n\n\n\n\n\nContact me:\nroysam[at]indiana.edu", 
            "title": "About me"
        }, 
        {
            "location": "/cv/", 
            "text": "Research Specialization and Interest\n\n\n\n\nRemote Sensing and GIS, Urban systems,patterns and hydrology,Land Change Science\n\n\n\n\nEducation\n\n\n\n\n\n\n\n\nDegree\n\n\nUniversity\n\n\nYear\n\n\nResearch Interest or Thesis\n\n\n\n\n\n\n\n\n\n\nPhD Candidate\n\n\nIndiana University\n\n\nExpected 2018\n\n\nLand cover change modeling and system dynamics using large scale spatio-temporal data analysis\n\n\n\n\n\n\nMS Earth Sciencs\n\n\nIndiana University\n\n\n2013\n\n\nThesis: Remote sensing \n GIS applications for drainage detection and Modeling in agricultural watersheds\n\n\n\n\n\n\nB.Tech\n\n\nVNIT\n\n\n2010\n\n\nThesis: Social Network Analysis for Mapping Segmented Growth in Urban Cities in India.\n\n\n\n\n\n\n\n\nTechnical\n\n\n\n\nIntermediate experience with Javascript, Python, Shell Scripts\n\n\nExtensive experience with Google Earth Engine, ESRI ArcGIS, ERDAS IMAGINE, ENVI.\n\n\nExperience with QGIS, eCognition, MATLAB, Fragstats, Patch Analyst, SPSS, QUAL2Kw, SocNetV, Expert Choice 2000, GRASS, ILWIS.\n\n\n\n\nEmployment\n\n\n\n\nMay 2018- 2019: Senior Developer Advocate Intern, Planet Labs\n\n\n\n\n\n\nResponsibilites include Growing and supporting Planet\u2019s technical user communities and developing new analytical tools and tutorials. Teaching workshops and delivering conference talks to technologists in academic communities and to developers in the geospatial and cloud industries. Collaborating on remote sensing science, including primary research on the evolution, geomorphology, and long-term welfare of the world\u2019s coastal ecosystems.\n\n\n\n\n\n\nJanuary 2018- May 2019: Developer Advocate Intern, Planet Labs\n\n\n\n\n\n\nResponsibilities include growing and supporting user communities for Planet\u2019s Developer Center and the Education and Research Program.Developing new analytical tools, tutorials, and workshops for technical users of Planet data and tools.\n\n\n\n\n\n\n2017-2018: Research Assistant, Indiana University funded by National Science Foundation(NSF)\n\n\n\n\n\n\nCoastal SEES Collaborative Research: Changes in actual and perceived coastal flood risks due to river management strategies (NSF: 1426997). Partner-PI. National Science Foundation. Responsible for looking at land loss models and remote sensing application to coastal land loss. Includes model building and assessment along with hydrological model based vulnerability assessment of same area looking at landscape pattern and progress.\n\n\n\n\n\n\n2015-2016: Research Assistant, Indiana University funded by Deltas Belmont Forum and National Science Foundation(NSF)\n\n\n\n\n\n\nCatalyzing action towards sustainability of deltaic systems with an integrated modeling framework for risk assessment (DELTAS: 1342946). Partner-PI. National Science Foundation, Belmont Forum. Partner PI [International collaborative network of 24 institutions] (2015-2016) Responsible for looking at physical models of vulnerability and risk assessment in deltaic areas and among populated regions within Brazil. Includes model building and assessment along with hydrological model based vulnerability assessment of same area.\n\n\n\n\n\n\n2013-2014: Reseach Assistant , Indiana University funded by National Aeronautics and Space Administration(NASA)\n\n\n\n\n\n\nNational Aeronautics and Space Administration(NASA: NNX11AF50G) (01/01/2013\u201312/31/2014) Carbon Dynamics, Land Cover Change, and Vulnerability of the World's Largest Coastal Mangrove Ecosystem to Climate Change, with Dr. Rinku Roy Chowdhury. Responsibilities includes review of existing datasets and literature. Evaluation of existing LULC classes using Landsat ETM+, and Geoeye classified raster datasets. Assessment of existing census and socio ecological data.\n\n\n\n\n\n\n2013-2015: Research Assistant, Indiana University funded by National Science Foundation(NSF)\n\n\n\n\n\n\nNational Science Foundation (NSF: 1065785) (07/01/2013\u201306/30/2015).  Collaborative  Research: Ecological Homogenization of Urban America, with Dr. Rinku Roy Chowdhury Responsibilities include analysis of land cover data at census block group and parcel levels using landscape metrics and analysis of land cover heterogeneity or homogeneity at both levels. This includes developing batch models for running large datasets and creating consistent data and analytical output for all 6 urban sites in the project.\n\n\n\n\n\n\n2011-2012: Research Assistant, Indiana University Purdue University at Indinapolis funded by United States Department of Agriculture \n Natural Resource Conservation(USDA \n NRCS)\n\n\n\n\n\n\nUnited States Department of Agriculture \n Natural Resource Conservation (USDA \n NRCS: 68-52KY-11-058) project for Creating and Automating Potential Polygon location and Site Characterization for Upland Storage in Watersheds, using Arc Map. A CTI (Compound Topographic Index) based Toolbox was developed using Model Builder (\u00ae ESRI) as a part of project deliverables to automate location of these potential sites.\n(September 2011- August 2012)\n\n\n\n\n\n\nJanuary 2011-June 2011: Research Assistant, Indian Institute of Technology (IIT) funder by Ministry of Environment and Forests\n\n\n\n\n\n\nResearch Associate with Water Resource Management Group at IIT Kanpur, for the Ganga River Basin Management Plan (GRBMP), under Ministry of Environment and Forests Responsibilities: Included assessment of the hydrology of the Upper Ganga Basin and working on flow simulation and scenario development for flow calculation. Organization: Indian Institute of Technology (IIT), Kanpur, January to June 2011.\n\n\n\n\n\n\nJune 2010- August 2010: Research Assistant, Indiana University funded by National Science Foundation(NSF)\n\n\n\n\n\n\nResearch Intern at the Core Geospatial and Utilities(CGO) business Unit Responsibilities at Risk Management Solution in India (RMSI), Dehradun: Created a characteristic model called SMCPM (Scenario Modeling with Catchment Priority Model) which utilizes the SCN method accompanied with SMART and MAUT utilization for user based criterion input and priority output. 22nd June to 21st August 2010\n\\newpage\n\n\n\n\nPeer-Reviewed Journal Articles\n\n\n\n\n\n\nRoy, Samapriya., Robeson, Scott., Mansur, A. V., Brondizio, Eduardo., Spatial Clustering using Multiplex Geo-constrained Networks in Amazon River Delta Under Preparation\n\n\n\n\n\n\nRoy, Samapriya., Robeson, Scott., Ortiz, Alejandra., Edmonds, Douglas Edmonds \u201cDecadal Changes in Mississippi Delta Morphology: Analyzing Landscape Patterns using Satellite Time Series Data\u201d Under preparation\n\n\n\n\n\n\nOrtiz, A. C., S. Roy, and D. A. Edmonds (2017), Land loss by pond expansion on the Mississippi River Delta Plain, Geophys. Res. Lett., 44, doi:10.1002/2017GL073079.\n\n\n\n\n\n\nMansur, Andressa Vianna, Eduardo Sonnewend Brondizio, Samapriya Roy, Pedro Paulo de Miranda Ara\u00fajo Soares, and Alice Newton. \"Adapting to urban challenges in the Amazon: flood risk and infrastructure deficiencies in Bel\u00e9m, Brazil.\" Regional Environmental Change (2017): 1-16.\n\n\n\n\n\n\nMansur, Andressa V., Eduardo S. Brond\u00edzio, Samapriya Roy, Scott Hetrick, Nathan D. Vogt, and Alice Newton. \"An assessment of urban vulnerability in the Amazon Delta and Estuary: a multi-criterion index of flood exposure, socio-economic conditions and infrastructure.\" Sustainability Science (2016): 1-19.\n\n\n\n\n\n\nRoy, Samapriya, and Katpatal,Y.B (2011) Cyclical Hierarchical Modeling for Water Quality Model based DSS Module in an urban river system, Journal of Environmental Engineering, ASCE. Vol. 137, Number 12, 1176-1184.\n\n\n\n\n\n\nTools \n Products\n\n\n\n\n\n\nGoogle Earth Engine Account Transfer Tool,Samapriya Roy. (2018, February 27). \nsamapriya/gee-takeout: Google Earth Engine Account Transfer Tool\n (Version 0.1). Zenodo. http://doi.org/10.5281/zenodo.1185158\n\n\n\n\n\n\nClip-Ship-Planet-CLI,Samapriya Roy. (2017, December 20). \nsamapriya/Clip-Ship-Planet-CLI: Clip-Ship-Batch Planet Command Line Interface\n (Version 0.2.1). Zenodo. http://doi.org/10.5281/zenodo.1119192\n\n\n\n\n\n\nSlack-Notifier-CLI-Addon,Samapriya Roy. (2017, September 5). \nsamapriya/Slack-Notifier-CLI-Addon: Slack Notifier-CLI Addon\n (Version 0.1.0). Zenodo. http://doi.org/10.5281/zenodo.885505\n\n\n\n\n\n\nPlanet-GEE-Pipeline-CLI,Samapriya Roy. (2018, March 8). \nsamapriya/Planet-GEE-Pipeline-CLI: Planet-GEE-Pipeline-CLI\n (Version 0.2.1). Zenodo. http://doi.org/10.5281/zenodo.1194323\n\n\n\n\n\n\nPlanet-GEE-Pipeline-GUI,Samapriya Roy. (2017, June 25). \nsamapriya/Planet-GEE-Pipeline-GUI: Planet-GEE-Pipeline-GUI\n (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.817739\n\n\n\n\n\n\nPlanet-Pipeline-GUI,Samapriya Roy. (2017, August 17). \nsamapriya/Planet-Pipeline-GUI: Planet-Pipeline-GUI\n (Version 0.3). Zenodo. http://doi.org/10.5281/zenodo.844149\nGEE Asset Manager Addons,Samapriya Roy. (2018, March 8). \nsamapriya/gee_asset_manager_addon: GEE Asset Manager with Addons\n (Version 0.2.3). Zenodo. http://doi.org/10.5281/zenodo.1194308\n\n\n\n\n\n\nArcMap Addons,Samapriya Roy. (2017, October 12). \nsamapriya/arcmap-addons: ArcMap Addons\n (Version 0.2). Zenodo. http://doi.org/10.5281/zenodo.1009210\nArcticDEM-Batch-Pipeline,Samapriya Roy. (2018, May 3). \nsamapriya/ArcticDEM-Batch-Pipeline: ArcticDEM-Batch-Pipeline\n (Version 0.1.2). Zenodo. http://doi.org/10.5281/zenodo.1240456\n\n\n\n\n\n\nJetstream-Unofficial-addon,Samapriya Roy. (2018, March 12). \nsamapriya/jetstream-unofficial-addon: jetstream-unofficial-addon\n (Version 0.1.2). Zenodo. http://doi.org/10.5281/zenodo.1196653\n\n\n\n\n\n\nPlanet-Batch-Slack-CLI,Samapriya Roy. (2017, December 4). \nsamapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI\n (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.1079887\n\n\n\n\n\n\nArticles, Blogposts and Publications\n\n\n\n\n\n\nRoy,Samapriya, \n\"Google Earth Engine Takeout: Tools and Guide for Code and Asset Transfer\"\n, 4th December 2017, \nRead Here\n\n\n\n\n\n\nRoy,Samapriya, \n\"Talk Slack to Me: Integrating Planet and Slack API for Automation \n Batch Notifications\"\n, 4th December 2017, \nRead Here\n\n\n\n\n\n\nRoy,Samapriya, \n\"Baking API Clients in a Raspberry Pi: Planet and Earth Engine in a Box\"\n, 22nd November 2017, \nRead Here \n\n\n\n\n\n\nRoy, Samapriya, \n\"Planet, People and Pixels: A Data Pipeline to link Planet API to Google Earth Engine\"\n, 10 July 2017, \nRead Here\n\n\n\n\n\n\nRoy Samapriya, \n\"Clip and Ship: Batch Clips using Planet\u2019s Clips API\"\n, 15 September 2017,\nRead Here\n\n\n\n\n\n\nRoy,Samapriya, \n\"Google Earth Engine Asset Manager and Addons: Building Tools of the Trade\"\n, 19 October 2017, \nRead Here\n\n\n\n\n\n\nInvited Talks and Trainings\n\n\n\n\n\n\nStanford Big Earth Hackathon April 14-15th 2018, Stanford University: Invited to coordiante use of \nPlanet data and API mechanics to formulate and work on Earth Sciences and big data problems\n with students as Developer advocate intern. \nGithub Link\n\n\n\n\n\n\nCSDMS 2018 Annual Meeting, May 22 -24th 2018, Boulder Colorado, USA: Organized Clinic \nIntroduction to Google Earth Engine\n \nGithub Link\n\n\n\n\n\n\nAmerican Geophysical Union Fall Meeting 2017: Invited Talk: \nEarth Science in Real Time with the Planet SmallSat Constellation.\n December 11-15th 2017\n\n\n\n\n\n\nConference presentations, Trainings and Attendance\n\n\n\n\n\n\nCyverse Container Camp 7th to 9th March: Container Technology for Scientific Research Introduction to Docker and Singularity images in High Performance Computing environments.\n\n\n\n\n\n\nAmerican Geophysical Union Annual Meeting 2017(AGU 2017) Spatial and Temporal Patterns of Land Loss in Mississipi River Delta. December 11-15th 2017. New Orleans, Louisiana\n\n\n\n\n\n\nPolar Geospatial BootCamp 2017: Focus on looking at high resolution satellite data along with using stereoscopic method to create high resolution digital elevation model. August 7-10th 2017, Minneapolis, Minnesota\n\n\n\n\n\n\nGoogle Earth Engine Summit 2017: Focus on methodology and large scale data analysis. Mountain View, California. June 12-14th 2017.\n\n\n\n\n\n\nIndiana Geographic Information Council Conference 2017.  Deep Time Stack Analysis of Coastal Land loss: Case Study of Mississippi Delta using Earth Engine. May 9-11th May 2017\n\n\n\n\n\n\nEdmonds,Douglas A., Caldwell,Rebecca L., Baumgardner,Sarah, Paola,Chris, Roy ,Samapriya, Nelson,Amelia: A global analysis of human habitation on river deltas. European Geosciences Union General Assembly, Vienna, 23-28th April 2017\n\n\n\n\n\n\nRoy, Samapriya., Edmonds, Douglas., Visualizing land loss across high spatio-temporal scales across Louisiana Coast, Planet Labs Headquarters December 2016.\n\n\n\n\n\n\nDeltas Belmont Forum End of Project Meeting August 2016: Large Scale data analysis and visualization application to specific delta cases along with urban land cover change.\n\n\n\n\n\n\nGoogle Earth Engine Summit 2016: Focus on methodology and large scale data analysis. Mountain View, California. June 13-16th 2016.\n\n\n\n\n\n\nCommunity Surface Dynamics Modeling System (CSDMS) Annual Meeting, A Composite Vulnerability for Urban Areas in Deltaic Regions: An application in the Amazon Delta, Colorado. 17-19th May 2016.\n\n\n\n\n\n\nIndiana Geographic Information Council Conference 2016.  Locally \n Globally Applied Classification Algorithms for Urban Land Cover Detection using Earth Engine. May 9-11th May 2016.\n\n\n\n\n\n\nSummer Training at the United States Army Core of Engineers (USACE) and South Florida Water Management District (SFWMD) joint Modeling Group (2015): South Florida Water Management Model, West Palm Beach, 26th July to 31st July 2015.\n\n\n\n\n\n\nRoy, Samapriya (2015) Hydrological Modeling for studying impacts of Urbanization: A cross site assessment at Community Surface Dynamics Modeling System (CSDMS) Annual Meeting, Colorado. 26-28nd May 2015\n\n\n\n\n\n\nRoy, Samapriya et.al (2015) Urban Watersheds and urbanizing hydrology: Assessment through dynamic modeling, Oral presentation at the Association of American Geographers(AAG) Annual Meeting, April 21-April 25th, 2015\n\n\n\n\n\n\nRoy, Samapriya, Roy Chowdhury, Rinku and Ficklin, Darren (2015) Dynamic Hydrological Modeling using SWAT within Florida Coastal Everglades. Florida Coastal Everglades LTER Mid Term Review Meeting, March 11-13, 2015.\n\n\n\n\n\n\nRoy, Samapriya, Darren L. Ficklin, Rinku Roy Chowdhury, Scott Robeson, Jarlath O Neil Dunn, James B Heffernan, Meredith K Steele, Peter M Groffman (2014) Dynamic Assessment of Urban Hydrologic Component using SWAT. International Long Term Ecological Research (ILTER): All Scientist Meeting of the Americas, Valdivia, Chile, December 1-3 , 2014\n\n\n\n\n\n\nField Research Experience\n\n\n\n\nSummer 2014 (05/15/2014-06/18/2014). Field research in Bangladesh: Conducted 140 household surveys, with data collection aimed at assessing socio-economic-political drivers of land use and livelihood vulnerability in 4 villages of the Sundarbans mangrove forest region.\n\n\n\n\nGrants and Fellowship\n\n\n\n\nCollege of Arts and Sciences Graduate Student Travel Award for attending Google Earth Engine Summit 2017 $200 April 2017.\n\n\nAwarded the Graduate \n Professional Student Government Travel Award for attending Google Earth Engine Summit 2017 $500 April 2017.\n\n\nAwarded the John Odland Graduate Research Fellowship to support graduate research $500 March 2017\n\n\nAwarded the Lester Spicer Department of Geography Poster Award Fellowship to support graduate research $250 March 2017\n\n\nAwarded the William R. Black Leadership Memorial Fellowship for $500 March 2017\n\n\nCo-PI on Extreme Science and Engineering Discovery Environment (XSEDE) Allocation Grant along with Douglas Edmonds for 50,000 Super-computing Units and 500 GB storage volume for project \"Automated API based Pipeline for high volume satellite data\", grant number TG-GEO160014 at Indiana University.\n\n\nCo PI on the Planet Labs Ambassador program providing us unprecedented daily satellite data for entire Louisiana coast an estimated value of over 40,000 square kilometer and estimated access to over 500,000 square kilometer daily (approximately) maximum access value considering RapidEye is (approx. 500,000 x $1.28/sqkm = $640,000)\n\n\nDigital Globe Imagery Grant for an area of over 1500 square kilometer (estimated value at $25,500) April 2016.\n\n\nAwarded the John Odland Graduate Research Fellowship to support graduate research $500 March 2016.\n\n\nAwarded the IndianaView Student Scholarship Program to participate and conduct research $750 March 2016\n\n\nAwarded the Partners of America (POA) 100,000 Strong award, for $1000 towards travel to the ILTER All Scientist Meeting of the Americas, Valdivia, Chile December 2014\n\n\nAwarded the ILTER Travel Award, for $500 towards travel to the ILTER All Scientist Meeting of the Americas, Valdivia, Chile December 2014\n\n\n\n\nTeaching Experience\n\n\n\n\nLead Instructor, G338: Introduction to Geographic Information Systems, Summer 2017, Indiana University, Bloomington\n\n\nLead  Instructor,  G237: Mapping  our  World, Spring  2017, Indiana University, Bloomington\n\n\nLead  Instructor,  G237: Mapping  our  World, Fall  2016, Indiana University, Bloomington\n\n\nTeaching Assistant, G237: Mapping  our  World, Spring 2016, Indiana University, Bloomington\n\n\nLead  Instructor,  G237: Mapping  our  World, Fall 2015, Indiana University, Bloomington\n\n\nGuest Lecture I202 Lecture Topic: Spatial Epidemiology September 25th 2014 at Indiana University, Bloomington\n\n\nTeaching Assistant: Environmental Geology Course, G117 Spring 2013, Indiana University Purdue University, Indianapolis\n\n\nTeaching Assistant: Environmental Geology Course, G117 Fall 2012, Indiana University Purdue University, Indianapolis\n\n\nTeaching Assistant: Environmental Geology Course, G117 Spring 2012, Indiana University Purdue University, Indianapolis\n\n\n\n\nMemberships and committees\n\n\n\n\nCo-Chair for GIS Day Day at Bloomington, Indiana University, 2016\n\n\nCollege Committee on Graduate Education, Graduate Student Representative (2015-2016)\n\n\nPlanning Committee for GIS Day at Bloomington, Indiana University 2015.\n\n\nStudent Member, American Society of Civil Engineers (ASCE)    2008-present\n\n\nEnvironmental and Water Resources Institute (EWRI)    2010-present\n\n\nStudent Member, American Association of Geographer (AAG)  2014-present\n\n\n\n\nCertifications and Trainings\n\n\n\n\nCollaborative Institutional Training Initiative (CITI) Human Research 2014\n\n\nTrimble Geospatial Training: eCognition- analysis strategies August 14th- 15th, 2014\n\n\n\n\nReferences\n\n\nAvailable on request.", 
            "title": "Current CV"
        }, 
        {
            "location": "/cv/#research-specialization-and-interest", 
            "text": "Remote Sensing and GIS, Urban systems,patterns and hydrology,Land Change Science", 
            "title": "Research Specialization and Interest"
        }, 
        {
            "location": "/cv/#education", 
            "text": "Degree  University  Year  Research Interest or Thesis      PhD Candidate  Indiana University  Expected 2018  Land cover change modeling and system dynamics using large scale spatio-temporal data analysis    MS Earth Sciencs  Indiana University  2013  Thesis: Remote sensing   GIS applications for drainage detection and Modeling in agricultural watersheds    B.Tech  VNIT  2010  Thesis: Social Network Analysis for Mapping Segmented Growth in Urban Cities in India.", 
            "title": "Education"
        }, 
        {
            "location": "/cv/#technical", 
            "text": "Intermediate experience with Javascript, Python, Shell Scripts  Extensive experience with Google Earth Engine, ESRI ArcGIS, ERDAS IMAGINE, ENVI.  Experience with QGIS, eCognition, MATLAB, Fragstats, Patch Analyst, SPSS, QUAL2Kw, SocNetV, Expert Choice 2000, GRASS, ILWIS.", 
            "title": "Technical"
        }, 
        {
            "location": "/cv/#employment", 
            "text": "May 2018- 2019: Senior Developer Advocate Intern, Planet Labs    Responsibilites include Growing and supporting Planet\u2019s technical user communities and developing new analytical tools and tutorials. Teaching workshops and delivering conference talks to technologists in academic communities and to developers in the geospatial and cloud industries. Collaborating on remote sensing science, including primary research on the evolution, geomorphology, and long-term welfare of the world\u2019s coastal ecosystems.    January 2018- May 2019: Developer Advocate Intern, Planet Labs    Responsibilities include growing and supporting user communities for Planet\u2019s Developer Center and the Education and Research Program.Developing new analytical tools, tutorials, and workshops for technical users of Planet data and tools.    2017-2018: Research Assistant, Indiana University funded by National Science Foundation(NSF)    Coastal SEES Collaborative Research: Changes in actual and perceived coastal flood risks due to river management strategies (NSF: 1426997). Partner-PI. National Science Foundation. Responsible for looking at land loss models and remote sensing application to coastal land loss. Includes model building and assessment along with hydrological model based vulnerability assessment of same area looking at landscape pattern and progress.    2015-2016: Research Assistant, Indiana University funded by Deltas Belmont Forum and National Science Foundation(NSF)    Catalyzing action towards sustainability of deltaic systems with an integrated modeling framework for risk assessment (DELTAS: 1342946). Partner-PI. National Science Foundation, Belmont Forum. Partner PI [International collaborative network of 24 institutions] (2015-2016) Responsible for looking at physical models of vulnerability and risk assessment in deltaic areas and among populated regions within Brazil. Includes model building and assessment along with hydrological model based vulnerability assessment of same area.    2013-2014: Reseach Assistant , Indiana University funded by National Aeronautics and Space Administration(NASA)    National Aeronautics and Space Administration(NASA: NNX11AF50G) (01/01/2013\u201312/31/2014) Carbon Dynamics, Land Cover Change, and Vulnerability of the World's Largest Coastal Mangrove Ecosystem to Climate Change, with Dr. Rinku Roy Chowdhury. Responsibilities includes review of existing datasets and literature. Evaluation of existing LULC classes using Landsat ETM+, and Geoeye classified raster datasets. Assessment of existing census and socio ecological data.    2013-2015: Research Assistant, Indiana University funded by National Science Foundation(NSF)    National Science Foundation (NSF: 1065785) (07/01/2013\u201306/30/2015).  Collaborative  Research: Ecological Homogenization of Urban America, with Dr. Rinku Roy Chowdhury Responsibilities include analysis of land cover data at census block group and parcel levels using landscape metrics and analysis of land cover heterogeneity or homogeneity at both levels. This includes developing batch models for running large datasets and creating consistent data and analytical output for all 6 urban sites in the project.    2011-2012: Research Assistant, Indiana University Purdue University at Indinapolis funded by United States Department of Agriculture   Natural Resource Conservation(USDA   NRCS)    United States Department of Agriculture   Natural Resource Conservation (USDA   NRCS: 68-52KY-11-058) project for Creating and Automating Potential Polygon location and Site Characterization for Upland Storage in Watersheds, using Arc Map. A CTI (Compound Topographic Index) based Toolbox was developed using Model Builder (\u00ae ESRI) as a part of project deliverables to automate location of these potential sites.\n(September 2011- August 2012)    January 2011-June 2011: Research Assistant, Indian Institute of Technology (IIT) funder by Ministry of Environment and Forests    Research Associate with Water Resource Management Group at IIT Kanpur, for the Ganga River Basin Management Plan (GRBMP), under Ministry of Environment and Forests Responsibilities: Included assessment of the hydrology of the Upper Ganga Basin and working on flow simulation and scenario development for flow calculation. Organization: Indian Institute of Technology (IIT), Kanpur, January to June 2011.    June 2010- August 2010: Research Assistant, Indiana University funded by National Science Foundation(NSF)    Research Intern at the Core Geospatial and Utilities(CGO) business Unit Responsibilities at Risk Management Solution in India (RMSI), Dehradun: Created a characteristic model called SMCPM (Scenario Modeling with Catchment Priority Model) which utilizes the SCN method accompanied with SMART and MAUT utilization for user based criterion input and priority output. 22nd June to 21st August 2010\n\\newpage", 
            "title": "Employment"
        }, 
        {
            "location": "/cv/#peer-reviewed-journal-articles", 
            "text": "Roy, Samapriya., Robeson, Scott., Mansur, A. V., Brondizio, Eduardo., Spatial Clustering using Multiplex Geo-constrained Networks in Amazon River Delta Under Preparation    Roy, Samapriya., Robeson, Scott., Ortiz, Alejandra., Edmonds, Douglas Edmonds \u201cDecadal Changes in Mississippi Delta Morphology: Analyzing Landscape Patterns using Satellite Time Series Data\u201d Under preparation    Ortiz, A. C., S. Roy, and D. A. Edmonds (2017), Land loss by pond expansion on the Mississippi River Delta Plain, Geophys. Res. Lett., 44, doi:10.1002/2017GL073079.    Mansur, Andressa Vianna, Eduardo Sonnewend Brondizio, Samapriya Roy, Pedro Paulo de Miranda Ara\u00fajo Soares, and Alice Newton. \"Adapting to urban challenges in the Amazon: flood risk and infrastructure deficiencies in Bel\u00e9m, Brazil.\" Regional Environmental Change (2017): 1-16.    Mansur, Andressa V., Eduardo S. Brond\u00edzio, Samapriya Roy, Scott Hetrick, Nathan D. Vogt, and Alice Newton. \"An assessment of urban vulnerability in the Amazon Delta and Estuary: a multi-criterion index of flood exposure, socio-economic conditions and infrastructure.\" Sustainability Science (2016): 1-19.    Roy, Samapriya, and Katpatal,Y.B (2011) Cyclical Hierarchical Modeling for Water Quality Model based DSS Module in an urban river system, Journal of Environmental Engineering, ASCE. Vol. 137, Number 12, 1176-1184.", 
            "title": "Peer-Reviewed Journal Articles"
        }, 
        {
            "location": "/cv/#tools-products", 
            "text": "Google Earth Engine Account Transfer Tool,Samapriya Roy. (2018, February 27).  samapriya/gee-takeout: Google Earth Engine Account Transfer Tool  (Version 0.1). Zenodo. http://doi.org/10.5281/zenodo.1185158    Clip-Ship-Planet-CLI,Samapriya Roy. (2017, December 20).  samapriya/Clip-Ship-Planet-CLI: Clip-Ship-Batch Planet Command Line Interface  (Version 0.2.1). Zenodo. http://doi.org/10.5281/zenodo.1119192    Slack-Notifier-CLI-Addon,Samapriya Roy. (2017, September 5).  samapriya/Slack-Notifier-CLI-Addon: Slack Notifier-CLI Addon  (Version 0.1.0). Zenodo. http://doi.org/10.5281/zenodo.885505    Planet-GEE-Pipeline-CLI,Samapriya Roy. (2018, March 8).  samapriya/Planet-GEE-Pipeline-CLI: Planet-GEE-Pipeline-CLI  (Version 0.2.1). Zenodo. http://doi.org/10.5281/zenodo.1194323    Planet-GEE-Pipeline-GUI,Samapriya Roy. (2017, June 25).  samapriya/Planet-GEE-Pipeline-GUI: Planet-GEE-Pipeline-GUI  (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.817739    Planet-Pipeline-GUI,Samapriya Roy. (2017, August 17).  samapriya/Planet-Pipeline-GUI: Planet-Pipeline-GUI  (Version 0.3). Zenodo. http://doi.org/10.5281/zenodo.844149\nGEE Asset Manager Addons,Samapriya Roy. (2018, March 8).  samapriya/gee_asset_manager_addon: GEE Asset Manager with Addons  (Version 0.2.3). Zenodo. http://doi.org/10.5281/zenodo.1194308    ArcMap Addons,Samapriya Roy. (2017, October 12).  samapriya/arcmap-addons: ArcMap Addons  (Version 0.2). Zenodo. http://doi.org/10.5281/zenodo.1009210\nArcticDEM-Batch-Pipeline,Samapriya Roy. (2018, May 3).  samapriya/ArcticDEM-Batch-Pipeline: ArcticDEM-Batch-Pipeline  (Version 0.1.2). Zenodo. http://doi.org/10.5281/zenodo.1240456    Jetstream-Unofficial-addon,Samapriya Roy. (2018, March 12).  samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon  (Version 0.1.2). Zenodo. http://doi.org/10.5281/zenodo.1196653    Planet-Batch-Slack-CLI,Samapriya Roy. (2017, December 4).  samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI  (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.1079887", 
            "title": "Tools &amp; Products"
        }, 
        {
            "location": "/cv/#articles-blogposts-and-publications", 
            "text": "Roy,Samapriya,  \"Google Earth Engine Takeout: Tools and Guide for Code and Asset Transfer\" , 4th December 2017,  Read Here    Roy,Samapriya,  \"Talk Slack to Me: Integrating Planet and Slack API for Automation   Batch Notifications\" , 4th December 2017,  Read Here    Roy,Samapriya,  \"Baking API Clients in a Raspberry Pi: Planet and Earth Engine in a Box\" , 22nd November 2017,  Read Here     Roy, Samapriya,  \"Planet, People and Pixels: A Data Pipeline to link Planet API to Google Earth Engine\" , 10 July 2017,  Read Here    Roy Samapriya,  \"Clip and Ship: Batch Clips using Planet\u2019s Clips API\" , 15 September 2017, Read Here    Roy,Samapriya,  \"Google Earth Engine Asset Manager and Addons: Building Tools of the Trade\" , 19 October 2017,  Read Here", 
            "title": "Articles, Blogposts and Publications"
        }, 
        {
            "location": "/cv/#invited-talks-and-trainings", 
            "text": "Stanford Big Earth Hackathon April 14-15th 2018, Stanford University: Invited to coordiante use of  Planet data and API mechanics to formulate and work on Earth Sciences and big data problems  with students as Developer advocate intern.  Github Link    CSDMS 2018 Annual Meeting, May 22 -24th 2018, Boulder Colorado, USA: Organized Clinic  Introduction to Google Earth Engine   Github Link    American Geophysical Union Fall Meeting 2017: Invited Talk:  Earth Science in Real Time with the Planet SmallSat Constellation.  December 11-15th 2017", 
            "title": "Invited Talks and Trainings"
        }, 
        {
            "location": "/cv/#conference-presentations-trainings-and-attendance", 
            "text": "Cyverse Container Camp 7th to 9th March: Container Technology for Scientific Research Introduction to Docker and Singularity images in High Performance Computing environments.    American Geophysical Union Annual Meeting 2017(AGU 2017) Spatial and Temporal Patterns of Land Loss in Mississipi River Delta. December 11-15th 2017. New Orleans, Louisiana    Polar Geospatial BootCamp 2017: Focus on looking at high resolution satellite data along with using stereoscopic method to create high resolution digital elevation model. August 7-10th 2017, Minneapolis, Minnesota    Google Earth Engine Summit 2017: Focus on methodology and large scale data analysis. Mountain View, California. June 12-14th 2017.    Indiana Geographic Information Council Conference 2017.  Deep Time Stack Analysis of Coastal Land loss: Case Study of Mississippi Delta using Earth Engine. May 9-11th May 2017    Edmonds,Douglas A., Caldwell,Rebecca L., Baumgardner,Sarah, Paola,Chris, Roy ,Samapriya, Nelson,Amelia: A global analysis of human habitation on river deltas. European Geosciences Union General Assembly, Vienna, 23-28th April 2017    Roy, Samapriya., Edmonds, Douglas., Visualizing land loss across high spatio-temporal scales across Louisiana Coast, Planet Labs Headquarters December 2016.    Deltas Belmont Forum End of Project Meeting August 2016: Large Scale data analysis and visualization application to specific delta cases along with urban land cover change.    Google Earth Engine Summit 2016: Focus on methodology and large scale data analysis. Mountain View, California. June 13-16th 2016.    Community Surface Dynamics Modeling System (CSDMS) Annual Meeting, A Composite Vulnerability for Urban Areas in Deltaic Regions: An application in the Amazon Delta, Colorado. 17-19th May 2016.    Indiana Geographic Information Council Conference 2016.  Locally   Globally Applied Classification Algorithms for Urban Land Cover Detection using Earth Engine. May 9-11th May 2016.    Summer Training at the United States Army Core of Engineers (USACE) and South Florida Water Management District (SFWMD) joint Modeling Group (2015): South Florida Water Management Model, West Palm Beach, 26th July to 31st July 2015.    Roy, Samapriya (2015) Hydrological Modeling for studying impacts of Urbanization: A cross site assessment at Community Surface Dynamics Modeling System (CSDMS) Annual Meeting, Colorado. 26-28nd May 2015    Roy, Samapriya et.al (2015) Urban Watersheds and urbanizing hydrology: Assessment through dynamic modeling, Oral presentation at the Association of American Geographers(AAG) Annual Meeting, April 21-April 25th, 2015    Roy, Samapriya, Roy Chowdhury, Rinku and Ficklin, Darren (2015) Dynamic Hydrological Modeling using SWAT within Florida Coastal Everglades. Florida Coastal Everglades LTER Mid Term Review Meeting, March 11-13, 2015.    Roy, Samapriya, Darren L. Ficklin, Rinku Roy Chowdhury, Scott Robeson, Jarlath O Neil Dunn, James B Heffernan, Meredith K Steele, Peter M Groffman (2014) Dynamic Assessment of Urban Hydrologic Component using SWAT. International Long Term Ecological Research (ILTER): All Scientist Meeting of the Americas, Valdivia, Chile, December 1-3 , 2014", 
            "title": "Conference presentations, Trainings and Attendance"
        }, 
        {
            "location": "/cv/#field-research-experience", 
            "text": "Summer 2014 (05/15/2014-06/18/2014). Field research in Bangladesh: Conducted 140 household surveys, with data collection aimed at assessing socio-economic-political drivers of land use and livelihood vulnerability in 4 villages of the Sundarbans mangrove forest region.", 
            "title": "Field Research Experience"
        }, 
        {
            "location": "/cv/#grants-and-fellowship", 
            "text": "College of Arts and Sciences Graduate Student Travel Award for attending Google Earth Engine Summit 2017 $200 April 2017.  Awarded the Graduate   Professional Student Government Travel Award for attending Google Earth Engine Summit 2017 $500 April 2017.  Awarded the John Odland Graduate Research Fellowship to support graduate research $500 March 2017  Awarded the Lester Spicer Department of Geography Poster Award Fellowship to support graduate research $250 March 2017  Awarded the William R. Black Leadership Memorial Fellowship for $500 March 2017  Co-PI on Extreme Science and Engineering Discovery Environment (XSEDE) Allocation Grant along with Douglas Edmonds for 50,000 Super-computing Units and 500 GB storage volume for project \"Automated API based Pipeline for high volume satellite data\", grant number TG-GEO160014 at Indiana University.  Co PI on the Planet Labs Ambassador program providing us unprecedented daily satellite data for entire Louisiana coast an estimated value of over 40,000 square kilometer and estimated access to over 500,000 square kilometer daily (approximately) maximum access value considering RapidEye is (approx. 500,000 x $1.28/sqkm = $640,000)  Digital Globe Imagery Grant for an area of over 1500 square kilometer (estimated value at $25,500) April 2016.  Awarded the John Odland Graduate Research Fellowship to support graduate research $500 March 2016.  Awarded the IndianaView Student Scholarship Program to participate and conduct research $750 March 2016  Awarded the Partners of America (POA) 100,000 Strong award, for $1000 towards travel to the ILTER All Scientist Meeting of the Americas, Valdivia, Chile December 2014  Awarded the ILTER Travel Award, for $500 towards travel to the ILTER All Scientist Meeting of the Americas, Valdivia, Chile December 2014", 
            "title": "Grants and Fellowship"
        }, 
        {
            "location": "/cv/#teaching-experience", 
            "text": "Lead Instructor, G338: Introduction to Geographic Information Systems, Summer 2017, Indiana University, Bloomington  Lead  Instructor,  G237: Mapping  our  World, Spring  2017, Indiana University, Bloomington  Lead  Instructor,  G237: Mapping  our  World, Fall  2016, Indiana University, Bloomington  Teaching Assistant, G237: Mapping  our  World, Spring 2016, Indiana University, Bloomington  Lead  Instructor,  G237: Mapping  our  World, Fall 2015, Indiana University, Bloomington  Guest Lecture I202 Lecture Topic: Spatial Epidemiology September 25th 2014 at Indiana University, Bloomington  Teaching Assistant: Environmental Geology Course, G117 Spring 2013, Indiana University Purdue University, Indianapolis  Teaching Assistant: Environmental Geology Course, G117 Fall 2012, Indiana University Purdue University, Indianapolis  Teaching Assistant: Environmental Geology Course, G117 Spring 2012, Indiana University Purdue University, Indianapolis", 
            "title": "Teaching Experience"
        }, 
        {
            "location": "/cv/#memberships-and-committees", 
            "text": "Co-Chair for GIS Day Day at Bloomington, Indiana University, 2016  College Committee on Graduate Education, Graduate Student Representative (2015-2016)  Planning Committee for GIS Day at Bloomington, Indiana University 2015.  Student Member, American Society of Civil Engineers (ASCE)    2008-present  Environmental and Water Resources Institute (EWRI)    2010-present  Student Member, American Association of Geographer (AAG)  2014-present", 
            "title": "Memberships and committees"
        }, 
        {
            "location": "/cv/#certifications-and-trainings", 
            "text": "Collaborative Institutional Training Initiative (CITI) Human Research 2014  Trimble Geospatial Training: eCognition- analysis strategies August 14th- 15th, 2014", 
            "title": "Certifications and Trainings"
        }, 
        {
            "location": "/cv/#references", 
            "text": "Available on request.", 
            "title": "References"
        }, 
        {
            "location": "/", 
            "text": "Introduction\n\n\nThese github projects are a collection of tools and improvements made to accomplish tasks as well as potential tools and applications designed for future applications. While moving between assets from \nPlanet Labs\n and \nGoogle Earth Engine\n it was imperative to create a pipeline that allows for easy transitions between the two service end points and hence tools were designed to link these two nodes and create effective pipelines. Since this required me to interact with Earth Engine I further developed addon tools the ambition of which is to helping users with batch actions on assets along with interacting and extending capabilities of existing GEE CLI.\n\n\n \nProjects Include work on following Platforms\n \n\n\n\n\nTwo projects included looking at different aspects of raster analysis and landscape modeling and hence I decided to create an ArcMap toolbox with tools that I have developed and continue to develop further. The toolbox can be downloaded and added to existing toolbox to create additional functionalities. The synthetic landscape models are designed to try different randomizing and clustering algorithms. An additional project was completed during the \nPolar Geospatial Bootcamp\n which allows users to bulk download and process 2m high resolution ArcticDEM provided by the Polar Geospatial Center. The list now also includes a notifier application using the Slack interface along with including an application of the Clips API from Planet.\n\n\n\n\n\n\n\n\n\n\nProject Name\n\n\nDigital Object Identification Number(DOI)\n\n\n\n\n\n\n\n\n\n\nGoogle Earth Engine Takeout\n\n\n\n\n\n\n\n\nPlanet-Batch-Slack-Pipeline\n\n\n\n\n\n\n\n\nClip-Ship-Planet-CLI\n\n\n\n\n\n\n\n\nSlack-Notifier-CLI-Addon\n\n\n\n\n\n\n\n\nPlanet-GEE-Pipeline-CLI\n\n\n\n\n\n\n\n\nPlanet-GEE-Pipeline-GUI\n\n\n\n\n\n\n\n\nPlanet-Pipeline-GUI\n\n\n\n\n\n\n\n\nGEE Asset Manager Addons\n\n\n\n\n\n\n\n\nSynthetic LandScape Models\n\n\n\n\n\n\n\n\nArcMap Addons\n\n\n\n\n\n\n\n\nArcticDEM-Batch-Pipeline\n\n\n\n\n\n\n\n\njetstream-unofficial-addon", 
            "title": "Introduction"
        }, 
        {
            "location": "/#introduction", 
            "text": "These github projects are a collection of tools and improvements made to accomplish tasks as well as potential tools and applications designed for future applications. While moving between assets from  Planet Labs  and  Google Earth Engine  it was imperative to create a pipeline that allows for easy transitions between the two service end points and hence tools were designed to link these two nodes and create effective pipelines. Since this required me to interact with Earth Engine I further developed addon tools the ambition of which is to helping users with batch actions on assets along with interacting and extending capabilities of existing GEE CLI.    Projects Include work on following Platforms     Two projects included looking at different aspects of raster analysis and landscape modeling and hence I decided to create an ArcMap toolbox with tools that I have developed and continue to develop further. The toolbox can be downloaded and added to existing toolbox to create additional functionalities. The synthetic landscape models are designed to try different randomizing and clustering algorithms. An additional project was completed during the  Polar Geospatial Bootcamp  which allows users to bulk download and process 2m high resolution ArcticDEM provided by the Polar Geospatial Center. The list now also includes a notifier application using the Slack interface along with including an application of the Clips API from Planet.      Project Name  Digital Object Identification Number(DOI)      Google Earth Engine Takeout     Planet-Batch-Slack-Pipeline     Clip-Ship-Planet-CLI     Slack-Notifier-CLI-Addon     Planet-GEE-Pipeline-CLI     Planet-GEE-Pipeline-GUI     Planet-Pipeline-GUI     GEE Asset Manager Addons     Synthetic LandScape Models     ArcMap Addons     ArcticDEM-Batch-Pipeline     jetstream-unofficial-addon", 
            "title": "Introduction"
        }, 
        {
            "location": "/pypi/", 
            "text": "PyPI Releases\n\n\nI have successfully converted some of the github projects into PyPI projects for allowing easier instalaltion and usage. Some of these have been tested for compatibility with Google Colaboratory and for building within a Docker image. Currently maintained pypi projects include\n\n\n\n\nPlanet API Pipeline \n Google Earth Engine Batch Assets Manager with Addons\n\n\nThe tool allows the user to upload Planet assets to Google Earth Engine from your local system. The tool has been modified and tested in Docker environment and Colab environments. It makes use of the manifest feature of image upload so that it can choose image and asset type appropriately. It also figures out property type automatically to avoid conflict in metadata property type in successive images. You can read more about the \nproject here\n\n\n\n\npip install ppipe\n\n\n\n\n\n\nPlanet Clip-Ship Tools CLI\n\n\nThe \nClips API has been deprecated\n and will no longer be supporting the standalone Clip and Ship service. Deprecation means that there is intention to remove the service at some point; it DOES NOT mean end-of-life for the service, yet. (Deprecate and end-of-life are two distinct terms.) There should be a plan to replace Clips API with new pre-processing functionality, but no new announcement or timetable for the launch of the new service and the removal (end-of-life) for Clips API has been made. Planet\u2019s Clip API was a compute API designed to allowed users to clip the images to their area of interest. This would save them time in preprocessing and also allow the user to save on their area quota which might have restrictions. You can read more about the \nproject here\n\n\n\n\npip install pclip\n\n\n\n\n\n\nGoogle Earth Engine Batch Assets Manager with Addons\n\n\nGoogle Earth Engine Batch Asset Manager with Addons is an extension of the existing CLI and additional tools were added to include functionality that was missing from the EarthEngine CLI. This includes printing quota, moving and batch copying images, print asset report and task lists to name a few of the features. It is developed case by case basis to include more features in the future as it becomes available or as need arises.You can read more about the \nproject here\n\n\n\n\npip install geeadd\n\n\n\n\n\n\nGoogle Takeout and Transfer: Tools and Guide for Code and Asset Transfer\n\n\nThis tool replicates a Google Earth Engine account and transfer everything over to a new account. Even if you are not replicating your account, think of this as an add on which allows you to download your entire code repositories, create an asset report and best of all iteratively change permissions to all image-collection and images whether or not within a folder.You can read more about the \nproject here\n\n\n\n\npip install geetakeout", 
            "title": "PyPI Projects"
        }, 
        {
            "location": "/pypi/#pypi-releases", 
            "text": "I have successfully converted some of the github projects into PyPI projects for allowing easier instalaltion and usage. Some of these have been tested for compatibility with Google Colaboratory and for building within a Docker image. Currently maintained pypi projects include", 
            "title": "PyPI Releases"
        }, 
        {
            "location": "/pypi/#planet-api-pipeline-google-earth-engine-batch-assets-manager-with-addons", 
            "text": "The tool allows the user to upload Planet assets to Google Earth Engine from your local system. The tool has been modified and tested in Docker environment and Colab environments. It makes use of the manifest feature of image upload so that it can choose image and asset type appropriately. It also figures out property type automatically to avoid conflict in metadata property type in successive images. You can read more about the  project here   pip install ppipe", 
            "title": "Planet API Pipeline &amp; Google Earth Engine Batch Assets Manager with Addons"
        }, 
        {
            "location": "/pypi/#planet-clip-ship-tools-cli", 
            "text": "The  Clips API has been deprecated  and will no longer be supporting the standalone Clip and Ship service. Deprecation means that there is intention to remove the service at some point; it DOES NOT mean end-of-life for the service, yet. (Deprecate and end-of-life are two distinct terms.) There should be a plan to replace Clips API with new pre-processing functionality, but no new announcement or timetable for the launch of the new service and the removal (end-of-life) for Clips API has been made. Planet\u2019s Clip API was a compute API designed to allowed users to clip the images to their area of interest. This would save them time in preprocessing and also allow the user to save on their area quota which might have restrictions. You can read more about the  project here   pip install pclip", 
            "title": "Planet Clip-Ship Tools CLI"
        }, 
        {
            "location": "/pypi/#google-earth-engine-batch-assets-manager-with-addons", 
            "text": "Google Earth Engine Batch Asset Manager with Addons is an extension of the existing CLI and additional tools were added to include functionality that was missing from the EarthEngine CLI. This includes printing quota, moving and batch copying images, print asset report and task lists to name a few of the features. It is developed case by case basis to include more features in the future as it becomes available or as need arises.You can read more about the  project here   pip install geeadd", 
            "title": "Google Earth Engine Batch Assets Manager with Addons"
        }, 
        {
            "location": "/pypi/#google-takeout-and-transfer-tools-and-guide-for-code-and-asset-transfer", 
            "text": "This tool replicates a Google Earth Engine account and transfer everything over to a new account. Even if you are not replicating your account, think of this as an add on which allows you to download your entire code repositories, create an asset report and best of all iteratively change permissions to all image-collection and images whether or not within a folder.You can read more about the  project here   pip install geetakeout", 
            "title": "Google Takeout and Transfer: Tools and Guide for Code and Asset Transfer"
        }, 
        {
            "location": "/projects/gee_takeout/", 
            "text": "Google Takeout and Transfer: Tools and Guide for Code and Asset Transfer\n\n\n\n\n\n\nCommand Line Interface Allows you to copy all codes and assets from one Google account to another\n\n\nNote: This is something that I have tested and have designed only for a windows machine with python 2.7.14 but can be easily ported into an different operating system. Use these tool and steps at your own risk and backup your scripts always just in case.\n\n\nIf you still want to proceed which I assume you do in case you are still reading, I am including descriptions links to the tool I made and the steps I used to achieve the same. The tool is a single command line interface with three sections.\n\n\nBefore you do this make sure of a few things\n\n\n\n\nBoth your google accounts have an external password, since it requires that to download and perform a lot of the operations. Also enable \n__Let Less Secure App use your account_\n on both these accounts._\n\n\nYour system has native python available in terminal or command prompt depending on what kind of system you are using. You can check this by typing\n \npython --version\n\n\n_Git is installed on your system. For windows you can find \n__installation here_\n\n\n_Earth Engine Command Line(earthengine cli) interface is installed, instructions are in the \n__developer page_\n\n\nYou have authenticated earthengine cli using \nearthengine authenticate\n\n\nMake sure you visit the \n__git source for your account_\n within earth engine and allow access._\n\n\nCheck git is accessible via your system path type\n \ngit help\n \nand check if the system can reach installed git command line tools.\n\n\n\n\nNow we setup and install the tool by running the following set of steps. The Github repository containing this tool and codes \ncan be found here\n\n\nFor windows \ndownload the zip here\n and after extraction go to the folder containing \"setup.py\" and open command prompt at that location and type\n\n\npython setup.py install  \npip install -r requirements.txt\n\n\n\nNow call the tool for the first time, by typing in \ngeetakeout -h\n. This will only work if you have python in the system path which you can test for opening up terminal or command prompt and typing \n**python**.\n If you want, you don't have to install the tool to use it you can browse to the folder geetakeout inside the main zipped folder. You can then call the tool by simple\n\n\npython geetakeout.py -h\n\n\nThe housekeeping and credential setup is optional since most of you have probably installed the earthengine cli and authenticated it using \n**earthengine authenticate**.\n\n\n \nAnatomy of the Process: How to transfer step by step\n\n\nGetting first things out of the way is to understand the three sections of this tool. To make life and this process simpler I designed the tool to have a flow so you can run these tools one after the other. The \nEE Setup and Housekeeping\n sections are optional\n, since I will generally updated the selenium driver for mozilla and it assumes you have authenticated your earthengine CLI\n. The tool might show an error if you have not authenticated using earthengine authenticate\n\n\nIf you have installed the tool run\ngeetakeout -h\n\nIf you have migrated into the folder\npython geetakeout.py -h\n\n\n\n\n\nThe GEE Takeout Tool CLI\nprintout\n\n\nSetting up the case study\n\n\nFor this blog I decided to make the transfer simple I have a university account but since my university if shifting umail services to a google app service it means my domain would change from @umail.iu.edu to @iu.edu which are separate accounts. I created the iu.edu GEE account recently.\n\n\n\n\nCode Editor comparison Left(my @umail account and right my @iu\naccount)\n\n\nThis also means that the root path for my home folder and repository are different. The idea is simple to be able to replicate the codes and assets from one account to the other. This includes every type if assets including collections but also making sure that the structure of the folders are same. That being said you will still have to change the home path in the codes but if the structure is same then only a single root-path changes.\n\n\n\n\nComparing the root path and assets folder for both\naccounts\n\n\nSo now that we have a setup, I am going to approach it step by step and have a walk through to explain the process better.\n\n\nStep 1: Getting your Repository Lists(gee_repo)\n\n\nThis assumes that you have visited the Git Source for your codes in Google Earth Engine and authorized it. If not \nallow it here\n and then you are set to download your repo contents and perform git operations. The tool is setup for accessing all repositories that are shared with you. This downloads the list into an html file which can then be parsed for your repositories.\n\n\n\n\nCreate GEE Repo\nList\n\n\nStep 2: Setting up your Git with Earth Engine Credentials(git_auth)\n\n\nYou can do this using two methods\n\n\n\n\nThe first simple includes you visiting your \ngitsource account page \nthat we accessed earlier and click \n**Generate Password **\nand follow instructions.\n\n\n\n\nI am going to talk more about the second method because this eliminates the need to get the password again and again since it is save as passkey. This will authenticate your git client with your git password using a browser less login and also store your gitkey\n\n\n\n\nGIT Auth (saves git\nkey)\n\n\nWe will use this again to setup our second account post authorization. This will print our gitkey location and make sure you copy that so you can swap in out as needed. Note the name of the key is in the format \n**git-\"username\"**\n \nin this case it is \n**git-roysam**\n\n\nStep 3: Authorize your Git Client with Git Key(git_swap)\n\n\nThe next step is to use the saved gitkey to authorize the git client. We are setting everything up so that we can clone the repositories to which we have access.\n\n\n\n\nAuthorize using the git key stored\nearlier\n\n\nStep 4: Clone your repositories(git_clone)\n\n\nThis tool makes use of your earlier created GIT list, now that your git client has been authorized in step 2, you should be able to download your repos. This tool uses the account already linked to your terminal account. If you are not sure try \n**earthengine ls**\n to see your username. The export path is noted for the collection of repositories.\n\n\n\n\nCloning your Git Repositories\n\n\nStep 5: Working with Assets: Generating Asset Report(ee_report)\n\n\nThis includes all your assets\n, including tables, images, image collections and folders. We need to make sure we have this list to work on copying over your assets to the secondary account. Running this is simple and just requires a location for the csv file (the full path).\n\n\n\n\nRunning Earth Engine\nReports\n\n\nThe output is a csv file consiting of the type of asset and the asset path to be replicated in the new account. And now that we have the list time to get permissions to copy these assets.\n\n\nStep 6: Setting Permissions to Assets(ee_permissions)\n\n\nWe now use the report file generated to grant read access to all assets in your account. Once this is completed you will be able to copy your assets apart from being able to copy your codes.\n\n\n\n\nGetting permissions to\nassets\n\n\nLet us Begin to Copy\n: We change gears and switch over to the destination account\n\n\nStep 7: Setting up the Destination Account(ee_user and git_swap)\n\n\nNow we have to do two steps one after the other, do a quick earthengine authenticate and authenticate to your new account and perform Step 2 and Step 3 this time using your new account. The tool \n**ee_user**\n will also allow you to change your accounts. I already created Step 2 for my secondary account and now I will use that to authorize my git client with the new account.\n\n\n\n\nChange your earthengine authentication and also validate your git\nkey\n\n\n\n\nNow we authorized the git client with the second\nkey\n\n\nStep 8: Replicate Repositories (git_create)\n\n\nTo setup our new account we need to build the outline of the earlier account, the repolists and folders inside these repos and then similary the folders and empty collections in the secondary account.\n\n\nNote: Git cannot push an empty repository so if you have an empty repository delete it before downloading and pushing to new\naccount\n\n\n\n\nGit Create your folder based on your earlier\naccount\n\n\nThe repo lists now look similar\n\n\n\n\nRepo created on secondary account\n\n\nStep 9: Push to New Account(git_replicate)\n\n\nNow we push all codes from our earlier account to our new account, this way our repositories will now be populated with the most recent codes.\n\n\n\"Do not push to any repository that already has code because this will overwrite it\"\n\n\n\n\ngit_replicate to new\naccount\n\n\nStep 10: Replicate Asset Structure(asset_create) and Assets (asset_replicate)\n\n\nThis is similar to git_create here we replicate the collection and folder structure so we can push our assets to them. You pass it the original report you created from your primary account and it sets up as needed.\n\n\n\n\nCreates asset structure (folders and collections)\n\n\nThis has replicated the collection and Image folder structures.\n\n\n\n\nAsset Collections and Folders have been created(Left: asset home before asset_create Right: asset home after asset_create)\n\n\nHowever this is still empty and the last step makes sure that your assets are actually copied over to your new asset home. I have included a counter to measure transfers left incase it is a large collection.\n\n\n\n\nAsset Replication: Copying assets to your home\nfolder\n\n\nThe final results is your assets and codes all copied, you will still have to edit codes to change your path as needed but for now we have replicated an Earth Engine account into a new location.\n\n\n\n\nReplicated Assets on Both Accounts Copied from Left to\nRight\n\n\nThere you go, over the last 10 steps we have managed to replicate and move an earth engine account from one place to another. Though I found this useful to move accounts within a university setting, I see some value in moving accounts and replicating when a project member leaves a project or for simply migrating at large. For now if an owner of an account deletes his/her account or looses access to his/her account and even if you are a writer to the repository and the collection, you will loose access to these codes and assets. So this can aid in maintaining continuity by moving codes to more persistent account.\n\n\nThough I have not tested this tools in a linux setting, these setup tools can be adapted and used easily in that framework, since I have tested the individual components in such setups.", 
            "title": "Google Earth Engine Takeout"
        }, 
        {
            "location": "/projects/gee_takeout/#google-takeout-and-transfer-tools-and-guide-for-code-and-asset-transfer", 
            "text": "Command Line Interface Allows you to copy all codes and assets from one Google account to another  Note: This is something that I have tested and have designed only for a windows machine with python 2.7.14 but can be easily ported into an different operating system. Use these tool and steps at your own risk and backup your scripts always just in case.  If you still want to proceed which I assume you do in case you are still reading, I am including descriptions links to the tool I made and the steps I used to achieve the same. The tool is a single command line interface with three sections.  Before you do this make sure of a few things   Both your google accounts have an external password, since it requires that to download and perform a lot of the operations. Also enable  __Let Less Secure App use your account_  on both these accounts._  Your system has native python available in terminal or command prompt depending on what kind of system you are using. You can check this by typing   python --version  _Git is installed on your system. For windows you can find  __installation here_  _Earth Engine Command Line(earthengine cli) interface is installed, instructions are in the  __developer page_  You have authenticated earthengine cli using  earthengine authenticate  Make sure you visit the  __git source for your account_  within earth engine and allow access._  Check git is accessible via your system path type   git help   and check if the system can reach installed git command line tools.   Now we setup and install the tool by running the following set of steps. The Github repository containing this tool and codes  can be found here  For windows  download the zip here  and after extraction go to the folder containing \"setup.py\" and open command prompt at that location and type  python setup.py install  \npip install -r requirements.txt  Now call the tool for the first time, by typing in  geetakeout -h . This will only work if you have python in the system path which you can test for opening up terminal or command prompt and typing  **python**.  If you want, you don't have to install the tool to use it you can browse to the folder geetakeout inside the main zipped folder. You can then call the tool by simple  python geetakeout.py -h  The housekeeping and credential setup is optional since most of you have probably installed the earthengine cli and authenticated it using  **earthengine authenticate**.    Anatomy of the Process: How to transfer step by step  Getting first things out of the way is to understand the three sections of this tool. To make life and this process simpler I designed the tool to have a flow so you can run these tools one after the other. The  EE Setup and Housekeeping  sections are optional , since I will generally updated the selenium driver for mozilla and it assumes you have authenticated your earthengine CLI . The tool might show an error if you have not authenticated using earthengine authenticate  If you have installed the tool run\ngeetakeout -h\n\nIf you have migrated into the folder\npython geetakeout.py -h   The GEE Takeout Tool CLI printout  Setting up the case study  For this blog I decided to make the transfer simple I have a university account but since my university if shifting umail services to a google app service it means my domain would change from @umail.iu.edu to @iu.edu which are separate accounts. I created the iu.edu GEE account recently.   Code Editor comparison Left(my @umail account and right my @iu account)  This also means that the root path for my home folder and repository are different. The idea is simple to be able to replicate the codes and assets from one account to the other. This includes every type if assets including collections but also making sure that the structure of the folders are same. That being said you will still have to change the home path in the codes but if the structure is same then only a single root-path changes.   Comparing the root path and assets folder for both accounts  So now that we have a setup, I am going to approach it step by step and have a walk through to explain the process better.  Step 1: Getting your Repository Lists(gee_repo)  This assumes that you have visited the Git Source for your codes in Google Earth Engine and authorized it. If not  allow it here  and then you are set to download your repo contents and perform git operations. The tool is setup for accessing all repositories that are shared with you. This downloads the list into an html file which can then be parsed for your repositories.   Create GEE Repo List  Step 2: Setting up your Git with Earth Engine Credentials(git_auth)  You can do this using two methods   The first simple includes you visiting your  gitsource account page  that we accessed earlier and click  **Generate Password ** and follow instructions.   I am going to talk more about the second method because this eliminates the need to get the password again and again since it is save as passkey. This will authenticate your git client with your git password using a browser less login and also store your gitkey   GIT Auth (saves git key)  We will use this again to setup our second account post authorization. This will print our gitkey location and make sure you copy that so you can swap in out as needed. Note the name of the key is in the format  **git-\"username\"**   in this case it is  **git-roysam**  Step 3: Authorize your Git Client with Git Key(git_swap)  The next step is to use the saved gitkey to authorize the git client. We are setting everything up so that we can clone the repositories to which we have access.   Authorize using the git key stored earlier  Step 4: Clone your repositories(git_clone)  This tool makes use of your earlier created GIT list, now that your git client has been authorized in step 2, you should be able to download your repos. This tool uses the account already linked to your terminal account. If you are not sure try  **earthengine ls**  to see your username. The export path is noted for the collection of repositories.   Cloning your Git Repositories  Step 5: Working with Assets: Generating Asset Report(ee_report)  This includes all your assets , including tables, images, image collections and folders. We need to make sure we have this list to work on copying over your assets to the secondary account. Running this is simple and just requires a location for the csv file (the full path).   Running Earth Engine Reports  The output is a csv file consiting of the type of asset and the asset path to be replicated in the new account. And now that we have the list time to get permissions to copy these assets.  Step 6: Setting Permissions to Assets(ee_permissions)  We now use the report file generated to grant read access to all assets in your account. Once this is completed you will be able to copy your assets apart from being able to copy your codes.   Getting permissions to assets  Let us Begin to Copy : We change gears and switch over to the destination account  Step 7: Setting up the Destination Account(ee_user and git_swap)  Now we have to do two steps one after the other, do a quick earthengine authenticate and authenticate to your new account and perform Step 2 and Step 3 this time using your new account. The tool  **ee_user**  will also allow you to change your accounts. I already created Step 2 for my secondary account and now I will use that to authorize my git client with the new account.   Change your earthengine authentication and also validate your git key   Now we authorized the git client with the second key  Step 8: Replicate Repositories (git_create)  To setup our new account we need to build the outline of the earlier account, the repolists and folders inside these repos and then similary the folders and empty collections in the secondary account.  Note: Git cannot push an empty repository so if you have an empty repository delete it before downloading and pushing to new account   Git Create your folder based on your earlier account  The repo lists now look similar   Repo created on secondary account  Step 9: Push to New Account(git_replicate)  Now we push all codes from our earlier account to our new account, this way our repositories will now be populated with the most recent codes.  \"Do not push to any repository that already has code because this will overwrite it\"   git_replicate to new account  Step 10: Replicate Asset Structure(asset_create) and Assets (asset_replicate)  This is similar to git_create here we replicate the collection and folder structure so we can push our assets to them. You pass it the original report you created from your primary account and it sets up as needed.   Creates asset structure (folders and collections)  This has replicated the collection and Image folder structures.   Asset Collections and Folders have been created(Left: asset home before asset_create Right: asset home after asset_create)  However this is still empty and the last step makes sure that your assets are actually copied over to your new asset home. I have included a counter to measure transfers left incase it is a large collection.   Asset Replication: Copying assets to your home folder  The final results is your assets and codes all copied, you will still have to edit codes to change your path as needed but for now we have replicated an Earth Engine account into a new location.   Replicated Assets on Both Accounts Copied from Left to Right  There you go, over the last 10 steps we have managed to replicate and move an earth engine account from one place to another. Though I found this useful to move accounts within a university setting, I see some value in moving accounts and replicating when a project member leaves a project or for simply migrating at large. For now if an owner of an account deletes his/her account or looses access to his/her account and even if you are a writer to the repository and the collection, you will loose access to these codes and assets. So this can aid in maintaining continuity by moving codes to more persistent account.  Though I have not tested this tools in a linux setting, these setup tools can be adapted and used easily in that framework, since I have tested the individual components in such setups.", 
            "title": "Google Takeout and Transfer: Tools and Guide for Code and Asset Transfer"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/", 
            "text": "Google Earth Engine Batch Asset Manager with Addons\n\n\n\n\nGoogle Earth Engine Batch Asset Manager with Addons is an extension of the one developed by Lukasz \nhere\n and additional tools were added to include functionality for moving assets, conversion of objects to fusion table, cleaning folders, querying tasks. The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. tab.\n\n\n\n\nTable of contents\n\n\n\n\nInstallation\n\n\nGetting started\n\n\nBatch uploader\n\n\nParsing metadata\n\n\n\n\n\n\nUsage examples\n\n\nEE User\n\n\nCreate\n\n\nUpload a directory with images and associate properties with each image:\n\n\nUpload a directory with images with specific NoData value to a selected destination:\n\n\nAsset List\n\n\nEarth Engine Asset Report\n\n\nTask Query\n\n\nTask Query during ingestion\n\n\nTask Report\n\n\nDelete a collection with content:\n\n\nAssets Move\n\n\nAssets Copy\n\n\nAssets Access\n\n\nSet Collection Property\n\n\nCleanup Utility\n\n\nCancel all tasks\n\n\n\n\n\n\n\n\nInstallation\n\n\nWe assume Earth Engine Python API is installed and EE authorised as desribed \nhere\n. To install:\n\n\ngit clone https://github.com/samapriya/gee_asset_manager_addon\ncd gee_asset_manager \n pip install .\n\n\n\n\nInstallation is an optional step; the application can be also run\ndirectly by executing geeadd.py script. The advantage of having it\ninstalled is being able to execute geeadd as any command line tool. I\nrecommend installation within virtual environment. To install run\n\n\npython setup.py develop or python setup.py install\n\nIn a linux distribution\nsudo python setup.py develop or sudo python setup.py install\n\n\n\n\nGetting started\n\n\nAs usual, to print help:\n\n\nusage: geeadd.py [-h]\n                 {ee_user,create,upload,lst,ee_report,collsize,tasks,taskquery,report,delete,mover,copy,access,collprop,cleanout,cancel}\n                 ...\n\nGoogle Earth Engine Batch Asset Manager with Addons\n\npositional arguments:\n  {ee_user,create,upload,lst,ee_report,collsize,tasks,taskquery,report,delete,mover,copy,access,collprop,cleanout,cancel}\n    ee_user             Allows you to associate/change GEE account to system\n    create              Allows the user to create an asset collection or\n                        folder in Google Earth Engine\n    upload              Batch Asset Uploader.\n    lst                 List assets in a folder/collection or write as text\n                        file\n    ee_report           Prints a detailed report of all Earth Engine Assets\n                        includes Asset Type, Path,Number of\n                        Assets,size(MB),unit,owner,readers,writers\n    collsize            Collects collection size in Human Readable form \n\n                        Number of assets\n    tasks               Queries currently running, enqued,failed\n    taskquery           Queries currently running, enqued,failed ingestions\n                        and uploaded assets\n    report              Create a report of all tasks and exports to a CSV file\n    delete              Deletes collection and all items inside. Supports\n                        Unix-like wildcards.\n    mover               Moves all assets from one collection to another\n    copy                Copies all assets from one collection to another:\n                        Including copying from other users if you have read\n                        permission to their assets\n    access              Sets Permissions for Images, Collection or all assets\n                        in EE Folder Example: python ee_permissions.py --mode\n                        \nfolder\n --asset \nusers/john/doe\n --user\n                        \njimmy@doe.com:R\n\n    collprop            Sets Overall Properties for Image Collection\n    cleanout            Clear folders with datasets from earlier downloaded\n    cancel              Cancel all running tasks\n\noptional arguments:\n  -h, --help            show this help message and exit\n\n\n\n\n\nTo obtain help for a specific functionality, simply call it with \nhelp\n\nswitch, e.g.: \ngeeadd upload -h\n. If you didn't install geeadd, then you\ncan run it just by going to \ngeeadd\n directory and running \npython\ngeeadd.py [arguments go here]\n\n\nBatch uploader\n\n\nThe script creates an Image Collection from GeoTIFFs in your local\ndirectory. By default, the collection name is the same as the local\ndirectory name; with optional parameter you can provide a different\nname. Another optional parameter is a path to a CSV file with metadata\nfor images, which is covered in the next section:\n\nParsing metadata\n.\n\n\nusage: geeadd.py upload [-h] -u USER --source SOURCE --dest DEST [-m METADATA]\n                        [--large] [--nodata NODATA]\n\noptional arguments:\n  -h, --help            show this help message and exit\n\nRequired named arguments.:\n  -u USER, --user USER  Google account name (gmail address).\n  --source SOURCE       Path to the directory with images for upload.\n  --dest DEST           Destination. Full path for upload to Google Earth\n                        Engine, e.g. users/johndoe/myponycollection\n\nOptional named arguments:\n  -m METADATA, --metadata METADATA\n                        Path to CSV with metadata.\n  --large               (Advanced) Use multipart upload. Might help if upload\n                        of large files is failing on some systems. Might cause\n                        other issues.\n  --nodata NODATA       The value to burn into the raster as NoData (missing\n                        data)\n\n\n\n\nParsing metadata\n\n\nBy metadata we understand here the properties associated with each image. Thanks to these, GEE user can easily filter collection based on specified criteria. The file with metadata should be organised as follows:\n\n\n\n\n\n\n\n\nfilename (without extension)\n\n\nproperty1 header\n\n\nproperty2 header\n\n\n\n\n\n\n\n\n\n\nfile1\n\n\nvalue1\n\n\nvalue2\n\n\n\n\n\n\nfile2\n\n\nvalue3\n\n\nvalue4\n\n\n\n\n\n\n\n\nNote that header can contain only letters, digits and underscores.\n\n\nExample:\n\n\n\n\n\n\n\n\nid_no\n\n\nclass\n\n\ncategory\n\n\nbinomial\n\n\nsystem:time_start\n\n\n\n\n\n\n\n\n\n\nmy_file_1\n\n\nGASTROPODA\n\n\nEN\n\n\nAaadonta constricta\n\n\n1478943081000\n\n\n\n\n\n\nmy_file_2\n\n\nGASTROPODA\n\n\nCR\n\n\nAaadonta irregularis\n\n\n1478943081000\n\n\n\n\n\n\n\n\nThe corresponding files are my_file_1.tif and my_file_2.tif. With each of the files five properties are associated: id_no, class, category, binomial and system:time_start. The latter is time in Unix epoch format, in milliseconds, as documented in GEE glosary. The program will match the file names from the upload directory with ones provided in the CSV and pass the metadata in JSON format:\n\n\n{ id_no: my_file_1, class: GASTROPODA, category: EN, binomial: Aaadonta constricta, system:time_start: 1478943081000}\n\n\n\n\nThe program will report any illegal fields, it will also complain if not all of the images passed for upload have metadata associated. User can opt to ignore it, in which case some assets will have no properties.\n\n\nHaving metadata helps in organising your asstets, but is not mandatory - you can skip it.\n\n\nUsage examples\n\n\nEE User\n\n\nThis tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back\n\n\nCreate\n\n\nThis tool allows you to create a collection or folder in your earth engine root directory. The tool uses the system cli to achieve this and this has been included so as to reduce the need to switch between multiple tools and CLI.\n\n\nusage: ppipe.py create [-h] --typ TYP --path PATH\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --typ TYP    Specify type: collection or folder\n  --path PATH  This is the path for the earth engine asset to be created full\n               path is needsed eg: users/johndoe/collection\n\n\n\n\nUpload a directory with images to your myfolder/mycollection and associate properties with each image:\n\n\ngeeadd upload -u johndoe@gmail.com --source path_to_directory_with_tif -m path_to_metadata.csv --dest users/johndoe/myfolder/myponycollection\n\n\n\n\nThe script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.\n\n\nUpload a directory with images with specific NoData value to a selected destination\n\n\ngeeadd upload -u johndoe@gmail.com --source path_to_directory_with_tif --dest users/johndoe/myfolder/myponycollection --nodata 222\n\n\n\n\nIn this case we need to supply full path to the destination, which is helpful when we upload to a shared folder. In the provided example we also burn value 222 into all rasters for missing data (NoData).\n\n\nAsset List\n\n\nThis tool is designed to either print or output asset lists within folders or collections using earthengine ls tool functions.\n\n\nusage: geeadd lst [-h] --location LOCATION --type TYPE [--items ITEMS]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --location LOCATION  This it the location of your folder/collection\n  --type TYPE          Whether you want the list to be printed or output as\n                       text(print/report)\n  --items ITEMS        Number of items to list\n\n\n\n\nEarth Engine Asset Report\n\n\nThis tool recursively goes through all your assets(Includes Images, ImageCollection,Table,) and generates a report containing the following fields\n[Type,Asset Type, Path,Number of Assets,size(MB),unit,owner,readers,writers].\n\n\nusage: geeadd.py ee_report [-h] --outfile OUTFILE\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --outfile OUTFILE  This it the location of your report csv file\n\n\n\n\nA simple setup is the following\n\ngeeadd --outfile \"C:\\johndoe\\report.csv\"\n\n\nTask Query\n\n\nThis script counts all currently running and ready tasks along with failed tasks.\n\n\nusage: geeadd.py tasks [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\ngeeadd.py tasks\n\n\n\n\nTask Query during ingestion\n\n\nThis script can be used intermittently to look at running, failed and ready(waiting) tasks during ingestion. This script is a special case using query tasks only when uploading assets to collection by providing collection pathway to see how collection size increases.\n\n\nusage: geeadd.py taskquery [-h] [--destination DESTINATION]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --destination DESTINATION\n                        Full path to asset where you are uploading files\n\ngeeadd.py taskquery \nusers/johndoe/myfolder/myponycollection\n\n\n\n\n\nTask Report\n\n\nSometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type\n\n\nusage: geeadd.py report [-h] [--r R] [--e E]\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --r R       Path \n CSV filename where the report will be saved\n  --e E       Path \n CSV filename where the errorlog will be saved\n\ngeeadd.py report --r \nreport.csv\n --e \nerrorlog.csv\n\n\n\n\n\nDelete a collection with content:\n\n\nThe delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution!\n\n\ngeeadd delete users/johndoe/test\n\n\n\n\nConsole output:\n\n\n2016-07-17 16:14:09,212 :: oauth2client.client :: INFO :: Attempting refresh to obtain initial access_token\n2016-07-17 16:14:09,213 :: oauth2client.client :: INFO :: Refreshing access_token\n2016-07-17 16:14:10,842 :: root :: INFO :: Attempting to delete collection test\n2016-07-17 16:14:16,898 :: root :: INFO :: Collection users/johndoe/test removed\n\n\n\n\nDelete all directories / collections based on a Unix-like pattern\n\n\ngeeadd delete users/johndoe/*weird[0-9]?name*\n\n\n\n\nAssets Move\n\n\nThis script allows us to recursively move assets from one collection to the other.\n\n\nusage: geeadd.py mover [-h] [--assetpath ASSETPATH] [--finalpath FINALPATH]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --assetpath ASSETPATH\n                        Existing path of assets\n  --finalpath FINALPATH\n                        New path for assets\ngeeadd.py mover --assetpath \nusers/johndoe/myfolder/myponycollection\n --destination \nusers/johndoe/myfolder/myotherponycollection\n\n\n\n\n\nAssets Copy\n\n\nThis script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections.\n\n\nusage: geeadd.py copy [-h] [--initial INITIAL] [--final FINAL]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --initial INITIAL  Existing path of assets\n  --final FINAL      New path for assets\ngeeadd.py mover --initial \nusers/johndoe/myfolder/myponycollection\n --final \nusers/johndoe/myfolder/myotherponycollection\n\n\n\n\n\nAssets Access\n\n\nThis tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time.\n\n\nusage: geeadd access [-h] --mode MODE --asset ASSET --user USER\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --mode MODE    This lets you select if you want to change permission or\n                 folder/collection/image\n  --asset ASSET  This is the path to the earth engine asset whose permission\n                 you are changing folder/collection/image\n  --user USER    This is the email address to whom you want to give read or\n                 write permission Usage: \njohn@doe.com:R\n or \njohn@doe.com:W\n\n                 R/W refers to read or write permission\ngeeadd.py access --mode folder --asset \nfolder/collection/image\n --user \njohn@doe.com:R\n\n\n\n\n\nSet Collection Property\n\n\nThis script is derived from the ee tool to set collection properties and will set overall properties for collection.\n\n\nusage: geeadd.py collprop [-h] [--coll COLL] [--p P]\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --coll COLL  Path of Image Collection\n  --p P        \nsystem:description=Description\n/\nsystem:provider_url=url\n/\nsys\n               tem:tags=tags\n/\nsystem:title=title\n\n\n\n\nCleanup Utility\n\n\nThis script is used to clean folders once all processes have been completed. In short this is a function to clear folder on local machine.\n\n\nusage: geeadd.py cleanout [-h] [--dirpath DIRPATH]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --dirpath DIRPATH  Folder you want to delete after all processes have been\n                     completed\ngeeadd.py cleanout --dirpath \n./folder\n\n\n\n\n\nCancel all tasks\n\n\nThis is a simpler tool, can be called directly from the earthengine cli as well\n\n\nearthengine cli command\nearthengine task cancel all\n\nusage: geeadd.py cancel [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\n\n\n\nChangelog\n\n\nv0.1.9\n\n\n\n\nAdded Earth Engine Asset Report Tool\n\n\nGeneral improvements\n\n\n\n\nv0.1.8\n\n\n\n\nFixed issues with install\n\n\nDependcies now part of setup.py\n\n\nUpdated Parser and general improvements", 
            "title": "Google Earth Engine Asset Manager Addon"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#google-earth-engine-batch-asset-manager-with-addons", 
            "text": "Google Earth Engine Batch Asset Manager with Addons is an extension of the one developed by Lukasz  here  and additional tools were added to include functionality for moving assets, conversion of objects to fusion table, cleaning folders, querying tasks. The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. tab.", 
            "title": "Google Earth Engine Batch Asset Manager with Addons"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#table-of-contents", 
            "text": "Installation  Getting started  Batch uploader  Parsing metadata    Usage examples  EE User  Create  Upload a directory with images and associate properties with each image:  Upload a directory with images with specific NoData value to a selected destination:  Asset List  Earth Engine Asset Report  Task Query  Task Query during ingestion  Task Report  Delete a collection with content:  Assets Move  Assets Copy  Assets Access  Set Collection Property  Cleanup Utility  Cancel all tasks", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#installation", 
            "text": "We assume Earth Engine Python API is installed and EE authorised as desribed  here . To install:  git clone https://github.com/samapriya/gee_asset_manager_addon\ncd gee_asset_manager   pip install .  Installation is an optional step; the application can be also run\ndirectly by executing geeadd.py script. The advantage of having it\ninstalled is being able to execute geeadd as any command line tool. I\nrecommend installation within virtual environment. To install run  python setup.py develop or python setup.py install\n\nIn a linux distribution\nsudo python setup.py develop or sudo python setup.py install", 
            "title": "Installation"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#getting-started", 
            "text": "As usual, to print help:  usage: geeadd.py [-h]\n                 {ee_user,create,upload,lst,ee_report,collsize,tasks,taskquery,report,delete,mover,copy,access,collprop,cleanout,cancel}\n                 ...\n\nGoogle Earth Engine Batch Asset Manager with Addons\n\npositional arguments:\n  {ee_user,create,upload,lst,ee_report,collsize,tasks,taskquery,report,delete,mover,copy,access,collprop,cleanout,cancel}\n    ee_user             Allows you to associate/change GEE account to system\n    create              Allows the user to create an asset collection or\n                        folder in Google Earth Engine\n    upload              Batch Asset Uploader.\n    lst                 List assets in a folder/collection or write as text\n                        file\n    ee_report           Prints a detailed report of all Earth Engine Assets\n                        includes Asset Type, Path,Number of\n                        Assets,size(MB),unit,owner,readers,writers\n    collsize            Collects collection size in Human Readable form  \n                        Number of assets\n    tasks               Queries currently running, enqued,failed\n    taskquery           Queries currently running, enqued,failed ingestions\n                        and uploaded assets\n    report              Create a report of all tasks and exports to a CSV file\n    delete              Deletes collection and all items inside. Supports\n                        Unix-like wildcards.\n    mover               Moves all assets from one collection to another\n    copy                Copies all assets from one collection to another:\n                        Including copying from other users if you have read\n                        permission to their assets\n    access              Sets Permissions for Images, Collection or all assets\n                        in EE Folder Example: python ee_permissions.py --mode\n                         folder  --asset  users/john/doe  --user\n                         jimmy@doe.com:R \n    collprop            Sets Overall Properties for Image Collection\n    cleanout            Clear folders with datasets from earlier downloaded\n    cancel              Cancel all running tasks\n\noptional arguments:\n  -h, --help            show this help message and exit  To obtain help for a specific functionality, simply call it with  help \nswitch, e.g.:  geeadd upload -h . If you didn't install geeadd, then you\ncan run it just by going to  geeadd  directory and running  python\ngeeadd.py [arguments go here]", 
            "title": "Getting started"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#batch-uploader", 
            "text": "The script creates an Image Collection from GeoTIFFs in your local\ndirectory. By default, the collection name is the same as the local\ndirectory name; with optional parameter you can provide a different\nname. Another optional parameter is a path to a CSV file with metadata\nfor images, which is covered in the next section: Parsing metadata .  usage: geeadd.py upload [-h] -u USER --source SOURCE --dest DEST [-m METADATA]\n                        [--large] [--nodata NODATA]\n\noptional arguments:\n  -h, --help            show this help message and exit\n\nRequired named arguments.:\n  -u USER, --user USER  Google account name (gmail address).\n  --source SOURCE       Path to the directory with images for upload.\n  --dest DEST           Destination. Full path for upload to Google Earth\n                        Engine, e.g. users/johndoe/myponycollection\n\nOptional named arguments:\n  -m METADATA, --metadata METADATA\n                        Path to CSV with metadata.\n  --large               (Advanced) Use multipart upload. Might help if upload\n                        of large files is failing on some systems. Might cause\n                        other issues.\n  --nodata NODATA       The value to burn into the raster as NoData (missing\n                        data)", 
            "title": "Batch uploader"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#parsing-metadata", 
            "text": "By metadata we understand here the properties associated with each image. Thanks to these, GEE user can easily filter collection based on specified criteria. The file with metadata should be organised as follows:     filename (without extension)  property1 header  property2 header      file1  value1  value2    file2  value3  value4     Note that header can contain only letters, digits and underscores.  Example:     id_no  class  category  binomial  system:time_start      my_file_1  GASTROPODA  EN  Aaadonta constricta  1478943081000    my_file_2  GASTROPODA  CR  Aaadonta irregularis  1478943081000     The corresponding files are my_file_1.tif and my_file_2.tif. With each of the files five properties are associated: id_no, class, category, binomial and system:time_start. The latter is time in Unix epoch format, in milliseconds, as documented in GEE glosary. The program will match the file names from the upload directory with ones provided in the CSV and pass the metadata in JSON format:  { id_no: my_file_1, class: GASTROPODA, category: EN, binomial: Aaadonta constricta, system:time_start: 1478943081000}  The program will report any illegal fields, it will also complain if not all of the images passed for upload have metadata associated. User can opt to ignore it, in which case some assets will have no properties.  Having metadata helps in organising your asstets, but is not mandatory - you can skip it.", 
            "title": "Parsing metadata"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#usage-examples", 
            "text": "", 
            "title": "Usage examples"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#ee-user", 
            "text": "This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back", 
            "title": "EE User"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#create", 
            "text": "This tool allows you to create a collection or folder in your earth engine root directory. The tool uses the system cli to achieve this and this has been included so as to reduce the need to switch between multiple tools and CLI.  usage: ppipe.py create [-h] --typ TYP --path PATH\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --typ TYP    Specify type: collection or folder\n  --path PATH  This is the path for the earth engine asset to be created full\n               path is needsed eg: users/johndoe/collection", 
            "title": "Create"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#upload-a-directory-with-images-to-your-myfoldermycollection-and-associate-properties-with-each-image", 
            "text": "geeadd upload -u johndoe@gmail.com --source path_to_directory_with_tif -m path_to_metadata.csv --dest users/johndoe/myfolder/myponycollection  The script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.", 
            "title": "Upload a directory with images to your myfolder/mycollection and associate properties with each image:"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#upload-a-directory-with-images-with-specific-nodata-value-to-a-selected-destination", 
            "text": "geeadd upload -u johndoe@gmail.com --source path_to_directory_with_tif --dest users/johndoe/myfolder/myponycollection --nodata 222  In this case we need to supply full path to the destination, which is helpful when we upload to a shared folder. In the provided example we also burn value 222 into all rasters for missing data (NoData).", 
            "title": "Upload a directory with images with specific NoData value to a selected destination"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#asset-list", 
            "text": "This tool is designed to either print or output asset lists within folders or collections using earthengine ls tool functions.  usage: geeadd lst [-h] --location LOCATION --type TYPE [--items ITEMS]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --location LOCATION  This it the location of your folder/collection\n  --type TYPE          Whether you want the list to be printed or output as\n                       text(print/report)\n  --items ITEMS        Number of items to list", 
            "title": "Asset List"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#earth-engine-asset-report", 
            "text": "This tool recursively goes through all your assets(Includes Images, ImageCollection,Table,) and generates a report containing the following fields\n[Type,Asset Type, Path,Number of Assets,size(MB),unit,owner,readers,writers].  usage: geeadd.py ee_report [-h] --outfile OUTFILE\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --outfile OUTFILE  This it the location of your report csv file  A simple setup is the following geeadd --outfile \"C:\\johndoe\\report.csv\"", 
            "title": "Earth Engine Asset Report"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#task-query", 
            "text": "This script counts all currently running and ready tasks along with failed tasks.  usage: geeadd.py tasks [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\ngeeadd.py tasks", 
            "title": "Task Query"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#task-query-during-ingestion", 
            "text": "This script can be used intermittently to look at running, failed and ready(waiting) tasks during ingestion. This script is a special case using query tasks only when uploading assets to collection by providing collection pathway to see how collection size increases.  usage: geeadd.py taskquery [-h] [--destination DESTINATION]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --destination DESTINATION\n                        Full path to asset where you are uploading files\n\ngeeadd.py taskquery  users/johndoe/myfolder/myponycollection", 
            "title": "Task Query during ingestion"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#task-report", 
            "text": "Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type  usage: geeadd.py report [-h] [--r R] [--e E]\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --r R       Path   CSV filename where the report will be saved\n  --e E       Path   CSV filename where the errorlog will be saved\n\ngeeadd.py report --r  report.csv  --e  errorlog.csv", 
            "title": "Task Report"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#delete-a-collection-with-content", 
            "text": "The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution!  geeadd delete users/johndoe/test  Console output:  2016-07-17 16:14:09,212 :: oauth2client.client :: INFO :: Attempting refresh to obtain initial access_token\n2016-07-17 16:14:09,213 :: oauth2client.client :: INFO :: Refreshing access_token\n2016-07-17 16:14:10,842 :: root :: INFO :: Attempting to delete collection test\n2016-07-17 16:14:16,898 :: root :: INFO :: Collection users/johndoe/test removed", 
            "title": "Delete a collection with content:"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#delete-all-directories-collections-based-on-a-unix-like-pattern", 
            "text": "geeadd delete users/johndoe/*weird[0-9]?name*", 
            "title": "Delete all directories / collections based on a Unix-like pattern"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#assets-move", 
            "text": "This script allows us to recursively move assets from one collection to the other.  usage: geeadd.py mover [-h] [--assetpath ASSETPATH] [--finalpath FINALPATH]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --assetpath ASSETPATH\n                        Existing path of assets\n  --finalpath FINALPATH\n                        New path for assets\ngeeadd.py mover --assetpath  users/johndoe/myfolder/myponycollection  --destination  users/johndoe/myfolder/myotherponycollection", 
            "title": "Assets Move"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#assets-copy", 
            "text": "This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections.  usage: geeadd.py copy [-h] [--initial INITIAL] [--final FINAL]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --initial INITIAL  Existing path of assets\n  --final FINAL      New path for assets\ngeeadd.py mover --initial  users/johndoe/myfolder/myponycollection  --final  users/johndoe/myfolder/myotherponycollection", 
            "title": "Assets Copy"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#assets-access", 
            "text": "This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time.  usage: geeadd access [-h] --mode MODE --asset ASSET --user USER\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --mode MODE    This lets you select if you want to change permission or\n                 folder/collection/image\n  --asset ASSET  This is the path to the earth engine asset whose permission\n                 you are changing folder/collection/image\n  --user USER    This is the email address to whom you want to give read or\n                 write permission Usage:  john@doe.com:R  or  john@doe.com:W \n                 R/W refers to read or write permission\ngeeadd.py access --mode folder --asset  folder/collection/image  --user  john@doe.com:R", 
            "title": "Assets Access"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#set-collection-property", 
            "text": "This script is derived from the ee tool to set collection properties and will set overall properties for collection.  usage: geeadd.py collprop [-h] [--coll COLL] [--p P]\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --coll COLL  Path of Image Collection\n  --p P         system:description=Description / system:provider_url=url / sys\n               tem:tags=tags / system:title=title", 
            "title": "Set Collection Property"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#cleanup-utility", 
            "text": "This script is used to clean folders once all processes have been completed. In short this is a function to clear folder on local machine.  usage: geeadd.py cleanout [-h] [--dirpath DIRPATH]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --dirpath DIRPATH  Folder you want to delete after all processes have been\n                     completed\ngeeadd.py cleanout --dirpath  ./folder", 
            "title": "Cleanup Utility"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#cancel-all-tasks", 
            "text": "This is a simpler tool, can be called directly from the earthengine cli as well  earthengine cli command\nearthengine task cancel all\n\nusage: geeadd.py cancel [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit", 
            "title": "Cancel all tasks"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#changelog", 
            "text": "", 
            "title": "Changelog"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#v019", 
            "text": "Added Earth Engine Asset Report Tool  General improvements", 
            "title": "v0.1.9"
        }, 
        {
            "location": "/projects/gee_asset_manager_addon/#v018", 
            "text": "Fixed issues with install  Dependcies now part of setup.py  Updated Parser and general improvements", 
            "title": "v0.1.8"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/", 
            "text": "Planet GEE Pipeline CLI\n\n\n\n\n\n\n\n\n\n\nWhile moving between assets from Planet Inc and Google Earth Engine it was imperative to create a pipeline that allows for easy transitions between the two service end points and this tool is designed to act as a step by step process chain from Planet Assets to batch upload and modification within the Google Earth Engine environment. The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. tab.\n\n\n\n\nTable of contents\n\n\n\n\nInstallation\n\n\nGetting started\n\n\nBatch uploader\n\n\nParsing metadata\n\n\n\n\n\n\nUsage examples\n\n\nPlanet Tools\n\n\nPlanet Key\n\n\nAOI JSON\n\n\nActivate or Check Asset\n\n\nCheck Total size of assets\n\n\nDownload Asset\n\n\nMetadata Parser\n\n\n\n\n\n\nEarth Engine Tools\n\n\nEE User\n\n\nCreate\n\n\nUpload a directory with images and associate properties with each image:\n\n\nUpload a directory with images with specific NoData value to a selected destination:\n\n\nAsset List\n\n\nAsset Size\n\n\nEarth Engine Asset Report\n\n\nTask Query\n\n\nTask Report\n\n\nDelete a collection with content:\n\n\nAssets Move\n\n\nAssets Copy\n\n\nAssets Access\n\n\nSet Collection Property\n\n\nCancel all tasks\n\n\n\n\n\n\nCredits\n\n\n\n\nInstallation\n\n\nWe assume Earth Engine Python API is installed and EE authorised as desribed \nhere\n. We also assume Planet Python API is installed you can install by simply running.\n\n\npip install planet\n\n\n\n\nFurther instructions can be found \nhere\n\n\nThis toolbox also uses some functionality from GDAL\n\nFor installing GDAL in Ubuntu\n\n\nsudo add-apt-repository ppa:ubuntugis/ppa \n sudo apt-get update\nsudo apt-get install gdal-bin\n\n\n\n\nFor Windows I found this \nguide\n from UCLA\n\n\nTo install \nPlanet-GEE-Pipeline-CLI:\n\n\ngit clone https://github.com/samapriya/Planet-GEE-Pipeline-CLI.git\ncd Planet-GEE-Pipeline-CLI \n pip install -r requirements.txt\n\nfor linux use sudo pip install -r requirements.txt\n\n\n\n\nThis release also contains a windows installer which bypasses the need for you to have admin permission, it does however require you to have python in the system path meaning when you open up command prompt you should be able to type python and start it within the command prompt window. Post installation using the installer you can just call ppipe using the command prompt similar to calling python. Give it a go post installation type\n\n\nppipe -h\n\n\n\n\nInstallation is an optional step; the application can be also run directly by executing ppipe.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. To install run\n\n\npython setup.py develop or python setup.py install\n\nIn a linux distribution\nsudo python setup.py develop or sudo python setup.py install\n\n\n\n\nGetting started\n\n\nAs usual, to print help:\n\n\nusage: ppipe.py [-h]\n                {\n                ,planetkey,aoijson,activatepl,space,downloadpl,metadata,ee_user,create,upload,lst,collsize,delete,tasks,taskquery,report,cancel,mover,copy,access,collprop,cleanout}\n                ...\n\nPlanet Pipeline with Google Earth Engine Batch Addons\n\npositional arguments:\n  { ,planetkey,aoijson,activatepl,space,downloadpl,metadata,ee_user,create,upload,lst,collsize,delete,tasks,taskquery,report,cancel,mover,copy,access,collprop,cleanout}\n                        ---------------------------------------\n                        -----Choose from Planet Tools Below-----\n                        ---------------------------------------\n    planetkey           Enter your planet API Key\n    aoijson             Tool to convert KML, Shapefile,WKT,GeoJSON or Landsat\n                        WRS PathRow file to AreaOfInterest.JSON file with\n                        structured query for use with Planet API 1.0\n    activatepl          Tool to query and/or activate Planet Assets\n    space               Tool to query total download size of activated assets\n                        \n local space left for download\n    downloadpl          Tool to download Planet Assets\n    metadata            Tool to tabulate and convert all metadata files from\n                        Planet or Digital Globe Assets\n                        -------------------------------------------\n                        ----Choose from Earth Engine Tools Below----\n                        -------------------------------------------\n    ee_user             Get Earth Engine API Key \n Paste it back to Command\n                        line/shell to change user\n    create              Allows the user to create an asset collection or\n                        folder in Google Earth Engine\n    upload              Batch Asset Uploader.\n    lst                 List assets in a folder/collection or write as text\n                        file\n    collsize            Collects collection size in Human Readable form \n\n                        Number of assets\n    delete              Deletes collection and all items inside. Supports\n                        Unix-like wildcards.\n    tasks               Queries currently running, enqued,failed\n    taskquery           Queries currently running, enqued,failed ingestions\n                        and uploaded assets\n    report              Create a report of all tasks and exports to a CSV file\n    cancel              Cancel all running tasks\n    mover               Moves all assets from one collection to another\n    copy                Copies all assets from one collection to another:\n                        Including copying from other users if you have read\n                        permission to their assets\n    access              Sets Permissions for Images, Collection or all assets\n                        in EE Folder Example: python ee_permissions.py --mode\n                        \nfolder\n --asset \nusers/john/doe\n --user\n                        \njimmy@doe.com:R\n\n    collprop            Sets Overall Properties for Image Collection\n    cleanout            Clear folders with datasets from earlier downloaded\n\noptional arguments:\n  -h, --help            show this help message and exit\n\n\n\n\nTo obtain help for a specific functionality, simply call it with \nhelp\n\nswitch, e.g.: \nppipe upload -h\n. If you didn't install ppipe, then you\ncan run it just by going to \nppipe\n directory and running \npython\nppipe.py [arguments go here]\n\n\nBatch uploader\n\n\nThe script creates an Image Collection from GeoTIFFs in your local\ndirectory. By default, the collection name is the same as the local\ndirectory name; with optional parameter you can provide a different\nname. Another optional parameter is a path to a CSV file with metadata\nfor images, which is covered in the next section:\n\nParsing metadata\n.\n\n\nusage: ppipe.py upload [-h] --source SOURCE --dest DEST [-m METADATA]\n                       [-mf MANIFEST] [--large] [--nodata NODATA] [-u USER]\n                       [-s SERVICE_ACCOUNT] [-k PRIVATE_KEY] [-b BUCKET]\n\noptional arguments:\n  -h, --help            show this help message and exit\n\nRequired named arguments.:\n  --source SOURCE       Path to the directory with images for upload.\n  --dest DEST           Destination. Full path for upload to Google Earth\n                        Engine, e.g. users/pinkiepie/myponycollection\n  -u USER, --user USER  Google account name (gmail address).\n\nOptional named arguments:\n  -m METADATA, --metadata METADATA\n                        Path to CSV with metadata.\n  -mf MANIFEST, --manifest MANIFEST\n                        Manifest type to be used,for planetscope use\n                        \nplanetscope\n\n  --large               (Advanced) Use multipart upload. Might help if upload\n                        of large files is failing on some systems. Might cause\n                        other issues.\n  --nodata NODATA       The value to burn into the raster as NoData (missing\n                        data)\n  -s SERVICE_ACCOUNT, --service-account SERVICE_ACCOUNT\n                        Google Earth Engine service account.\n  -k PRIVATE_KEY, --private-key PRIVATE_KEY\n                        Google Earth Engine private key file.\n  -b BUCKET, --bucket BUCKET\n                        Google Cloud Storage bucket name.\n\n\n\n\nParsing metadata\n\n\nBy metadata we understand here the properties associated with each image. Thanks to these, GEE user can easily filter collection based on specified criteria. The file with metadata should be organised as follows:\n\n\n\n\n\n\n\n\nfilename (without extension)\n\n\nproperty1 header\n\n\nproperty2 header\n\n\n\n\n\n\n\n\n\n\nfile1\n\n\nvalue1\n\n\nvalue2\n\n\n\n\n\n\nfile2\n\n\nvalue3\n\n\nvalue4\n\n\n\n\n\n\n\n\nNote that header can contain only letters, digits and underscores.\n\n\nExample:\n\n\n\n\n\n\n\n\nid_no\n\n\nclass\n\n\ncategory\n\n\nbinomial\n\n\nsystem:time_start\n\n\n\n\n\n\n\n\n\n\nmy_file_1\n\n\nGASTROPODA\n\n\nEN\n\n\nAaadonta constricta\n\n\n1478943081000\n\n\n\n\n\n\nmy_file_2\n\n\nGASTROPODA\n\n\nCR\n\n\nAaadonta irregularis\n\n\n1478943081000\n\n\n\n\n\n\n\n\nThe corresponding files are my_file_1.tif and my_file_2.tif. With each of the files five properties are associated: id_no, class, category, binomial and system:time_start. The latter is time in Unix epoch format, in milliseconds, as documented in GEE glosary. The program will match the file names from the upload directory with ones provided in the CSV and pass the metadata in JSON format:\n\n\n{ id_no: my_file_1, class: GASTROPODA, category: EN, binomial: Aaadonta constricta, system:time_start: 1478943081000}\n\n\n\n\nThe program will report any illegal fields, it will also complain if not all of the images passed for upload have metadata associated. User can opt to ignore it, in which case some assets will have no properties.\n\n\nHaving metadata helps in organising your asstets, but is not mandatory - you can skip it.\n\n\nUsage examples\n\n\nUsage examples have been segmented into two parts focusing on both planet tools as well as earth engine tools, earth engine tools include additional developments in CLI which allows you to recursively interact with their python API\n\n\nPlanet Tools\n\n\nThe Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.\n\n\nPlanet Key\n\n\nThis tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools\n\n\nusage: ppipe.py planetkey [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\n\n\n\nIf using on a private machine the Key is saved as a csv file for all future runs of the tool.\n\n\nAOI JSON\n\n\nThe aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.\n\n\nusage: ppipe.py aoijson [-h] [--start START] [--end END] [--cloud CLOUD]\n                     [--inputfile INPUTFILE] [--geo GEO] [--loc LOC]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --start START         Start date in YYYY-MM-DD?\n  --end END             End date in YYYY-MM-DD?\n  --cloud CLOUD         Maximum Cloud Cover(0-1) representing 0-100\n  --inputfile INPUTFILE\n                        Choose a kml/shapefile/geojson or WKT file for\n                        AOI(KML/SHP/GJSON/WKT) or WRS (6 digit RowPath\n                        Example: 023042)\n  --geo GEO             map.geojson/aoi.kml/aoi.shp/aoi.wkt file\n  --loc LOC             Location where aoi.json file is to be stored\n\n\n\n\nActivate or Check Asset\n\n\nThe activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier\n\n\nusage: ppipe.py activatepl [-h] [--aoi AOI] [--action ACTION] [--asst ASST]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --aoi AOI        Choose aoi.json file created earlier\n  --action ACTION  choose between check/activate\n  --asst ASST      Choose between planet asset types (PSOrthoTile\n                   analytic/REOrthoTile analytic/PSOrthoTile\n                   analytic_xml/REOrthoTile analytic_xml\n\n\n\n\n\nCheck Total size of assets\n\n\nIt is important to sometimes estimate the overall size of download before you can actually download activated assets. This tool allows you to estimate local storage available at any location and overall size of download in MB or GB. This tool makes use of an existing url get request to look at content size and estimate overall download size of download for the activated assets.\n\n\nusage: ppipe.py space [-h] [--aoi AOI] [--local LOCAL] [--asset ASSET]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --aoi AOI      Choose aoi.json file created earlier\n  --local LOCAL  local path where you are downloading assets\n  --asset ASSET  Choose between planet asset types (PSOrthoTile\n                 analytic/PSOrthoTile analytic_dn/PSOrthoTile\n                 visual/PSScene4Band analytic/PSScene4Band\n                 analytic_dn/PSScene3Band analytic/PSScene3Band\n                 analytic_dn/PSScene3Band visual/REOrthoTile\n                 analytic/REOrthoTile visual\n\n\n\n\nDownload Asset\n\n\nHaving metadata helps in organising your asstets, but is not mandatory - you can skip it.\nThe downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types.\n\n\nusage: ppipe.py downloadpl [-h] [--aoi AOI] [--action ACTION] [--asst ASST]\n                           [--pathway PATHWAY]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --aoi AOI          Choose aoi.json file created earlier\n  --action ACTION    choose download\n  --asst ASST        Choose between planet asset types (PSOrthoTile\n                     analytic/REOrthoTile analytic/PSOrthoTile\n                     analytic_xml/REOrthoTile analytic_xml\n  --pathway PATHWAY  Folder Pathways where PlanetAssets are saved exampled\n                     ./PlanetScope ./RapidEye\n\n\n\n\nMetadata Parser\n\n\nThe metadata tab is a more powerful tool and consists of metadata parsing for All PlanetScope and RapiEye Assets along with Digital Globe MultiSpectral and DigitalGlobe PanChromatic datasets. This was developed as a standalone to process xml metadata files from multiple sources and is important step is the user plans to upload these assets to Google Earth Engine.\n\n\nusage: ppipe.py metadata [-h] [--asset ASSET] [--mf MF] [--mfile MFILE]\n                      [--errorlog ERRORLOG]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --asset ASSET        Choose PS OrthoTile(PSO)|PS OrthoTile DN(PSO_DN)|PS\n                       OrthoTile Visual(PSO_V)|PS4Band Analytic(PS4B)|PS4Band\n                       DN(PS4B_DN)|PS3Band Analytic(PS3B)|PS3Band\n                       DN(PS3B_DN)|PS3Band Visual(PS3B_V)|RE OrthoTile\n                       (REO)|RE OrthoTile Visual(REO_V)|DigitalGlobe\n                       MultiSpectral(DGMS)|DigitalGlobe Panchromatic(DGP)?\n  --mf MF              Metadata folder?\n  --mfile MFILE        Metadata filename to be exported along with Path.csv\n  --errorlog ERRORLOG  Errorlog to be exported along with Path.csv\n\n\n\n\nEarth Engine Tools\n\n\nThe ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. This is also a seperate package for earth engine users to use and can be downloaded \nhere\n\n\nEE User\n\n\nThis tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back\n\n\nCreate\n\n\nThis tool allows you to create a collection or folder in your earth engine root directory. The tool uses the system cli to achieve this and this has been included so as to reduce the need to switch between multiple tools and CLI.\n\n\nusage: ppipe.py create [-h] --typ TYP --path PATH\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --typ TYP    Specify type: collection or folder\n  --path PATH  This is the path for the earth engine asset to be created full\n               path is needsed eg: users/johndoe/collection\n\n\n\n\nUpload a directory with images to your myfolder/mycollection and associate properties with each image:\n\n\nppipe upload -u johndoe@gmail.com --source path_to_directory_with_tif -m path_to_metadata.csv -mf maifest_type(ex:planetscope) --dest users/johndoe/myfolder/myponycollection\n\n\n\n\nThe script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.You can also skip manifest for RapidEye imagery or any other source that does not require metadata field type handling.\n\n\nUpload a directory with images with specific NoData value to a selected destination\n\n\nppipe upload -u johndoe@gmail.com --source path_to_directory_with_tif --dest users/johndoe/myfolder/myponycollection --nodata 222\n\n\n\n\nIn this case we need to supply full path to the destination, which is helpful when we upload to a shared folder. In the provided example we also burn value 222 into all rasters for missing data (NoData).\n\n\nAsset List\n\n\nThis tool is designed to either print or output asset lists within folders or collections using earthengine ls tool functions.\n\n\nusage: geeadd.py lst [-h] --location LOCATION --typ TYP [--items ITEMS]\n                     [--output OUTPUT]\n\noptional arguments:\n  -h, --help           show this help message and exit\n\nRequired named arguments.:\n  --location LOCATION  This it the location of your folder/collection\n  --typ TYP            Whether you want the list to be printed or output as\n                       text[print/report]\n\nOptional named arguments:\n  --items ITEMS        Number of items to list\n  --output OUTPUT      Folder location for report to be exported\n\n\n\n\nAsset Size\n\n\nThis tool allows you to query the size of any Earth Engine asset[Images, Image Collections, Tables and Folders] and prints out the number of assets and total asset size in non-byte encoding meaning KB, MB, GB, TB depending on size.\n\n\nusage: geeadd assetsize [-h] --asset ASSET\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --asset ASSET  Earth Engine Asset for which to get size properties\n\n\n\n\nEarth Engine Asset Report\n\n\nThis tool recursively goes through all your assets(Includes Images, ImageCollection,Table,) and generates a report containing the following fields\n[Type,Asset Type, Path,Number of Assets,size(MB),unit,owner,readers,writers].\n\n\nusage: geeadd.py ee_report [-h] --outfile OUTFILE\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --outfile OUTFILE  This it the location of your report csv file\n\n\n\n\nA simple setup is the following\n\ngeeadd --outfile \"C:\\johndoe\\report.csv\"\n\n\nTask Query\n\n\nThis script counts all currently running and ready tasks along with failed tasks.\n\n\nusage: geeadd.py tasks [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\ngeeadd.py tasks\n\n\n\n\nTask Report\n\n\nSometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type\n\n\nusage: geeadd taskreport [-h] [--r R]\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --r R       Folder Path where the reports will be saved\n\n\n\n\nDelete a collection with content:\n\n\nThe delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution!\n\n\ngeeadd delete users/johndoe/test\n\n\n\n\nConsole output:\n\n\n2016-07-17 16:14:09,212 :: oauth2client.client :: INFO :: Attempting refresh to obtain initial access_token\n2016-07-17 16:14:09,213 :: oauth2client.client :: INFO :: Refreshing access_token\n2016-07-17 16:14:10,842 :: root :: INFO :: Attempting to delete collection test\n2016-07-17 16:14:16,898 :: root :: INFO :: Collection users/johndoe/test removed\n\n\n\n\nDelete all directories / collections based on a Unix-like pattern\n\n\ngeeadd delete users/johndoe/*weird[0-9]?name*\n\n\n\n\nAssets Move\n\n\nThis script allows us to recursively move assets from one collection to the other.\n\n\nusage: geeadd.py mover [-h] [--assetpath ASSETPATH] [--finalpath FINALPATH]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --assetpath ASSETPATH\n                        Existing path of assets\n  --finalpath FINALPATH\n                        New path for assets\ngeeadd.py mover --assetpath \nusers/johndoe/myfolder/myponycollection\n --destination \nusers/johndoe/myfolder/myotherponycollection\n\n\n\n\n\nAssets Copy\n\n\nThis script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections.\n\n\nusage: geeadd.py copy [-h] [--initial INITIAL] [--final FINAL]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --initial INITIAL  Existing path of assets\n  --final FINAL      New path for assets\ngeeadd.py mover --initial \nusers/johndoe/myfolder/myponycollection\n --final \nusers/johndoe/myfolder/myotherponycollection\n\n\n\n\n\nAssets Access\n\n\nThis tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time.\n\n\nusage: geeadd access [-h] --mode MODE --asset ASSET --user USER\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --mode MODE    This lets you select if you want to change permission or\n                 folder/collection/image\n  --asset ASSET  This is the path to the earth engine asset whose permission\n                 you are changing folder/collection/image\n  --user USER    This is the email address to whom you want to give read or\n                 write permission Usage: \njohn@doe.com:R\n or \njohn@doe.com:W\n\n                 R/W refers to read or write permission\ngeeadd.py access --mode folder --asset \nfolder/collection/image\n --user \njohn@doe.com:R\n\n\n\n\n\nSet Collection Property\n\n\nThis script is derived from the ee tool to set collection properties and will set overall properties for collection.\n\n\nusage: geeadd.py collprop [-h] [--coll COLL] [--p P]\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --coll COLL  Path of Image Collection\n  --p P        \nsystem:description=Description\n/\nsystem:provider_url=url\n/\nsys\n               tem:tags=tags\n/\nsystem:title=title\n\n\n\n\nCancel all tasks\n\n\nThis is a simpler tool, can be called directly from the earthengine cli as well\n\n\nearthengine cli command\nearthengine task cancel all\n\nusage: geeadd.py cancel [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\n\n\n\nCredits\n\n\nJetStream\n A portion of the work is suported by JetStream Grant TG-GEO160014.\n\n\nAlso supported by \nPlanet Labs Ambassador Program\n\n\nOriginal upload function adapted from \nLukasz's asset manager tool\n\n\nChangelog\n\n\nv0.3.0\n\n\n\n\nAllows for quiet authentication for use in Google Colab or non interactive environments\n\n\nImproved planet key entry and authentication protocols\n\n\n\n\nv0.2.91\n\n\n\n\nFixed issue with Surface Reflectance metadata and manifest lib\n\n\nImproved ingestion support for (PSScene4Band analytic_Sr)[PS4B_SR]\n\n\n\n\nv0.2.9\n\n\n\n\nFixed issues with generating id list\n\n\nImproved overall security of command calls\n\n\n\n\nv0.2.2\n\n\n\n\nMajor improvements to ingestion using manifest ingest in Google Earth Engine\n\n\nContains manifest for all commonly used Planet Data item and asset combinations\n\n\nAdded additional tool to Earth Engine Enhancement including quota check before upload to GEE\n\n\n\n\nv0.2.1\n\n\n\n\nFixed initialization loop issue\n\n\n\n\nv0.2.0\n\n\n\n\nMetadata parser and Uploader Can now handle PlanetScope 4 Band Surface Reflectance Datasets\n\n\nGeneral Improvements\n\n\n\n\nv0.1.9\n\n\n\n\nChanges made to reflect updated GEE Addon tools\n\n\ngeneral improvements\n\n\n\n\nv0.1.8\n\n\n\n\nMinor fixes to parser and general improvements\n\n\nPlanet Key is now stored in a configuration folder which is safer \"C:\\users.config\\planet\"\n\n\nEarth Engine now requires you to assign a field type for metadata meaning an alphanumeric column like satID cannot also have numeric values unless specified explicitly . Manifest option has been added to handle this (just use -mf \"planetscope\")\n\n\nAdded capability to query download size and local disk capacity before downloading planet assets.\n\n\nAdded the list function to generate list of collections or folders including reports\n\n\nAdded the collection size tool which allows you to estimate total size or quota used from your allocated quota.\n\n\nogr2ft feature is removed since Earth Engine now allows vector and table uploading.", 
            "title": "Planet-Google Earth Engine Pipeline CLI"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#planet-gee-pipeline-cli", 
            "text": "While moving between assets from Planet Inc and Google Earth Engine it was imperative to create a pipeline that allows for easy transitions between the two service end points and this tool is designed to act as a step by step process chain from Planet Assets to batch upload and modification within the Google Earth Engine environment. The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. tab.", 
            "title": "Planet GEE Pipeline CLI"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#table-of-contents", 
            "text": "Installation  Getting started  Batch uploader  Parsing metadata    Usage examples  Planet Tools  Planet Key  AOI JSON  Activate or Check Asset  Check Total size of assets  Download Asset  Metadata Parser    Earth Engine Tools  EE User  Create  Upload a directory with images and associate properties with each image:  Upload a directory with images with specific NoData value to a selected destination:  Asset List  Asset Size  Earth Engine Asset Report  Task Query  Task Report  Delete a collection with content:  Assets Move  Assets Copy  Assets Access  Set Collection Property  Cancel all tasks    Credits", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#installation", 
            "text": "We assume Earth Engine Python API is installed and EE authorised as desribed  here . We also assume Planet Python API is installed you can install by simply running.  pip install planet  Further instructions can be found  here  This toolbox also uses some functionality from GDAL \nFor installing GDAL in Ubuntu  sudo add-apt-repository ppa:ubuntugis/ppa   sudo apt-get update\nsudo apt-get install gdal-bin  For Windows I found this  guide  from UCLA  To install  Planet-GEE-Pipeline-CLI:  git clone https://github.com/samapriya/Planet-GEE-Pipeline-CLI.git\ncd Planet-GEE-Pipeline-CLI   pip install -r requirements.txt\n\nfor linux use sudo pip install -r requirements.txt  This release also contains a windows installer which bypasses the need for you to have admin permission, it does however require you to have python in the system path meaning when you open up command prompt you should be able to type python and start it within the command prompt window. Post installation using the installer you can just call ppipe using the command prompt similar to calling python. Give it a go post installation type  ppipe -h  Installation is an optional step; the application can be also run directly by executing ppipe.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. To install run  python setup.py develop or python setup.py install\n\nIn a linux distribution\nsudo python setup.py develop or sudo python setup.py install", 
            "title": "Installation"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#getting-started", 
            "text": "As usual, to print help:  usage: ppipe.py [-h]\n                {\n                ,planetkey,aoijson,activatepl,space,downloadpl,metadata,ee_user,create,upload,lst,collsize,delete,tasks,taskquery,report,cancel,mover,copy,access,collprop,cleanout}\n                ...\n\nPlanet Pipeline with Google Earth Engine Batch Addons\n\npositional arguments:\n  { ,planetkey,aoijson,activatepl,space,downloadpl,metadata,ee_user,create,upload,lst,collsize,delete,tasks,taskquery,report,cancel,mover,copy,access,collprop,cleanout}\n                        ---------------------------------------\n                        -----Choose from Planet Tools Below-----\n                        ---------------------------------------\n    planetkey           Enter your planet API Key\n    aoijson             Tool to convert KML, Shapefile,WKT,GeoJSON or Landsat\n                        WRS PathRow file to AreaOfInterest.JSON file with\n                        structured query for use with Planet API 1.0\n    activatepl          Tool to query and/or activate Planet Assets\n    space               Tool to query total download size of activated assets\n                          local space left for download\n    downloadpl          Tool to download Planet Assets\n    metadata            Tool to tabulate and convert all metadata files from\n                        Planet or Digital Globe Assets\n                        -------------------------------------------\n                        ----Choose from Earth Engine Tools Below----\n                        -------------------------------------------\n    ee_user             Get Earth Engine API Key   Paste it back to Command\n                        line/shell to change user\n    create              Allows the user to create an asset collection or\n                        folder in Google Earth Engine\n    upload              Batch Asset Uploader.\n    lst                 List assets in a folder/collection or write as text\n                        file\n    collsize            Collects collection size in Human Readable form  \n                        Number of assets\n    delete              Deletes collection and all items inside. Supports\n                        Unix-like wildcards.\n    tasks               Queries currently running, enqued,failed\n    taskquery           Queries currently running, enqued,failed ingestions\n                        and uploaded assets\n    report              Create a report of all tasks and exports to a CSV file\n    cancel              Cancel all running tasks\n    mover               Moves all assets from one collection to another\n    copy                Copies all assets from one collection to another:\n                        Including copying from other users if you have read\n                        permission to their assets\n    access              Sets Permissions for Images, Collection or all assets\n                        in EE Folder Example: python ee_permissions.py --mode\n                         folder  --asset  users/john/doe  --user\n                         jimmy@doe.com:R \n    collprop            Sets Overall Properties for Image Collection\n    cleanout            Clear folders with datasets from earlier downloaded\n\noptional arguments:\n  -h, --help            show this help message and exit  To obtain help for a specific functionality, simply call it with  help \nswitch, e.g.:  ppipe upload -h . If you didn't install ppipe, then you\ncan run it just by going to  ppipe  directory and running  python\nppipe.py [arguments go here]", 
            "title": "Getting started"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#batch-uploader", 
            "text": "The script creates an Image Collection from GeoTIFFs in your local\ndirectory. By default, the collection name is the same as the local\ndirectory name; with optional parameter you can provide a different\nname. Another optional parameter is a path to a CSV file with metadata\nfor images, which is covered in the next section: Parsing metadata .  usage: ppipe.py upload [-h] --source SOURCE --dest DEST [-m METADATA]\n                       [-mf MANIFEST] [--large] [--nodata NODATA] [-u USER]\n                       [-s SERVICE_ACCOUNT] [-k PRIVATE_KEY] [-b BUCKET]\n\noptional arguments:\n  -h, --help            show this help message and exit\n\nRequired named arguments.:\n  --source SOURCE       Path to the directory with images for upload.\n  --dest DEST           Destination. Full path for upload to Google Earth\n                        Engine, e.g. users/pinkiepie/myponycollection\n  -u USER, --user USER  Google account name (gmail address).\n\nOptional named arguments:\n  -m METADATA, --metadata METADATA\n                        Path to CSV with metadata.\n  -mf MANIFEST, --manifest MANIFEST\n                        Manifest type to be used,for planetscope use\n                         planetscope \n  --large               (Advanced) Use multipart upload. Might help if upload\n                        of large files is failing on some systems. Might cause\n                        other issues.\n  --nodata NODATA       The value to burn into the raster as NoData (missing\n                        data)\n  -s SERVICE_ACCOUNT, --service-account SERVICE_ACCOUNT\n                        Google Earth Engine service account.\n  -k PRIVATE_KEY, --private-key PRIVATE_KEY\n                        Google Earth Engine private key file.\n  -b BUCKET, --bucket BUCKET\n                        Google Cloud Storage bucket name.", 
            "title": "Batch uploader"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#parsing-metadata", 
            "text": "By metadata we understand here the properties associated with each image. Thanks to these, GEE user can easily filter collection based on specified criteria. The file with metadata should be organised as follows:     filename (without extension)  property1 header  property2 header      file1  value1  value2    file2  value3  value4     Note that header can contain only letters, digits and underscores.  Example:     id_no  class  category  binomial  system:time_start      my_file_1  GASTROPODA  EN  Aaadonta constricta  1478943081000    my_file_2  GASTROPODA  CR  Aaadonta irregularis  1478943081000     The corresponding files are my_file_1.tif and my_file_2.tif. With each of the files five properties are associated: id_no, class, category, binomial and system:time_start. The latter is time in Unix epoch format, in milliseconds, as documented in GEE glosary. The program will match the file names from the upload directory with ones provided in the CSV and pass the metadata in JSON format:  { id_no: my_file_1, class: GASTROPODA, category: EN, binomial: Aaadonta constricta, system:time_start: 1478943081000}  The program will report any illegal fields, it will also complain if not all of the images passed for upload have metadata associated. User can opt to ignore it, in which case some assets will have no properties.  Having metadata helps in organising your asstets, but is not mandatory - you can skip it.", 
            "title": "Parsing metadata"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#usage-examples", 
            "text": "Usage examples have been segmented into two parts focusing on both planet tools as well as earth engine tools, earth engine tools include additional developments in CLI which allows you to recursively interact with their python API", 
            "title": "Usage examples"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#planet-tools", 
            "text": "The Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.", 
            "title": "Planet Tools"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#planet-key", 
            "text": "This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools  usage: ppipe.py planetkey [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit  If using on a private machine the Key is saved as a csv file for all future runs of the tool.", 
            "title": "Planet Key"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#aoi-json", 
            "text": "The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.  usage: ppipe.py aoijson [-h] [--start START] [--end END] [--cloud CLOUD]\n                     [--inputfile INPUTFILE] [--geo GEO] [--loc LOC]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --start START         Start date in YYYY-MM-DD?\n  --end END             End date in YYYY-MM-DD?\n  --cloud CLOUD         Maximum Cloud Cover(0-1) representing 0-100\n  --inputfile INPUTFILE\n                        Choose a kml/shapefile/geojson or WKT file for\n                        AOI(KML/SHP/GJSON/WKT) or WRS (6 digit RowPath\n                        Example: 023042)\n  --geo GEO             map.geojson/aoi.kml/aoi.shp/aoi.wkt file\n  --loc LOC             Location where aoi.json file is to be stored", 
            "title": "AOI JSON"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#activate-or-check-asset", 
            "text": "The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier  usage: ppipe.py activatepl [-h] [--aoi AOI] [--action ACTION] [--asst ASST]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --aoi AOI        Choose aoi.json file created earlier\n  --action ACTION  choose between check/activate\n  --asst ASST      Choose between planet asset types (PSOrthoTile\n                   analytic/REOrthoTile analytic/PSOrthoTile\n                   analytic_xml/REOrthoTile analytic_xml", 
            "title": "Activate or Check Asset"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#check-total-size-of-assets", 
            "text": "It is important to sometimes estimate the overall size of download before you can actually download activated assets. This tool allows you to estimate local storage available at any location and overall size of download in MB or GB. This tool makes use of an existing url get request to look at content size and estimate overall download size of download for the activated assets.  usage: ppipe.py space [-h] [--aoi AOI] [--local LOCAL] [--asset ASSET]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --aoi AOI      Choose aoi.json file created earlier\n  --local LOCAL  local path where you are downloading assets\n  --asset ASSET  Choose between planet asset types (PSOrthoTile\n                 analytic/PSOrthoTile analytic_dn/PSOrthoTile\n                 visual/PSScene4Band analytic/PSScene4Band\n                 analytic_dn/PSScene3Band analytic/PSScene3Band\n                 analytic_dn/PSScene3Band visual/REOrthoTile\n                 analytic/REOrthoTile visual", 
            "title": "Check Total size of assets"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#download-asset", 
            "text": "Having metadata helps in organising your asstets, but is not mandatory - you can skip it.\nThe downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types.  usage: ppipe.py downloadpl [-h] [--aoi AOI] [--action ACTION] [--asst ASST]\n                           [--pathway PATHWAY]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --aoi AOI          Choose aoi.json file created earlier\n  --action ACTION    choose download\n  --asst ASST        Choose between planet asset types (PSOrthoTile\n                     analytic/REOrthoTile analytic/PSOrthoTile\n                     analytic_xml/REOrthoTile analytic_xml\n  --pathway PATHWAY  Folder Pathways where PlanetAssets are saved exampled\n                     ./PlanetScope ./RapidEye", 
            "title": "Download Asset"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#metadata-parser", 
            "text": "The metadata tab is a more powerful tool and consists of metadata parsing for All PlanetScope and RapiEye Assets along with Digital Globe MultiSpectral and DigitalGlobe PanChromatic datasets. This was developed as a standalone to process xml metadata files from multiple sources and is important step is the user plans to upload these assets to Google Earth Engine.  usage: ppipe.py metadata [-h] [--asset ASSET] [--mf MF] [--mfile MFILE]\n                      [--errorlog ERRORLOG]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --asset ASSET        Choose PS OrthoTile(PSO)|PS OrthoTile DN(PSO_DN)|PS\n                       OrthoTile Visual(PSO_V)|PS4Band Analytic(PS4B)|PS4Band\n                       DN(PS4B_DN)|PS3Band Analytic(PS3B)|PS3Band\n                       DN(PS3B_DN)|PS3Band Visual(PS3B_V)|RE OrthoTile\n                       (REO)|RE OrthoTile Visual(REO_V)|DigitalGlobe\n                       MultiSpectral(DGMS)|DigitalGlobe Panchromatic(DGP)?\n  --mf MF              Metadata folder?\n  --mfile MFILE        Metadata filename to be exported along with Path.csv\n  --errorlog ERRORLOG  Errorlog to be exported along with Path.csv", 
            "title": "Metadata Parser"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#earth-engine-tools", 
            "text": "The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. This is also a seperate package for earth engine users to use and can be downloaded  here", 
            "title": "Earth Engine Tools"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#ee-user", 
            "text": "This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back", 
            "title": "EE User"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#create", 
            "text": "This tool allows you to create a collection or folder in your earth engine root directory. The tool uses the system cli to achieve this and this has been included so as to reduce the need to switch between multiple tools and CLI.  usage: ppipe.py create [-h] --typ TYP --path PATH\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --typ TYP    Specify type: collection or folder\n  --path PATH  This is the path for the earth engine asset to be created full\n               path is needsed eg: users/johndoe/collection", 
            "title": "Create"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#upload-a-directory-with-images-to-your-myfoldermycollection-and-associate-properties-with-each-image", 
            "text": "ppipe upload -u johndoe@gmail.com --source path_to_directory_with_tif -m path_to_metadata.csv -mf maifest_type(ex:planetscope) --dest users/johndoe/myfolder/myponycollection  The script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.You can also skip manifest for RapidEye imagery or any other source that does not require metadata field type handling.", 
            "title": "Upload a directory with images to your myfolder/mycollection and associate properties with each image:"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#upload-a-directory-with-images-with-specific-nodata-value-to-a-selected-destination", 
            "text": "ppipe upload -u johndoe@gmail.com --source path_to_directory_with_tif --dest users/johndoe/myfolder/myponycollection --nodata 222  In this case we need to supply full path to the destination, which is helpful when we upload to a shared folder. In the provided example we also burn value 222 into all rasters for missing data (NoData).", 
            "title": "Upload a directory with images with specific NoData value to a selected destination"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#asset-list", 
            "text": "This tool is designed to either print or output asset lists within folders or collections using earthengine ls tool functions.  usage: geeadd.py lst [-h] --location LOCATION --typ TYP [--items ITEMS]\n                     [--output OUTPUT]\n\noptional arguments:\n  -h, --help           show this help message and exit\n\nRequired named arguments.:\n  --location LOCATION  This it the location of your folder/collection\n  --typ TYP            Whether you want the list to be printed or output as\n                       text[print/report]\n\nOptional named arguments:\n  --items ITEMS        Number of items to list\n  --output OUTPUT      Folder location for report to be exported", 
            "title": "Asset List"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#asset-size", 
            "text": "This tool allows you to query the size of any Earth Engine asset[Images, Image Collections, Tables and Folders] and prints out the number of assets and total asset size in non-byte encoding meaning KB, MB, GB, TB depending on size.  usage: geeadd assetsize [-h] --asset ASSET\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --asset ASSET  Earth Engine Asset for which to get size properties", 
            "title": "Asset Size"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#earth-engine-asset-report", 
            "text": "This tool recursively goes through all your assets(Includes Images, ImageCollection,Table,) and generates a report containing the following fields\n[Type,Asset Type, Path,Number of Assets,size(MB),unit,owner,readers,writers].  usage: geeadd.py ee_report [-h] --outfile OUTFILE\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --outfile OUTFILE  This it the location of your report csv file  A simple setup is the following geeadd --outfile \"C:\\johndoe\\report.csv\"", 
            "title": "Earth Engine Asset Report"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#task-query", 
            "text": "This script counts all currently running and ready tasks along with failed tasks.  usage: geeadd.py tasks [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\ngeeadd.py tasks", 
            "title": "Task Query"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#task-report", 
            "text": "Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type  usage: geeadd taskreport [-h] [--r R]\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --r R       Folder Path where the reports will be saved", 
            "title": "Task Report"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#delete-a-collection-with-content", 
            "text": "The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution!  geeadd delete users/johndoe/test  Console output:  2016-07-17 16:14:09,212 :: oauth2client.client :: INFO :: Attempting refresh to obtain initial access_token\n2016-07-17 16:14:09,213 :: oauth2client.client :: INFO :: Refreshing access_token\n2016-07-17 16:14:10,842 :: root :: INFO :: Attempting to delete collection test\n2016-07-17 16:14:16,898 :: root :: INFO :: Collection users/johndoe/test removed", 
            "title": "Delete a collection with content:"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#delete-all-directories-collections-based-on-a-unix-like-pattern", 
            "text": "geeadd delete users/johndoe/*weird[0-9]?name*", 
            "title": "Delete all directories / collections based on a Unix-like pattern"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#assets-move", 
            "text": "This script allows us to recursively move assets from one collection to the other.  usage: geeadd.py mover [-h] [--assetpath ASSETPATH] [--finalpath FINALPATH]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --assetpath ASSETPATH\n                        Existing path of assets\n  --finalpath FINALPATH\n                        New path for assets\ngeeadd.py mover --assetpath  users/johndoe/myfolder/myponycollection  --destination  users/johndoe/myfolder/myotherponycollection", 
            "title": "Assets Move"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#assets-copy", 
            "text": "This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections.  usage: geeadd.py copy [-h] [--initial INITIAL] [--final FINAL]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --initial INITIAL  Existing path of assets\n  --final FINAL      New path for assets\ngeeadd.py mover --initial  users/johndoe/myfolder/myponycollection  --final  users/johndoe/myfolder/myotherponycollection", 
            "title": "Assets Copy"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#assets-access", 
            "text": "This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time.  usage: geeadd access [-h] --mode MODE --asset ASSET --user USER\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --mode MODE    This lets you select if you want to change permission or\n                 folder/collection/image\n  --asset ASSET  This is the path to the earth engine asset whose permission\n                 you are changing folder/collection/image\n  --user USER    This is the email address to whom you want to give read or\n                 write permission Usage:  john@doe.com:R  or  john@doe.com:W \n                 R/W refers to read or write permission\ngeeadd.py access --mode folder --asset  folder/collection/image  --user  john@doe.com:R", 
            "title": "Assets Access"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#set-collection-property", 
            "text": "This script is derived from the ee tool to set collection properties and will set overall properties for collection.  usage: geeadd.py collprop [-h] [--coll COLL] [--p P]\n\noptional arguments:\n  -h, --help   show this help message and exit\n  --coll COLL  Path of Image Collection\n  --p P         system:description=Description / system:provider_url=url / sys\n               tem:tags=tags / system:title=title", 
            "title": "Set Collection Property"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#cancel-all-tasks", 
            "text": "This is a simpler tool, can be called directly from the earthengine cli as well  earthengine cli command\nearthengine task cancel all\n\nusage: geeadd.py cancel [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit", 
            "title": "Cancel all tasks"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#credits", 
            "text": "JetStream  A portion of the work is suported by JetStream Grant TG-GEO160014.  Also supported by  Planet Labs Ambassador Program  Original upload function adapted from  Lukasz's asset manager tool", 
            "title": "Credits"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#changelog", 
            "text": "", 
            "title": "Changelog"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#v030", 
            "text": "Allows for quiet authentication for use in Google Colab or non interactive environments  Improved planet key entry and authentication protocols", 
            "title": "v0.3.0"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#v0291", 
            "text": "Fixed issue with Surface Reflectance metadata and manifest lib  Improved ingestion support for (PSScene4Band analytic_Sr)[PS4B_SR]", 
            "title": "v0.2.91"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#v029", 
            "text": "Fixed issues with generating id list  Improved overall security of command calls", 
            "title": "v0.2.9"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#v022", 
            "text": "Major improvements to ingestion using manifest ingest in Google Earth Engine  Contains manifest for all commonly used Planet Data item and asset combinations  Added additional tool to Earth Engine Enhancement including quota check before upload to GEE", 
            "title": "v0.2.2"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#v021", 
            "text": "Fixed initialization loop issue", 
            "title": "v0.2.1"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#v020", 
            "text": "Metadata parser and Uploader Can now handle PlanetScope 4 Band Surface Reflectance Datasets  General Improvements", 
            "title": "v0.2.0"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#v019", 
            "text": "Changes made to reflect updated GEE Addon tools  general improvements", 
            "title": "v0.1.9"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_cli/#v018", 
            "text": "Minor fixes to parser and general improvements  Planet Key is now stored in a configuration folder which is safer \"C:\\users.config\\planet\"  Earth Engine now requires you to assign a field type for metadata meaning an alphanumeric column like satID cannot also have numeric values unless specified explicitly . Manifest option has been added to handle this (just use -mf \"planetscope\")  Added capability to query download size and local disk capacity before downloading planet assets.  Added the list function to generate list of collections or folders including reports  Added the collection size tool which allows you to estimate total size or quota used from your allocated quota.  ogr2ft feature is removed since Earth Engine now allows vector and table uploading.", 
            "title": "v0.1.8"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/", 
            "text": "Clip Ship Planet CLI addon\n\n\n\n\n\n\n\n\nNote: The \nClips API has been deprecated\n - and will no longer be supporting the standalone Clip and Ship service. Deprecation means that there is intention to remove the service at some point; it DOES NOT mean end-of-life for the service, yet. (Deprecate and end-of-life are two distinct terms.) There should be a plan to replace Clips API with new pre-processing functionality, but no new announcement or timetable for the launch of the new service and the removal (end-of-life) for Clips API has been made.\n\n\nPlanet's Clip API was a compute API designed to allowed users to clip the images to their area of interest. This would save them time in preprocessing and also allow the user to save on their area quota which might have restrictions. Based on Planet's Education and Research Program this quota is set at 10,000 square kilometers a month, which means saving up on quota is very useful. The discussion also led to an important clarification that users are in fact charged only for the area downloaded post clip if using the clip operation and hence this tool. This tool takes a sequential approach from activation to generating a clip request for multiple images activated and then processing the download tokens to actually download the clipped image files. The tool also consists of a sort function which allows the user to extract the files and sort them by type and deleting the original files to save on space.\n\n\nInstallation\n\n\nTo install the Clip-Ship-Planet-CLI you can simply perform the following action with Linux(Tested on Ubuntu 16):\n\n\ngit clone https://github.com/samapriya/Clip-Ship-Planet-CLI.git\ncd Clip-Ship-Planet-CLI \n pip install -r requirements.txt\n\n\n\n\nOn a windows as well as a linux machine, installation is an optional step; the application can also be run directly by executing pclip.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment but you can also install it to system python and should not create any conflicts. To install on windows download the setup files as a zip package, unpack and run\n\n\npython setup.py develop or python setup.py install\n\n\n\n\nIn a linux distribution\n\n\nsudo python setup.py develop or sudo python setup.py install\npclip -h\n\n\n\n\n\n\nTable of contents\n\n\n\n\nGetting started\n\n\nUsage examples\n\n\nPlanet Key\n\n\n\n\n\n\nPlanet Quota\n\n\nAOI JSON\n\n\nActivate or Check Asset\n\n\nList IDs\n\n\n\n\n\n\nClipping with GeoJSON\n\n\nClipping with JSON\n\n\nDownloading Clipped Imagery\n\n\nSorting\n\n\n\n\n\n\n\n\nGetting started\n\n\nAs usual, to print help:\n\n\nPlanet Clip Tools CLI\n\npositional arguments:\n  { ,planetkey,aoijson,activate,aoiupdate,idlist,geojsonc,jsonc,downloadclips,sort}\n                        -------------------------------------------\n                        -----Choose from Planet Clip Tools-----\n                        -------------------------------------------\n    planetkey           Enter your planet API Key\n    quota               Prints your quota details\n    aoijson             Tool to convert KML, Shapefile,WKT,GeoJSON or Landsat\n                        WRS PathRow file to AreaOfInterest.JSON file with\n                        structured query for use with Planet API 1.0\n    activate            Tool to query and/or activate Planet Assets\n    idlist              Allows users to generate an id list for the selected\n                        item and asset type for example item_asset=\n                        PSOrthoTile analytic/PSScene3Band visual. This is used\n                        with the clip tool\n    geojsonc            Allows users to batch submit clipping request to the\n                        Planet Clip API using geometry in geojson file\n    jsonc               Allows users to batch submit clipping request to the\n                        Planet Clip API using geometry in structured json\n                        file. This is preferred because the structured JSON\n                        allows the activate tool to stream line asset ids\n                        being requested and to extract geometry from the same\n                        file\n    downloadclips       Allows users to batch download clipped assets post\n                        computation using a directory path(Requires you to\n                        first activate and run geojson or json tool)\n    sort                Allows users to unzip downloaded files to new folder\n                        and sorts into images and metadata\n\noptional arguments:\n  -h, --help            show this help message and exit\n\n\n\n\nUsage examples\n\n\nThe tools have been designed to follow a sequential setup from activation, clip, download and even sort and includes steps that help resolve additional issues a user might face trying to download clipped area of interests instead of entire scenes. The system will ask you to enter your API key before the CLI starts(this will prompt you only once to change API key use the Planet Key tool).\n\n\nPlanet Key\n\n\nThis tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools. Ites makes use of the Planet client and esentially executes \nplanet init\n\n\nusage: pclip planetkey [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\n\n\n\nPlanet Quota\n\n\nThis tool prints details on your existing quota and your area remaining\n\n\nusage: pclip quota\n\noptional arguments:\n  -h, --help  show this help message and exit\n\n\n\n\nAOI JSON\n\n\nThe aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.\n\n\nusage: pclip aoijson [-h] [--start START] [--end END] [--cloud CLOUD]\n                     [--inputfile INPUTFILE] [--geo GEO] [--loc LOC]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --start START         Start date in YYYY-MM-DD?\n  --end END             End date in YYYY-MM-DD?\n  --cloud CLOUD         Maximum Cloud Cover(0-1) representing 0-100\n  --inputfile INPUTFILE\n                        Choose a kml/shapefile/geojson or WKT file for\n                        AOI(KML/SHP/GJSON/WKT) or WRS (6 digit RowPath\n                        Example: 023042)\n  --geo GEO             map.geojson/aoi.kml/aoi.shp/aoi.wkt file\n  --loc LOC             Location where aoi.json file is to be stored\n\n\n\n\nAs with the \nPlanet-GEE-Pipeline-CLI\n the aoijson tool allows the user to bring any filetype of interest, which includes GEOJSON, WKT, KML or SHP file including but not limited to WRS rowpath setup and structures it to enable filtered query using Planet's data API. A simple setup would be\n\n\npclip aoijson --start \"2017-06-01\" --end \"2017-12-31\" --cloud \"0.15\" --inputfile \"GJSON\" --geo \"C:\\planet\\myarea.geojson\" --loc \"C:\\planet\"\n\n\nthe output is always named as aoi.json.\n\n\nActivate or Check Asset\n\n\nThe activate tool allows the users to either check or activate planet assets. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier. This is a necessary step since the clip API can only work with those ID(s) which have been activated. In the future the list ID tool will check for number of activated id and wait for all of them to be activated before generating an ID list.\n\n\nusage: pclip activate [-h] [--aoi AOI] [--action ACTION] [--asst ASST]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --aoi AOI        Choose aoi.json file created earlier\n  --action ACTION  choose between check/activate\n  --asset ASST      Choose between planet asset types (PSOrthoTile\n                   analytic/PSOrthoTile analytic_dn/PSOrthoTile\n                   visual/PSScene4Band analytic/PSScene4Band\n                   analytic_dn/PSScene3Band analytic/PSScene3Band\n                   analytic_dn/PSScene3Band visual/REOrthoTile\n                   analytic/REOrthoTile visual\n\n\n\n\nAn example setup for asset activation is the following\n\n\npclip activate --aoi \"C:\\planet\\aoi.json\" --action \"activate\" --asset \"PSOrthoTile analytic\"\n\n\nList IDs\n\n\nThe next step is to list ID(s) that you have activated, this creates a temporary file containing the list of ID(s) which can be used to iteratively call the clips API. This is a modification of the activation function to use only the item id instead of item type and asset id and write to file for future use.\n\n\nusage: pclip idlist [-h] [--aoi AOI] [--asset ASSET]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --aoi AOI      Input path to the structured json file from which we will\n                 generate the clips\n  --asset ASSET  Choose from asset type for example:\nPSOrthoTile\n                 analytic\n|\nREOrthoTile analytic\n\n\n\n\n\nThe example setup for this command is the following\n\n\npclip idlist --aoi \u201cC:\\planet\\aoi.json\u201d --asset \u201cPSOrthoTile analytic\u201d\n\n\nClipping with GeoJSON\n\n\nA geejson file can be used directly to clip and query the area of interest and then submit clip process. I added this is a functionality but want to make clear that this does not take into consideration any other filters such as cloud cover or start and end date, and hence should be used only when you do not need to apply any filter.\n\n\nusage: pclip geojsonc [-h] [--path PATH] [--item ITEM] [--asset ASSET]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --path PATH    Path to the geojson file including filename (Example:\n                 C:\\users ile.geojson)\n  --item ITEM    Choose from item type for example:\nPSOrthoTile\n,\nREOrthoTile\n\n  --asset ASSET  Choose from asset type for example: \nvisual\n,\nanalytic\n\n  ```\n A simple setup for the JSON tool is the following\n\n```pclip geojsonc --path \u201cC:\\planet\\aoi.geojson\u201d --item \u201cPSOrthoTile\u201d --asset \u201canalytic\n```\n\n### Clipping with JSON\nThis is the preferred style of submitting the clip requests using the IDlist we generated earlier. This is already structured before even activating assets and includes the additional filters you might have used for selecting the images.\n\n\n\n\nusage: pclip jsonc [-h] [--path PATH] [--item ITEM] [--asset ASSET]\n\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --path PATH    Path to the json file including filename (Example: C:\\users\n                 ile.json)\n  --item ITEM    Choose from item type for example:\"PSOrthoTile\",\"REOrthoTile\"\n  --asset ASSET  Choose from asset type for example: \"visual\",\"analytic\"\n  ```\nA simple setup for the JSON tool is the following\n\n\npclip jsonc --path \u201cC:\\planet\\aoi.json\u201d --item \u201cPSOrthoTile\u201d --asset \u201canalytic\"\n\n\nDownloading Clipped Imagery\n\n\nThe last step includes providing a location where the clipped imagery can be downloaded. This includes the zip files that are generated from the earlier step and include a download token that expires over time. This batch downloads the clipped zip files to destination directory\n\n\nusage: pclip downloadclips [-h] [--dir DIR]\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --dir DIR   Output directory to save the assets. All files are zipped and\n              include metadata\n\n\n\n\nA simple setup includes just the location to the download directory for the zipped \n clipped files to be downloaded\n\n\npclip downloadclips --dir \u201cC:\\planet\\zipped\n\n\n\n\n\nSorting\n\n\nAs an additional measure and because it makes arranging and handling datasets easily, this setup comes completed with a sort tool. If a output directory is provided for the unzipped files, the tool unzips all files, moves the images and metadata to seperate directories and then deletes the original zipped files to save space. \n\n\nusage: pclip sort [-h] [--zipped ZIPPED] [--unzipped UNZIPPED]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --zipped ZIPPED      Folder containing downloaded clipped files which are\n                       zipped\n  --unzipped UNZIPPED  Folder where you want your files to be unzipped and\n                       sorted\n\n\n\n\nA simple would be the following (Images and metadata are sorted into an image and metadata folder inside the unzipped files folder)\n\n\npclip sort --zipped \u201cC:\\planet\\zipped\u201d --unzipped \u201cC:\\planet\\unzipped\u201d\n\n\n\n\nChangelog\n\n\nv0.2.2\n\n\n\n\nImproved Planet Key Handler\n\n\nAdded new tool to insepect planet account quota\n\n\n\n\nv0.2.1\n\n\n\n\nThanks to commit suggested by \nRabscuttler\n\n\nFixed issues with help text and installer\n\n\n\n\nv0.2.0\n\n\n\n\nFixed issues with config files\n\n\n\n\nv0.1.9\n\n\n\n\nNow handles running and succeeded status better\n\n\nNow enumerates during clip and download to allow user estimates on number of assets clipped and/or downloaded\n\n\n\n\nv0.1.8\n\n\n\n\nIncludes required packages list within installer\n\n\nRobust GEOJSON Parsing\n\n\n\n\nv0.1.7\n\n\n\n\nFixed issues with processing visual asset types\n\n\nThe Clip function now handles error codes if the post response code is not 202(accepted for processing) then the error code and item and asset type is printed.\n\n\n\n\nv0.1.6\n\n\n\n\nHandles single time input API Key, this is needed only once to start the program\n\n\nFixed issue with base metadata folder during sort\n\n\nUpdated asset argument for asset activation to match styles\n\n\n\n\nv0.1.5\n\n\n\n\nUpdated Requirements.txt to include pyshp\n\n\nFixed subprocess shell error, for now shell=True\n\n\n\n\nv0.1.4\n\n\n\n\nGeneral Improvements\n\n\n\n\nv0.1.3\n\n\n\n\nGeneral Improvements\n\n\n\n\nv0.1.2\n\n\n\n\nTested on Ubuntu 16.04 and now handles permissions problem\n\n\nTemporary files now written to config folders to avoid admin permission\n\n\n\n\nv0.1.1\n\n\n\n\nGeneral Improvements", 
            "title": "Clip Ship Planet CLI"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#clip-ship-planet-cli-addon", 
            "text": "Note: The  Clips API has been deprecated  - and will no longer be supporting the standalone Clip and Ship service. Deprecation means that there is intention to remove the service at some point; it DOES NOT mean end-of-life for the service, yet. (Deprecate and end-of-life are two distinct terms.) There should be a plan to replace Clips API with new pre-processing functionality, but no new announcement or timetable for the launch of the new service and the removal (end-of-life) for Clips API has been made.  Planet's Clip API was a compute API designed to allowed users to clip the images to their area of interest. This would save them time in preprocessing and also allow the user to save on their area quota which might have restrictions. Based on Planet's Education and Research Program this quota is set at 10,000 square kilometers a month, which means saving up on quota is very useful. The discussion also led to an important clarification that users are in fact charged only for the area downloaded post clip if using the clip operation and hence this tool. This tool takes a sequential approach from activation to generating a clip request for multiple images activated and then processing the download tokens to actually download the clipped image files. The tool also consists of a sort function which allows the user to extract the files and sort them by type and deleting the original files to save on space.", 
            "title": "Clip Ship Planet CLI addon"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#installation", 
            "text": "To install the Clip-Ship-Planet-CLI you can simply perform the following action with Linux(Tested on Ubuntu 16):  git clone https://github.com/samapriya/Clip-Ship-Planet-CLI.git\ncd Clip-Ship-Planet-CLI   pip install -r requirements.txt  On a windows as well as a linux machine, installation is an optional step; the application can also be run directly by executing pclip.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment but you can also install it to system python and should not create any conflicts. To install on windows download the setup files as a zip package, unpack and run  python setup.py develop or python setup.py install  In a linux distribution  sudo python setup.py develop or sudo python setup.py install\npclip -h", 
            "title": "Installation"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#table-of-contents", 
            "text": "Getting started  Usage examples  Planet Key    Planet Quota  AOI JSON  Activate or Check Asset  List IDs    Clipping with GeoJSON  Clipping with JSON  Downloading Clipped Imagery  Sorting", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#getting-started", 
            "text": "As usual, to print help:  Planet Clip Tools CLI\n\npositional arguments:\n  { ,planetkey,aoijson,activate,aoiupdate,idlist,geojsonc,jsonc,downloadclips,sort}\n                        -------------------------------------------\n                        -----Choose from Planet Clip Tools-----\n                        -------------------------------------------\n    planetkey           Enter your planet API Key\n    quota               Prints your quota details\n    aoijson             Tool to convert KML, Shapefile,WKT,GeoJSON or Landsat\n                        WRS PathRow file to AreaOfInterest.JSON file with\n                        structured query for use with Planet API 1.0\n    activate            Tool to query and/or activate Planet Assets\n    idlist              Allows users to generate an id list for the selected\n                        item and asset type for example item_asset=\n                        PSOrthoTile analytic/PSScene3Band visual. This is used\n                        with the clip tool\n    geojsonc            Allows users to batch submit clipping request to the\n                        Planet Clip API using geometry in geojson file\n    jsonc               Allows users to batch submit clipping request to the\n                        Planet Clip API using geometry in structured json\n                        file. This is preferred because the structured JSON\n                        allows the activate tool to stream line asset ids\n                        being requested and to extract geometry from the same\n                        file\n    downloadclips       Allows users to batch download clipped assets post\n                        computation using a directory path(Requires you to\n                        first activate and run geojson or json tool)\n    sort                Allows users to unzip downloaded files to new folder\n                        and sorts into images and metadata\n\noptional arguments:\n  -h, --help            show this help message and exit", 
            "title": "Getting started"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#usage-examples", 
            "text": "The tools have been designed to follow a sequential setup from activation, clip, download and even sort and includes steps that help resolve additional issues a user might face trying to download clipped area of interests instead of entire scenes. The system will ask you to enter your API key before the CLI starts(this will prompt you only once to change API key use the Planet Key tool).", 
            "title": "Usage examples"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#planet-key", 
            "text": "This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools. Ites makes use of the Planet client and esentially executes  planet init  usage: pclip planetkey [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit", 
            "title": "Planet Key"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#planet-quota", 
            "text": "This tool prints details on your existing quota and your area remaining  usage: pclip quota\n\noptional arguments:\n  -h, --help  show this help message and exit", 
            "title": "Planet Quota"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#aoi-json", 
            "text": "The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.  usage: pclip aoijson [-h] [--start START] [--end END] [--cloud CLOUD]\n                     [--inputfile INPUTFILE] [--geo GEO] [--loc LOC]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --start START         Start date in YYYY-MM-DD?\n  --end END             End date in YYYY-MM-DD?\n  --cloud CLOUD         Maximum Cloud Cover(0-1) representing 0-100\n  --inputfile INPUTFILE\n                        Choose a kml/shapefile/geojson or WKT file for\n                        AOI(KML/SHP/GJSON/WKT) or WRS (6 digit RowPath\n                        Example: 023042)\n  --geo GEO             map.geojson/aoi.kml/aoi.shp/aoi.wkt file\n  --loc LOC             Location where aoi.json file is to be stored  As with the  Planet-GEE-Pipeline-CLI  the aoijson tool allows the user to bring any filetype of interest, which includes GEOJSON, WKT, KML or SHP file including but not limited to WRS rowpath setup and structures it to enable filtered query using Planet's data API. A simple setup would be  pclip aoijson --start \"2017-06-01\" --end \"2017-12-31\" --cloud \"0.15\" --inputfile \"GJSON\" --geo \"C:\\planet\\myarea.geojson\" --loc \"C:\\planet\"  the output is always named as aoi.json.", 
            "title": "AOI JSON"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#activate-or-check-asset", 
            "text": "The activate tool allows the users to either check or activate planet assets. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier. This is a necessary step since the clip API can only work with those ID(s) which have been activated. In the future the list ID tool will check for number of activated id and wait for all of them to be activated before generating an ID list.  usage: pclip activate [-h] [--aoi AOI] [--action ACTION] [--asst ASST]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --aoi AOI        Choose aoi.json file created earlier\n  --action ACTION  choose between check/activate\n  --asset ASST      Choose between planet asset types (PSOrthoTile\n                   analytic/PSOrthoTile analytic_dn/PSOrthoTile\n                   visual/PSScene4Band analytic/PSScene4Band\n                   analytic_dn/PSScene3Band analytic/PSScene3Band\n                   analytic_dn/PSScene3Band visual/REOrthoTile\n                   analytic/REOrthoTile visual  An example setup for asset activation is the following  pclip activate --aoi \"C:\\planet\\aoi.json\" --action \"activate\" --asset \"PSOrthoTile analytic\"", 
            "title": "Activate or Check Asset"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#list-ids", 
            "text": "The next step is to list ID(s) that you have activated, this creates a temporary file containing the list of ID(s) which can be used to iteratively call the clips API. This is a modification of the activation function to use only the item id instead of item type and asset id and write to file for future use.  usage: pclip idlist [-h] [--aoi AOI] [--asset ASSET]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --aoi AOI      Input path to the structured json file from which we will\n                 generate the clips\n  --asset ASSET  Choose from asset type for example: PSOrthoTile\n                 analytic | REOrthoTile analytic   The example setup for this command is the following  pclip idlist --aoi \u201cC:\\planet\\aoi.json\u201d --asset \u201cPSOrthoTile analytic\u201d", 
            "title": "List IDs"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#clipping-with-geojson", 
            "text": "A geejson file can be used directly to clip and query the area of interest and then submit clip process. I added this is a functionality but want to make clear that this does not take into consideration any other filters such as cloud cover or start and end date, and hence should be used only when you do not need to apply any filter.  usage: pclip geojsonc [-h] [--path PATH] [--item ITEM] [--asset ASSET]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --path PATH    Path to the geojson file including filename (Example:\n                 C:\\users ile.geojson)\n  --item ITEM    Choose from item type for example: PSOrthoTile , REOrthoTile \n  --asset ASSET  Choose from asset type for example:  visual , analytic \n  ```\n A simple setup for the JSON tool is the following\n\n```pclip geojsonc --path \u201cC:\\planet\\aoi.geojson\u201d --item \u201cPSOrthoTile\u201d --asset \u201canalytic ```\n\n### Clipping with JSON\nThis is the preferred style of submitting the clip requests using the IDlist we generated earlier. This is already structured before even activating assets and includes the additional filters you might have used for selecting the images.  usage: pclip jsonc [-h] [--path PATH] [--item ITEM] [--asset ASSET]  optional arguments:\n  -h, --help     show this help message and exit\n  --path PATH    Path to the json file including filename (Example: C:\\users\n                 ile.json)\n  --item ITEM    Choose from item type for example:\"PSOrthoTile\",\"REOrthoTile\"\n  --asset ASSET  Choose from asset type for example: \"visual\",\"analytic\"\n  ```\nA simple setup for the JSON tool is the following  pclip jsonc --path \u201cC:\\planet\\aoi.json\u201d --item \u201cPSOrthoTile\u201d --asset \u201canalytic\"", 
            "title": "Clipping with GeoJSON"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#downloading-clipped-imagery", 
            "text": "The last step includes providing a location where the clipped imagery can be downloaded. This includes the zip files that are generated from the earlier step and include a download token that expires over time. This batch downloads the clipped zip files to destination directory  usage: pclip downloadclips [-h] [--dir DIR]\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --dir DIR   Output directory to save the assets. All files are zipped and\n              include metadata  A simple setup includes just the location to the download directory for the zipped   clipped files to be downloaded  pclip downloadclips --dir \u201cC:\\planet\\zipped", 
            "title": "Downloading Clipped Imagery"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#sorting", 
            "text": "As an additional measure and because it makes arranging and handling datasets easily, this setup comes completed with a sort tool. If a output directory is provided for the unzipped files, the tool unzips all files, moves the images and metadata to seperate directories and then deletes the original zipped files to save space.   usage: pclip sort [-h] [--zipped ZIPPED] [--unzipped UNZIPPED]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --zipped ZIPPED      Folder containing downloaded clipped files which are\n                       zipped\n  --unzipped UNZIPPED  Folder where you want your files to be unzipped and\n                       sorted  A simple would be the following (Images and metadata are sorted into an image and metadata folder inside the unzipped files folder)  pclip sort --zipped \u201cC:\\planet\\zipped\u201d --unzipped \u201cC:\\planet\\unzipped\u201d", 
            "title": "Sorting"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#changelog", 
            "text": "", 
            "title": "Changelog"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#v022", 
            "text": "Improved Planet Key Handler  Added new tool to insepect planet account quota", 
            "title": "v0.2.2"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#v021", 
            "text": "Thanks to commit suggested by  Rabscuttler  Fixed issues with help text and installer", 
            "title": "v0.2.1"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#v020", 
            "text": "Fixed issues with config files", 
            "title": "v0.2.0"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#v019", 
            "text": "Now handles running and succeeded status better  Now enumerates during clip and download to allow user estimates on number of assets clipped and/or downloaded", 
            "title": "v0.1.9"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#v018", 
            "text": "Includes required packages list within installer  Robust GEOJSON Parsing", 
            "title": "v0.1.8"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#v017", 
            "text": "Fixed issues with processing visual asset types  The Clip function now handles error codes if the post response code is not 202(accepted for processing) then the error code and item and asset type is printed.", 
            "title": "v0.1.7"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#v016", 
            "text": "Handles single time input API Key, this is needed only once to start the program  Fixed issue with base metadata folder during sort  Updated asset argument for asset activation to match styles", 
            "title": "v0.1.6"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#v015", 
            "text": "Updated Requirements.txt to include pyshp  Fixed subprocess shell error, for now shell=True", 
            "title": "v0.1.5"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#v014", 
            "text": "General Improvements", 
            "title": "v0.1.4"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#v013", 
            "text": "General Improvements", 
            "title": "v0.1.3"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#v012", 
            "text": "Tested on Ubuntu 16.04 and now handles permissions problem  Temporary files now written to config folders to avoid admin permission", 
            "title": "v0.1.2"
        }, 
        {
            "location": "/projects/clip_ship_planet_cli/#v011", 
            "text": "General Improvements", 
            "title": "v0.1.1"
        }, 
        {
            "location": "/projects/planet_batch_slack/", 
            "text": "Planet Batch \n Slack Pipeline CLI\n\n\n\n\n\n\n\n\n\n\n Planet Labs(Full line up of Satellites) and Planet \n Slack Technologies\nLogo\n\n\nFor writing a readme file this time I have adapted a shared piece written for the medium article. The first part which is setting up the slack account, creating an application and a slack bot has been discussed in the \narticle here\n. In the past I have written tools which act as pipelines for you to process single areas of interest at the time that could be chained, the need to write something that does a bit more heavy lifting arose. This command line interface(CLI) application was created to handle groups and teams that have multiple areas of interest and multiple input and output buckets and locations to function smoothly. I have integrated this to slack so you can be on the move while this task can be on a scheduler and update you when finished.\n\n\nTable of contents\n\n\n\n\nInstallation\n\n\nGetting started\n\n\nBatch Approach to Structured JSON\n\n\nBatch Activation\n\n\nBatch Download and Balance\n\n\nAdditional Tools\n\n\nCitation and Credits\n\n\nChangelog\n\n\n\n\nInstallation\n\n\nThe next step we will setup the \nPlanet-Batch-Slack-Pipeline-CLI\n and integrate our previously built slack app for notifications. To setup the prerequisites you need to install the Planet\nPython API Client and Slack Python API Clients.\n* To install the tool you can go to the GitHub page at \nPlanet-Batch-Slack-Pipeline-CLI\n. As always two of my favorite operating systems are Windows and Linux, and to install on Linux\n\n\ngit clone https://github.com/samapriya/Planet-Batch-Slack-Pipeline-CLI.git\ncd Planet-Batch-Slack-Pipeline-CLI \n sudo python setup.py install\npip install -r requirements.txt\n\n\n\n\n\n\nfor windows download the zip and after extraction go to the folder containing \"setup.py\" and open command prompt at that location and type\n\n\n\n\npython setup.py install  \npip install -r requirements.txt\n\n\n\n\nNow call the tool for the first time, by typing in \npbatch -h\n. This will only work if you have python in the system path which you can test for opening up terminal or command prompt and typing \npython.\n\n\nGetting started\n\n\nOnce the requirements have been satisfied the first thing we would setup would be the OAuth Keys we created. The tools consists of a bunch of slack tools as well including capability to just use this tool to send slack messages, attachments and clean up channel as needed.\n\n\n\nPlanet Batch Tools and Slack Addons Interface\n\n\nThe two critical setup tools to make Slack ready and integrated are the smain and sbot tools where you will enter the OAuth for the application and OAuth for the bot that you generated earlier. These are then stored into your session for future use, you can call them using\n\n\npbatch smain\nUse the \"\nOAuth Access Token\n\" generated earlier\n. You can find the \ntutorial here\n\n\npbatch sbot\nUse \"\nBot User OAuth Access Token\" \ngenerated earlier\n. You can find the \ntutorial here\n\n\nOnce this is done your bot is now setup to message you when a task is completed. In our case these are tied into individual tools within the batch toolkit we just installed.\n\n\nTo be clear these tools were designed based on what I thought was an effective way of looking at data, downloading them and chaining the processes together. They are still a set of individual tools to make sure that one operation is independent of the other and does not break in case of a problem. So a non-monolithic design in some sense to make sure the pieces work. We will go through each of them in the order of use \npbatch planetkey\n is the obvious one which is your planet API key and will\nallow you to store this locally to a session.\n\n\nThe \naoijson\n tool is the same tool used in the Planet-EE CLI within the pbatch bundle allows you to bring any existing KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles to a structured geojson file, this is so that the Planet API can read the structured geojson which has additional filters such as cloud cover and range of dates. The tool can then allow you to\nconvert the geojson file to include filters to be used with the Planet Data API.\n\n\nLet us say we want to convert this \nmap.geojson\n to a structured aoi.json from \nJune 1 2017 to June 30th 2017 with 15% cloud cover as our maximum\n. We would pass the following command\n\n\npbatch.py aoijson --start \n2017-06-01\n --end \n2017-06-30\n --cloud \n0.15\n --inputfile \nGJSON\n --geo \nlocal path to map.geojson file\n --loc \npath where aoi.json output file will be created\n\n\n\n\n\nBatch Approach to Structured JSON\n\n\nThis tool was then rewritten and included in the application to overcome two issues 1) Automatically detect the type of input file I am passing (For now it can automatically handle geojson, kml, shapefile and wkt files). The files are then saved in an output directory with the \nfilename_aoi.json . \n\n\n\n\nThe tool can also read from a csv file and parse different start dates, end dates and cloud cover for different files and create structured jsons to multiple locations making an multi path export easy. The csv headers should be\nThe csv file needs to have following headers and setup\n\n\n\n\n\n\n\n\npathways\n\n\nstart\n\n\nend\n\n\ncloud\n\n\noutdir\n\n\n\n\n\n\n\n\n\n\nC:\\demo\\dallas.geojson\n\n\n2017-01-01\n\n\n2017-01-02\n\n\n0.15\n\n\nC:\\demo\n\n\n\n\n\n\nC:\\demo\\denver.geojson\n\n\n2017-01-01\n\n\n2017-03-02\n\n\n0.15\n\n\nC:\\demo\n\n\n\n\n\n\nC:\\demo\\sfo.geojson\n\n\n2017-01-01\n\n\n2017-05-02\n\n\n0.15\n\n\nC:\\demo\n\n\n\n\n\n\nC:\\demo\\indy.geojson\n\n\n2017-01-01\n\n\n2017-09-02\n\n\n0.15\n\n\nC:\\demo\n\n\n\n\n\n\n\n\nBelow is a folder based batch execution to convert multiple geojson files to structured json files\n\n\n\nBatch Activation\n\n\nThis tool was rewritten to provide users with two options to activate their assets. They can either point the tool at a folder and select the item and asset combination or they can specify a CSV file which contains each asset and item type and path to the structured JSON file. A setup would be as simple as \n\n\npbatch activate --indir \"path to folder with structured json files\" --asset \"item asset type example: PSOrthoTile analytic\"\n\n\nThe csv file need to have headers\n\n\n\n\n\n\n\n\npathways\n\n\nasset\n\n\n\n\n\n\n\n\n\n\nC:\\demo\\dallas_aoi.json\n\n\nPSOrthoTile analytic\n\n\n\n\n\n\nC:\\demo\\denver_aoi.json\n\n\nREOrthoTile analytic\n\n\n\n\n\n\nC:\\demo\\sfo_aoi.json\n\n\nPSScene4Band analytic\n\n\n\n\n\n\nC:\\demo\\indy_aoi.json\n\n\nPSScene4Band analytic_sr\n\n\n\n\n\n\n\n\n\n\nThe tool generates a slack readout which includes the filename , the asset and item type and the number of item and asset combinations that have been requested for activation\n\n\nBatch Download and Balance\n\n\nWe run the \ndownloader\n tool to batch download these assets and again you can choose to have either a folder or a csv file containing path to the json files, the item asset combination and the output location. A simple setup is thus\n\n\npbatch downloader --indir \nPathway to your json file\n --asset \nPSOrthoTile analytic\n --outdir \nC:\\output-folder\n\n\n\n\n\nThe csv file needs to have following headers and setup\n\n\n\n\n\n\n\n\npathways\n\n\ndirectory\n\n\nasset\n\n\n\n\n\n\n\n\n\n\nC:\\demo\\dallas_aoi.json\n\n\nC:\\demo\\t1\n\n\nPSOrthoTile analytic\n\n\n\n\n\n\nC:\\demo\\denver_aoi.json\n\n\nC:\\demo\\t1\n\n\nREOrthoTile analytic\n\n\n\n\n\n\nC:\\demo\\sfo_aoi.json\n\n\nC:\\demo\\t1\n\n\nPSOrthoTile analytic_xml\n\n\n\n\n\n\nC:\\demo\\indy_aoi.json\n\n\nC:\\demo\\t1\n\n\nREOrthoTile analytic_xml\n\n\n\n\n\n\n\n\nBatch downloading\nusing folder\n\n\nThis tool is unique for a few reasons\n\n\n\n\nThis can using the CSV sort to identify different pathways to strctured jsons in different locations, but it can also download different assets for each input file and write to different location each time. Meaning this can be of production value to teams who have different source folders and output buckets where they would want their data to be written.\n\n\nThe tool also prints information of Number of assets already active, number of assets that could not be activated and the total number of assets. Incase number of assets active do not match those that can be activated it will wait and show you a progressbar before trying again. This is the load balancing for each input file while making sure you don't have to estimate wait times for large requests.\n\n\n\n\nThe tool generates a slack message posted on your channel letting you keep track of downloads.\n\n\nAdditional Tools\n\n\nTwo things that keep changing are space (The amount of space needed to store your data) and the time since you may want to look at different time windows . With this in mind an easy way to update you about the total space for the assets you activated I created a tool called \npbatch space\n . A simple setup would be\n\n\npbatch space --indir \"Input directory with structured json file\" --asset \"PSOrthoTile analytic\"\n\n\n\nAnd it can also consume a csv file where the csv file need to have headers\n\n\nCSV Setup to estimate size of assets in GB \n\n\n\n\n\n\n\n\npathways\n\n\nasset\n\n\n\n\n\n\n\n\n\n\nC:\\demo\\dallas_aoi.json\n\n\nPSOrthoTile analytic\n\n\n\n\n\n\nC:\\demo\\denver_aoi.json\n\n\nREOrthoTile analytic\n\n\n\n\n\n\nC:\\demo\\sfo_aoi.json\n\n\nPSScene4Band analytic\n\n\n\n\n\n\nC:\\demo\\indy_aoi.json\n\n\nPSScene4Band analytic_sr\n\n\n\n\n\n\n\n\n\n\nSlack will record the last time you ran this tool because new assets may have activated since you last ran this or new assets may have\nbecome available for you to activate. This tool is useful only after you have activated your assets.\n\n\nTo quickly change start times on structured JSON I created another tool \npbatch aoiupdate\n. Most often all your data needs can be considered as x number of days from whatever you sent . Meaning I may want to look at 30 days of data from the end date and we don't want to recreate the structured json files. Turns out we can easily change that using a time delta function and simply rewrite the start date for our json files from which to start looking\nfor data.\n\n\n\n\nNote: Your end date should be current date or later the way the date is now written is date greater than equal to 30 days from today and end date remains constant\n\n\n\n\n\n\nA good rule of them to be safe with this tool is to save the end date into the\nfuture so for example. The setup maybe\n\n\npbatch aoiupdate --indir \"directory\" --days 30\n\n\nstart date \"**2017-01-01**\" end date \"**2017-12-31**\"\n\n\nnew start date \"**2017-10-26**\" new end date \"**2017-12-31**\"\n\n\n\nYou can also create a CSV setup for this as well with different days for each\nJSON file\n\n\nCSV Setup to update structured JSON. Note: the number of days is calculated\nfrom current date\n\n\n\n\n\n\n\n\npathways\n\n\ndays\n\n\n\n\n\n\n\n\n\n\nC:\\demo\\dallas_aoi.json\n\n\n3\n\n\n\n\n\n\nC:\\demo\\denver_aoi.json\n\n\n5\n\n\n\n\n\n\nC:\\demo\\sfo_aoi.json\n\n\n14\n\n\n\n\n\n\nC:\\demo\\indy_aoi.json\n\n\n23\n\n\n\n\n\n\n\n\nAnother tool that was solely written witht he purpose of integration with Google Earth Engine is the  \npbatch metadata\n which allows you to tabulate metadata and this is for use in conjunction with Google Earth Engine atleast for my workflow.\n\n\nCitation and Credits\n\n\nYou can cite the tool as\n\n\n\n\nSamapriya Roy. (2017, December 4). samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.1079887\n\n\n\n\nThanks to the \nPlanet Ambassador Program\n\n\nChangelog\n\n\nv0.1.4\n\n\n\n\nNow handles CSV formatting for all tools\n\n\nAdded CSV Example setups folder for use\n\n\n\n\nv0.1.3 (Pre-release)\n\n\n\n\nHandles issues with subprocess module and long wait\n\n\n\n\nv0.1.2 (Pre-release)\n\n\n\n\nUpdates to parsing CSV, optimized handling of filetypes\n\n\n\n\nv0.1.1 (Pre-release)\n\n\n\n\nUpdates to Batch download handler", 
            "title": "Planet Batch Slack Pipeline"
        }, 
        {
            "location": "/projects/planet_batch_slack/#planet-batch-slack-pipeline-cli", 
            "text": "Planet Labs(Full line up of Satellites) and Planet   Slack Technologies\nLogo  For writing a readme file this time I have adapted a shared piece written for the medium article. The first part which is setting up the slack account, creating an application and a slack bot has been discussed in the  article here . In the past I have written tools which act as pipelines for you to process single areas of interest at the time that could be chained, the need to write something that does a bit more heavy lifting arose. This command line interface(CLI) application was created to handle groups and teams that have multiple areas of interest and multiple input and output buckets and locations to function smoothly. I have integrated this to slack so you can be on the move while this task can be on a scheduler and update you when finished.", 
            "title": "Planet Batch &amp; Slack Pipeline CLI"
        }, 
        {
            "location": "/projects/planet_batch_slack/#table-of-contents", 
            "text": "Installation  Getting started  Batch Approach to Structured JSON  Batch Activation  Batch Download and Balance  Additional Tools  Citation and Credits  Changelog", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/planet_batch_slack/#installation", 
            "text": "The next step we will setup the  Planet-Batch-Slack-Pipeline-CLI  and integrate our previously built slack app for notifications. To setup the prerequisites you need to install the Planet\nPython API Client and Slack Python API Clients.\n* To install the tool you can go to the GitHub page at  Planet-Batch-Slack-Pipeline-CLI . As always two of my favorite operating systems are Windows and Linux, and to install on Linux  git clone https://github.com/samapriya/Planet-Batch-Slack-Pipeline-CLI.git\ncd Planet-Batch-Slack-Pipeline-CLI   sudo python setup.py install\npip install -r requirements.txt   for windows download the zip and after extraction go to the folder containing \"setup.py\" and open command prompt at that location and type   python setup.py install  \npip install -r requirements.txt  Now call the tool for the first time, by typing in  pbatch -h . This will only work if you have python in the system path which you can test for opening up terminal or command prompt and typing  python.", 
            "title": "Installation"
        }, 
        {
            "location": "/projects/planet_batch_slack/#getting-started", 
            "text": "Once the requirements have been satisfied the first thing we would setup would be the OAuth Keys we created. The tools consists of a bunch of slack tools as well including capability to just use this tool to send slack messages, attachments and clean up channel as needed.  \nPlanet Batch Tools and Slack Addons Interface  The two critical setup tools to make Slack ready and integrated are the smain and sbot tools where you will enter the OAuth for the application and OAuth for the bot that you generated earlier. These are then stored into your session for future use, you can call them using  pbatch smain Use the \" OAuth Access Token \" generated earlier . You can find the  tutorial here  pbatch sbot Use \" Bot User OAuth Access Token\"  generated earlier . You can find the  tutorial here  Once this is done your bot is now setup to message you when a task is completed. In our case these are tied into individual tools within the batch toolkit we just installed.  To be clear these tools were designed based on what I thought was an effective way of looking at data, downloading them and chaining the processes together. They are still a set of individual tools to make sure that one operation is independent of the other and does not break in case of a problem. So a non-monolithic design in some sense to make sure the pieces work. We will go through each of them in the order of use  pbatch planetkey  is the obvious one which is your planet API key and will\nallow you to store this locally to a session.  The  aoijson  tool is the same tool used in the Planet-EE CLI within the pbatch bundle allows you to bring any existing KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles to a structured geojson file, this is so that the Planet API can read the structured geojson which has additional filters such as cloud cover and range of dates. The tool can then allow you to\nconvert the geojson file to include filters to be used with the Planet Data API.  Let us say we want to convert this  map.geojson  to a structured aoi.json from  June 1 2017 to June 30th 2017 with 15% cloud cover as our maximum . We would pass the following command  pbatch.py aoijson --start  2017-06-01  --end  2017-06-30  --cloud  0.15  --inputfile  GJSON  --geo  local path to map.geojson file  --loc  path where aoi.json output file will be created", 
            "title": "Getting started"
        }, 
        {
            "location": "/projects/planet_batch_slack/#batch-approach-to-structured-json", 
            "text": "This tool was then rewritten and included in the application to overcome two issues 1) Automatically detect the type of input file I am passing (For now it can automatically handle geojson, kml, shapefile and wkt files). The files are then saved in an output directory with the  filename_aoi.json .    The tool can also read from a csv file and parse different start dates, end dates and cloud cover for different files and create structured jsons to multiple locations making an multi path export easy. The csv headers should be\nThe csv file needs to have following headers and setup     pathways  start  end  cloud  outdir      C:\\demo\\dallas.geojson  2017-01-01  2017-01-02  0.15  C:\\demo    C:\\demo\\denver.geojson  2017-01-01  2017-03-02  0.15  C:\\demo    C:\\demo\\sfo.geojson  2017-01-01  2017-05-02  0.15  C:\\demo    C:\\demo\\indy.geojson  2017-01-01  2017-09-02  0.15  C:\\demo     Below is a folder based batch execution to convert multiple geojson files to structured json files", 
            "title": "Batch Approach to Structured JSON"
        }, 
        {
            "location": "/projects/planet_batch_slack/#batch-activation", 
            "text": "This tool was rewritten to provide users with two options to activate their assets. They can either point the tool at a folder and select the item and asset combination or they can specify a CSV file which contains each asset and item type and path to the structured JSON file. A setup would be as simple as   pbatch activate --indir \"path to folder with structured json files\" --asset \"item asset type example: PSOrthoTile analytic\"  The csv file need to have headers     pathways  asset      C:\\demo\\dallas_aoi.json  PSOrthoTile analytic    C:\\demo\\denver_aoi.json  REOrthoTile analytic    C:\\demo\\sfo_aoi.json  PSScene4Band analytic    C:\\demo\\indy_aoi.json  PSScene4Band analytic_sr      The tool generates a slack readout which includes the filename , the asset and item type and the number of item and asset combinations that have been requested for activation", 
            "title": "Batch Activation"
        }, 
        {
            "location": "/projects/planet_batch_slack/#batch-download-and-balance", 
            "text": "We run the  downloader  tool to batch download these assets and again you can choose to have either a folder or a csv file containing path to the json files, the item asset combination and the output location. A simple setup is thus  pbatch downloader --indir  Pathway to your json file  --asset  PSOrthoTile analytic  --outdir  C:\\output-folder   The csv file needs to have following headers and setup     pathways  directory  asset      C:\\demo\\dallas_aoi.json  C:\\demo\\t1  PSOrthoTile analytic    C:\\demo\\denver_aoi.json  C:\\demo\\t1  REOrthoTile analytic    C:\\demo\\sfo_aoi.json  C:\\demo\\t1  PSOrthoTile analytic_xml    C:\\demo\\indy_aoi.json  C:\\demo\\t1  REOrthoTile analytic_xml     Batch downloading\nusing folder  This tool is unique for a few reasons   This can using the CSV sort to identify different pathways to strctured jsons in different locations, but it can also download different assets for each input file and write to different location each time. Meaning this can be of production value to teams who have different source folders and output buckets where they would want their data to be written.  The tool also prints information of Number of assets already active, number of assets that could not be activated and the total number of assets. Incase number of assets active do not match those that can be activated it will wait and show you a progressbar before trying again. This is the load balancing for each input file while making sure you don't have to estimate wait times for large requests.   The tool generates a slack message posted on your channel letting you keep track of downloads.", 
            "title": "Batch Download and Balance"
        }, 
        {
            "location": "/projects/planet_batch_slack/#additional-tools", 
            "text": "Two things that keep changing are space (The amount of space needed to store your data) and the time since you may want to look at different time windows . With this in mind an easy way to update you about the total space for the assets you activated I created a tool called  pbatch space  . A simple setup would be  pbatch space --indir \"Input directory with structured json file\" --asset \"PSOrthoTile analytic\"  And it can also consume a csv file where the csv file need to have headers  CSV Setup to estimate size of assets in GB      pathways  asset      C:\\demo\\dallas_aoi.json  PSOrthoTile analytic    C:\\demo\\denver_aoi.json  REOrthoTile analytic    C:\\demo\\sfo_aoi.json  PSScene4Band analytic    C:\\demo\\indy_aoi.json  PSScene4Band analytic_sr      Slack will record the last time you ran this tool because new assets may have activated since you last ran this or new assets may have\nbecome available for you to activate. This tool is useful only after you have activated your assets.  To quickly change start times on structured JSON I created another tool  pbatch aoiupdate . Most often all your data needs can be considered as x number of days from whatever you sent . Meaning I may want to look at 30 days of data from the end date and we don't want to recreate the structured json files. Turns out we can easily change that using a time delta function and simply rewrite the start date for our json files from which to start looking\nfor data.   Note: Your end date should be current date or later the way the date is now written is date greater than equal to 30 days from today and end date remains constant    A good rule of them to be safe with this tool is to save the end date into the\nfuture so for example. The setup maybe  pbatch aoiupdate --indir \"directory\" --days 30  start date \"**2017-01-01**\" end date \"**2017-12-31**\"\n\n\nnew start date \"**2017-10-26**\" new end date \"**2017-12-31**\"  You can also create a CSV setup for this as well with different days for each\nJSON file  CSV Setup to update structured JSON. Note: the number of days is calculated\nfrom current date     pathways  days      C:\\demo\\dallas_aoi.json  3    C:\\demo\\denver_aoi.json  5    C:\\demo\\sfo_aoi.json  14    C:\\demo\\indy_aoi.json  23     Another tool that was solely written witht he purpose of integration with Google Earth Engine is the   pbatch metadata  which allows you to tabulate metadata and this is for use in conjunction with Google Earth Engine atleast for my workflow.", 
            "title": "Additional Tools"
        }, 
        {
            "location": "/projects/planet_batch_slack/#citation-and-credits", 
            "text": "You can cite the tool as   Samapriya Roy. (2017, December 4). samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.1079887   Thanks to the  Planet Ambassador Program", 
            "title": "Citation and Credits"
        }, 
        {
            "location": "/projects/planet_batch_slack/#changelog", 
            "text": "", 
            "title": "Changelog"
        }, 
        {
            "location": "/projects/planet_batch_slack/#v014", 
            "text": "Now handles CSV formatting for all tools  Added CSV Example setups folder for use", 
            "title": "v0.1.4"
        }, 
        {
            "location": "/projects/planet_batch_slack/#v013-pre-release", 
            "text": "Handles issues with subprocess module and long wait", 
            "title": "v0.1.3 (Pre-release)"
        }, 
        {
            "location": "/projects/planet_batch_slack/#v012-pre-release", 
            "text": "Updates to parsing CSV, optimized handling of filetypes", 
            "title": "v0.1.2 (Pre-release)"
        }, 
        {
            "location": "/projects/planet_batch_slack/#v011-pre-release", 
            "text": "Updates to Batch download handler", 
            "title": "v0.1.1 (Pre-release)"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/", 
            "text": "Planet Pipeline GUI\n\n\nThe Planet Pipeline GUI came from the actual CLI (command line interface tools) to enable the use of tools required to access control and download planet labs assets as well as parse metadata in a tabular form which maybe required by other applications.\n\n\n\n\n\n\nTable of contents\n\n\n\n\nInstallation\n\n\nGetting started\n\n\nPlanet Key\n\n\nAOI JSON\n\n\nActivate or Check Asset\n\n\nDownload Size\n\n\nDownload Asset\n\n\nMetadata Parser\n\n\n\n\n\n\nCredits\n\n\n\n\nInstallation\n\n\nWe assume Planet Python API is installed you can install by simply running \n\n\npip install planet\n\n\n\n\nFurther instructions can be found \nhere\n \n\n\nTo install the tool:\n\n\ngit clone https://github.com/samapriya/Planet-Pipeline-GUI.git\ncd Planet-Pipeline-GUI \n pip install .\n\n\n\n\nThe application can be also run directly by executing PlanetPipe_GUI.pyc script. \nYou require two important packages for this to run\n\n\nWxPython(which is what the GUI is built on)\nfor windows(Tested in Windows 10)\nhttps://wxpython.org/download.php\n\nfor linux(Tested in Ubuntu 16)\nsudo add-apt-repository \ndeb http://archive.ubuntu.com/ubuntu utopic main restricted universe\n  \nsudo apt-get update\napt-cache search python-wxgtk3.0\nsudo apt-get install python-wxgtk3.0\n\n\n\n\nGetting started\n\n\nThis should be pretty simple on windows systems with python \n=2.7.13 you can just double click the PlanetPipe_GUI.pyc for linux user python PlanetPipe_GUI.pyc\n\n\nPlanet Key\n\n\nThis tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools\n\n\n\n\nIf using on a private machine the Key is saved as a csv file for all future runs of the tool.\n\n\nAOI JSON\n\n\nThe aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.\n\n\n\n\nActivate or Check Asset\n\n\nThe activatepl tab allows the users to either check or activate planet assets. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier\n\n\n\n\nDownload Size\n\n\nThe space tool allows the user to check the size of the activated assets before downloading while also estimating the available space on the local drive. The overall size and space is printed in MB and GB\n\n\n\n\nDownload Asset\n\n\nThe downloadpl tab allows the users to download assets that have been activated. \n\n\n\n\nCredits\n\n\nJetStream\n A portion of the work is suported by JetStream Grant TG-GEO160014.\n\n\nAlso supported by \nPlanet Labs Ambassador Program", 
            "title": "Planet Bulk Data Downloader GUI"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#planet-pipeline-gui", 
            "text": "The Planet Pipeline GUI came from the actual CLI (command line interface tools) to enable the use of tools required to access control and download planet labs assets as well as parse metadata in a tabular form which maybe required by other applications.", 
            "title": "Planet Pipeline GUI"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#table-of-contents", 
            "text": "Installation  Getting started  Planet Key  AOI JSON  Activate or Check Asset  Download Size  Download Asset  Metadata Parser    Credits", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#installation", 
            "text": "We assume Planet Python API is installed you can install by simply running   pip install planet  Further instructions can be found  here    To install the tool:  git clone https://github.com/samapriya/Planet-Pipeline-GUI.git\ncd Planet-Pipeline-GUI   pip install .  The application can be also run directly by executing PlanetPipe_GUI.pyc script. \nYou require two important packages for this to run  WxPython(which is what the GUI is built on)\nfor windows(Tested in Windows 10)\nhttps://wxpython.org/download.php\n\nfor linux(Tested in Ubuntu 16)\nsudo add-apt-repository  deb http://archive.ubuntu.com/ubuntu utopic main restricted universe   \nsudo apt-get update\napt-cache search python-wxgtk3.0\nsudo apt-get install python-wxgtk3.0", 
            "title": "Installation"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#getting-started", 
            "text": "This should be pretty simple on windows systems with python  =2.7.13 you can just double click the PlanetPipe_GUI.pyc for linux user python PlanetPipe_GUI.pyc", 
            "title": "Getting started"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#planet-key", 
            "text": "This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools   If using on a private machine the Key is saved as a csv file for all future runs of the tool.", 
            "title": "Planet Key"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#aoi-json", 
            "text": "The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.", 
            "title": "AOI JSON"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#activate-or-check-asset", 
            "text": "The activatepl tab allows the users to either check or activate planet assets. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier", 
            "title": "Activate or Check Asset"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#download-size", 
            "text": "The space tool allows the user to check the size of the activated assets before downloading while also estimating the available space on the local drive. The overall size and space is printed in MB and GB", 
            "title": "Download Size"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#download-asset", 
            "text": "The downloadpl tab allows the users to download assets that have been activated.", 
            "title": "Download Asset"
        }, 
        {
            "location": "/projects/planet_pipeline_gui/#credits", 
            "text": "JetStream  A portion of the work is suported by JetStream Grant TG-GEO160014.  Also supported by  Planet Labs Ambassador Program", 
            "title": "Credits"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/", 
            "text": "Planet GEE Pipeline GUI\n\n\n\n\n\n\n\n\nThe Planet Pipeline GUI came from the actual CLI (command line interface tools) to enable the use of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.\n\n\n\n\nTable of contents\n\n\n\n\nInstallation\n\n\nUsage examples\n\n\nPlanet Tools\n\n\nPlanet Key\n\n\nAOI JSON\n\n\nActivate or Check Asset\n\n\nDownload Asset\n\n\nMetadata Parser\n\n\n\n\n\n\nEarth Engine Tools\n\n\nEE User\n\n\nUpload a directory with images and associate properties with each image:\n\n\nUpload a directory with images with specific NoData value to a selected destination:\n\n\nTask Query\n\n\nTask Query during ingestion\n\n\nTask Report\n\n\nDelete a collection with content:\n\n\nAssets Move\n\n\nAssets Copy\n\n\nAssets Access\n\n\nSet Collection Property\n\n\nConvert to Fusion Table\n\n\nCleanup Utility\n\n\nCancel all tasks\n\n\n\n\n\n\nCredits\n\n\n\n\nInstallation\n\n\nWe assume Earth Engine Python API is installed and EE authorised as desribed \nhere\n. We also assume Planet Python API is installed you can install by simply running \n\n\npip install planet\n\n\n\n\nFurther instructions can be found \nhere\n \n\n\nYou require two important packages for this to run\n\n\nWxPython(which is what the GUI is built on)\nfor windows(Tested in Windows 10)\nhttps://wxpython.org/download.php\npip install wxPython\n\nfor linux(Tested in Ubuntu 16)\nsudo add-apt-repository \ndeb http://archive.ubuntu.com/ubuntu utopic main restricted universe\n  \nsudo apt-get update\napt-cache search python-wxgtk3.0\nsudo apt-get install python-wxgtk3.0\n\n\n\n\nThis toolbox also uses some functionality from GDAL\nFor installing GDAL in Ubuntu\n\n\nsudo add-apt-repository ppa:ubuntugis/ppa \n sudo apt-get update\nsudo apt-get install gdal-bin\n\n\n\n\nFor Windows I found this \nguide\n from UCLA\n\n\nUsage examples\n\n\nUsage examples have been segmented into two parts focusing on both planet tools as well as earth engine tools, earth engine tools include additional developments in CLI which allows you to recursively interact with their python API. To run the tool open a command prompt window or terminal and type\n\n\npython ee_ppipe.pyc\n\n\n\n\nPlanet Tools\n\n\nThe Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.\n\n\nPlanet Key\n\n\nThis tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools\n\n\n\n\nIf using on a private machine the Key is saved as a csv file for all future runs of the tool.\n\n\nAOI JSON\n\n\nThe aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.\n\n\n\n\nActivate or Check Asset\n\n\nThe activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier\n\n\n\n\nDownload Asset\n\n\nHaving metadata helps in organising your asstets, but is not mandatory - you can skip it.\nThe downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types.\n\n\n\n\nMetadata Parser\n\n\nThe metadata tab is a more powerful tool and consists of metadata parsing for PlanetScope OrthoTile RapiEye OrthoTile along with Digital Globe MultiSpectral and DigitalGlobe PanChromatic datasets. This was developed as a standalone to process xml metadata files from multiple sources and is important step is the user plans to upload these assets to Google Earth Engine. The combine Planet-GEE Pipeline tool will be made available soon for testing.\n\n\n\n\nEarth Engine Tools\n\n\nThe ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. This is also a seperate package for earth engine users to use and can be downloaded \nhere\n\n\nEE User\n\n\nThis tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back\n\n\n\nUpload a directory with images to your myfolder/mycollection and associate properties with each image:\n\n\n\n\nThe script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.\n\n\nTask Query\n\n\nThis script counts all currently running and ready tasks along with failed tasks.\n\n\n\n\nTask Query during ingestion\n\n\nThis script can be used intermittently to look at running, failed and ready(waiting) tasks during ingestion. This script is a special case using query tasks only when uploading assets to collection by providing collection pathway to see how collection size increases.\n\n\n\n\nTask Report\n\n\nSometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type\n\n\n\n\nDelete a collection with content:\n\n\nThe delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution!\n\n\n\n\nAssets Move\n\n\nThis script allows us to recursively move assets from one collection to the other.\n\n\n\n\nAssets Copy\n\n\nThis script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections.\n\n\n\n\nAssets Access\n\n\nThis tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time.\n\n\n\n\nSet Collection Property\n\n\nThis script is derived from the ee tool to set collection properties and will set overall properties for collection. \n\n\n\n\nConvert to Fusion Table\n\n\nOnce validated with gdal and google fusion table it can be used to convert any geoObject to google fusion table. Forked and contributed by Gennadii \nhere\n. The scripts can be used only with a specific google account\n\n\n\n\nCleanup Utility\n\n\nThis script is used to clean folders once all processes have been completed. In short this is a function to clear folder on local machine.\n\n\n\n\nCancel all tasks\n\n\nThis is a simpler tool, can be called directly from the earthengine cli as well\n\n\n\n\nCredits\n\n\nJetStream\n A portion of the work is suported by JetStream Grant TG-GEO160014.\n\n\nAlso supported by \nPlanet Labs Ambassador Program", 
            "title": "Planet-Google Earth Engine Pipeline GUI"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#planet-gee-pipeline-gui", 
            "text": "The Planet Pipeline GUI came from the actual CLI (command line interface tools) to enable the use of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.", 
            "title": "Planet GEE Pipeline GUI"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#table-of-contents", 
            "text": "Installation  Usage examples  Planet Tools  Planet Key  AOI JSON  Activate or Check Asset  Download Asset  Metadata Parser    Earth Engine Tools  EE User  Upload a directory with images and associate properties with each image:  Upload a directory with images with specific NoData value to a selected destination:  Task Query  Task Query during ingestion  Task Report  Delete a collection with content:  Assets Move  Assets Copy  Assets Access  Set Collection Property  Convert to Fusion Table  Cleanup Utility  Cancel all tasks    Credits", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#installation", 
            "text": "We assume Earth Engine Python API is installed and EE authorised as desribed  here . We also assume Planet Python API is installed you can install by simply running   pip install planet  Further instructions can be found  here    You require two important packages for this to run  WxPython(which is what the GUI is built on)\nfor windows(Tested in Windows 10)\nhttps://wxpython.org/download.php\npip install wxPython\n\nfor linux(Tested in Ubuntu 16)\nsudo add-apt-repository  deb http://archive.ubuntu.com/ubuntu utopic main restricted universe   \nsudo apt-get update\napt-cache search python-wxgtk3.0\nsudo apt-get install python-wxgtk3.0  This toolbox also uses some functionality from GDAL\nFor installing GDAL in Ubuntu  sudo add-apt-repository ppa:ubuntugis/ppa   sudo apt-get update\nsudo apt-get install gdal-bin  For Windows I found this  guide  from UCLA", 
            "title": "Installation"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#usage-examples", 
            "text": "Usage examples have been segmented into two parts focusing on both planet tools as well as earth engine tools, earth engine tools include additional developments in CLI which allows you to recursively interact with their python API. To run the tool open a command prompt window or terminal and type  python ee_ppipe.pyc", 
            "title": "Usage examples"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#planet-tools", 
            "text": "The Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.", 
            "title": "Planet Tools"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#planet-key", 
            "text": "This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools   If using on a private machine the Key is saved as a csv file for all future runs of the tool.", 
            "title": "Planet Key"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#aoi-json", 
            "text": "The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.", 
            "title": "AOI JSON"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#activate-or-check-asset", 
            "text": "The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier", 
            "title": "Activate or Check Asset"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#download-asset", 
            "text": "Having metadata helps in organising your asstets, but is not mandatory - you can skip it.\nThe downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types.", 
            "title": "Download Asset"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#metadata-parser", 
            "text": "The metadata tab is a more powerful tool and consists of metadata parsing for PlanetScope OrthoTile RapiEye OrthoTile along with Digital Globe MultiSpectral and DigitalGlobe PanChromatic datasets. This was developed as a standalone to process xml metadata files from multiple sources and is important step is the user plans to upload these assets to Google Earth Engine. The combine Planet-GEE Pipeline tool will be made available soon for testing.", 
            "title": "Metadata Parser"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#earth-engine-tools", 
            "text": "The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. This is also a seperate package for earth engine users to use and can be downloaded  here", 
            "title": "Earth Engine Tools"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#ee-user", 
            "text": "This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back", 
            "title": "EE User"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#upload-a-directory-with-images-to-your-myfoldermycollection-and-associate-properties-with-each-image", 
            "text": "The script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.", 
            "title": "Upload a directory with images to your myfolder/mycollection and associate properties with each image:"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#task-query", 
            "text": "This script counts all currently running and ready tasks along with failed tasks.", 
            "title": "Task Query"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#task-query-during-ingestion", 
            "text": "This script can be used intermittently to look at running, failed and ready(waiting) tasks during ingestion. This script is a special case using query tasks only when uploading assets to collection by providing collection pathway to see how collection size increases.", 
            "title": "Task Query during ingestion"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#task-report", 
            "text": "Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type", 
            "title": "Task Report"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#delete-a-collection-with-content", 
            "text": "The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution!", 
            "title": "Delete a collection with content:"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#assets-move", 
            "text": "This script allows us to recursively move assets from one collection to the other.", 
            "title": "Assets Move"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#assets-copy", 
            "text": "This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections.", 
            "title": "Assets Copy"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#assets-access", 
            "text": "This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time.", 
            "title": "Assets Access"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#set-collection-property", 
            "text": "This script is derived from the ee tool to set collection properties and will set overall properties for collection.", 
            "title": "Set Collection Property"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#convert-to-fusion-table", 
            "text": "Once validated with gdal and google fusion table it can be used to convert any geoObject to google fusion table. Forked and contributed by Gennadii  here . The scripts can be used only with a specific google account", 
            "title": "Convert to Fusion Table"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#cleanup-utility", 
            "text": "This script is used to clean folders once all processes have been completed. In short this is a function to clear folder on local machine.", 
            "title": "Cleanup Utility"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#cancel-all-tasks", 
            "text": "This is a simpler tool, can be called directly from the earthengine cli as well", 
            "title": "Cancel all tasks"
        }, 
        {
            "location": "/projects/planet_gee_pipeline_gui/#credits", 
            "text": "JetStream  A portion of the work is suported by JetStream Grant TG-GEO160014.  Also supported by  Planet Labs Ambassador Program", 
            "title": "Credits"
        }, 
        {
            "location": "/projects/jetstream_unofficial_cli/", 
            "text": "Jetstream Unofficial Addon for Atmosphere VM(s)\n\n\n\n\n\n\nJetstream provides researchers and students with a unique approach to cloud computing and data analysis on demand. Moving away from a job submission model which is more pervasice on High Performance Cluster computing this allows users to create user-friendly installs of \u201cvirtual machines\u201d which can then be custom installed with softwares and packages and even imaged further to make science replicable. The system in theory also allows a BYoOS (Bring your own Operating System) designed to further allow more flexibility while sharing the backbone of high performance compute and cloud storage infrastructure. The idea of imaging a system allows the user to not only share their data or analysis but their entire setup that allows them to run the analysis making it a plug and play model in terms of research replication and transference. As the NSF project proceeds further it allows users to apply for grants or allocation time on the machines and indeed renew these free of cost for researchers and students. \n\n\nFor my work looking at developing a Satellite Imagery Input Output (IO) pipeline I was allocated a Jetstream grant and while using a couple of these systems I always thought it would be easier to query number of instances and volumes running on my account, my burn rate and left over allocation over sometime. And something useful was the possibility to perform system actions such as shutdown(stop), restart, start, reboot among others programatically. It is with this idea that I approach the API backend at JetStream and built just a few tools to allows users to perform these actions without the need to log into their web browsers and allows user to call upon instance actions through a rough command line interface (CLI).\n\n\nI would love to thank Steve Gregory who is getting the Official API ready for release and Jeremy Fischer who have helped me in more ways and allowed me to keep asking questions and learn from them to build the unofficial api for Jetstream.\n\n\nThe Jetstream's mission and funding is supported by NSF and any and all citations are useful so please make sure to cite and citation information can be found at the end of this readme or use the \nlink\n\n\nNote: I have used this on the Jetstream-IU system but it should work on the TACC end as well and the information I decided to show per instance is based on what I used, there are much more information generated but the tool prints a subset\n\n\n\n\nTable of contents\n\n\n\n\nGetting started\n\n\nSave API Password as Credential\n\n\nQuery Current Instances\n\n\nQuery Current Volumes\n\n\nPerform Instance Actions\n\n\n\n\n\n\n\n\nGetting started\n\n\nTo get started you need an XSEDE account of a Jetstream Rapid Access Account and you can create one using instructions \nhere\n. Once into the system you will still need an allocation and the wiki page setup by the project explains these along with everything you can do step by step \nhere\n. Once you have a project allocation and create some instances and volumes you can query and perform instance actions.\n\n\nJust browse to the folder and perform a \npython jetstream.py -h\n:\n\n\nusage: jetstream.py [-h] { ,jskey,instance,volume,action} ...\n\nJetStream API Unofficial\n\npositional arguments:\n  { ,jskey,instance,volume,action}\n                        -------------------------------------------\n                        -----Choose from JetStream Tools Below-----\n                        -------------------------------------------\n    jskey               Allows you to save your JetStream API Password\n    instance            Allows users to print out all instance information\n    volume              Allows users to print out all volume information\n    action              Allows user to start, suspend,resume,reboot instance\n\noptional arguments:\n  -h, --help            show this help message and exit\n\n\n\n\n\nSave API Password as Credential\n\n\nThis tool allows the user to save the credential or password file into \nusers/.config/jetstream\n making sure that they are user specific and are not shared on a system resource or location. This password is not your XSEDE or Jetstream Rapid Access account password but a API password which has to be requested atleast for now. It uses a getpass implementation and write the password as a csv and the other tools first tries to read this and if not present asks for your password.\n\n\nusage: jetstream.py jskey [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\n\n\n\n\nQuery Current Instances\n\n\nAs the name stated this allows the user to query instances related to your account. This queries all instances for now and prints certain key parameters(these are a subset that I selected for my use) but can be easily modified in the code to print other information.\n\n\nusage: jetstream.py instance [-h] [--username USERNAME] [--password PASSWORD]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --username USERNAME  Jetstream API username\n  --password PASSWORD  Jetstream API password: \nOptional if you already saved\n                       jetstream key\n\n\n\n\n\nIncase you have already saved your password a setup would be simply\n\n\npython jetstream.py instance --username \njohndoe\n\n\n\n\n\nif not\n\n\npython jetstream.py instance --username \njohndoe\n --password \npass\n\n\n\n\n\nQuery Current Volumes\n\n\nThe current volume options queries current volumes on your project. Note that the current info does not tell you which instance a volume is attached to which might be a nice endpoint readout to have.\n\n\nusage: jetstream.py volume [-h] [--username USERNAME] [--password PASSWORD]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --username USERNAME  Jetstream API username\n  --password PASSWORD  Jetstream API password: \nOptional if you already saved\n                       jetstream key\n\n\n\n\n\nIncase you have already saved your password a setup would be simply\n\n\npython jetstream.py volume --username \njohndoe\n\n\n\n\n\nif not\n\n\npython jetstream.py volume --username \njohndoe\n --password \npass\n\n\n\n\n\nPerform Instance Actions\n\n\nThis is perhaps one of the most important aspect of building this tool, the idea was to get a read out or a email message when a system has been idle for sometime and then auto shutdown or initiate a shutdown remotely. This command allows you to choose the instance you want to perform an instance action using the instance ID mentioned on your instance profile page. The next thing you need to know is what kind of action you want to perform. There are certain things that will create a conflict, for example: If you are trying to shutdown an already shutoff system or start an already active system. In that case it will print out a conflict message.\n\n\nusage: jetstream.py action [-h] [--username USERNAME] [--password PASSWORD]\n                           [--id ID] [--action ACTION]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --username USERNAME  Jetstream API username\n  --password PASSWORD  Jetstream API password: \nOptional if you already saved\n                       jetstream key\n\n  --id ID              Jetstream Instance ID on your Instance Detail Page\n  --action ACTION      Jetstream Instance Action,\n                       \nstart|stop|suspend|resume|reboot\n\n\n\n\n\nIncase you have already saved your password a setup would be simply\n\n\npython jetstream.py action --username \njohndoe\n --id \n00000\n --action \nstart\n\n\n\n\n\nif not \n\n\npython jetstream.py action --username \njohndoe\n --password \npass\n --id \n00000\n --action \nstart\n\n\n\n\n\nI would like to thank my grant from JetStream TG-GEO160014. You can cite this tool as \n\n\nSamapriya Roy. (2017, August 16). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon. Zenodo. http://doi.org/10.5281/zenodo.844018\n\n\nAnd I would like to include Jetstream citations for others to use\n\n\nStewart, C.A., Cockerill, T.M., Foster, I., Hancock, D., Merchant, N., Skidmore, E., Stanzione, D., Taylor, J., Tuecke, S., Turner, G., Vaughn, M., and Gaffney, N.I., Jetstream: a self-provisioned, scalable science and engineering cloud environment. 2015, In Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure. St. Louis, Missouri.  ACM: 2792774.  p. 1-8. http://dx.doi.org/10.1145/2792745.2792774\n\n\nand\n\n\nJohn Towns, Timothy Cockerill, Maytal Dahan, Ian Foster, Kelly Gaither, Andrew Grimshaw, Victor Hazlewood, Scott Lathrop, Dave Lifka, Gregory D. Peterson, Ralph Roskies, J. Ray Scott, Nancy Wilkins-Diehr, \"XSEDE: Accelerating Scientific Discovery\", Computing in Science \n Engineering, vol.16, no. 5, pp. 62-74, Sept.-Oct. 2014, doi:10.1109/MCSE.2014.80", 
            "title": "Jetstream Unofficial Addon"
        }, 
        {
            "location": "/projects/jetstream_unofficial_cli/#jetstream-unofficial-addon-for-atmosphere-vms", 
            "text": "Jetstream provides researchers and students with a unique approach to cloud computing and data analysis on demand. Moving away from a job submission model which is more pervasice on High Performance Cluster computing this allows users to create user-friendly installs of \u201cvirtual machines\u201d which can then be custom installed with softwares and packages and even imaged further to make science replicable. The system in theory also allows a BYoOS (Bring your own Operating System) designed to further allow more flexibility while sharing the backbone of high performance compute and cloud storage infrastructure. The idea of imaging a system allows the user to not only share their data or analysis but their entire setup that allows them to run the analysis making it a plug and play model in terms of research replication and transference. As the NSF project proceeds further it allows users to apply for grants or allocation time on the machines and indeed renew these free of cost for researchers and students.   For my work looking at developing a Satellite Imagery Input Output (IO) pipeline I was allocated a Jetstream grant and while using a couple of these systems I always thought it would be easier to query number of instances and volumes running on my account, my burn rate and left over allocation over sometime. And something useful was the possibility to perform system actions such as shutdown(stop), restart, start, reboot among others programatically. It is with this idea that I approach the API backend at JetStream and built just a few tools to allows users to perform these actions without the need to log into their web browsers and allows user to call upon instance actions through a rough command line interface (CLI).  I would love to thank Steve Gregory who is getting the Official API ready for release and Jeremy Fischer who have helped me in more ways and allowed me to keep asking questions and learn from them to build the unofficial api for Jetstream.  The Jetstream's mission and funding is supported by NSF and any and all citations are useful so please make sure to cite and citation information can be found at the end of this readme or use the  link  Note: I have used this on the Jetstream-IU system but it should work on the TACC end as well and the information I decided to show per instance is based on what I used, there are much more information generated but the tool prints a subset", 
            "title": "Jetstream Unofficial Addon for Atmosphere VM(s)"
        }, 
        {
            "location": "/projects/jetstream_unofficial_cli/#table-of-contents", 
            "text": "Getting started  Save API Password as Credential  Query Current Instances  Query Current Volumes  Perform Instance Actions", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/jetstream_unofficial_cli/#getting-started", 
            "text": "To get started you need an XSEDE account of a Jetstream Rapid Access Account and you can create one using instructions  here . Once into the system you will still need an allocation and the wiki page setup by the project explains these along with everything you can do step by step  here . Once you have a project allocation and create some instances and volumes you can query and perform instance actions.  Just browse to the folder and perform a  python jetstream.py -h :  usage: jetstream.py [-h] { ,jskey,instance,volume,action} ...\n\nJetStream API Unofficial\n\npositional arguments:\n  { ,jskey,instance,volume,action}\n                        -------------------------------------------\n                        -----Choose from JetStream Tools Below-----\n                        -------------------------------------------\n    jskey               Allows you to save your JetStream API Password\n    instance            Allows users to print out all instance information\n    volume              Allows users to print out all volume information\n    action              Allows user to start, suspend,resume,reboot instance\n\noptional arguments:\n  -h, --help            show this help message and exit", 
            "title": "Getting started"
        }, 
        {
            "location": "/projects/jetstream_unofficial_cli/#save-api-password-as-credential", 
            "text": "This tool allows the user to save the credential or password file into  users/.config/jetstream  making sure that they are user specific and are not shared on a system resource or location. This password is not your XSEDE or Jetstream Rapid Access account password but a API password which has to be requested atleast for now. It uses a getpass implementation and write the password as a csv and the other tools first tries to read this and if not present asks for your password.  usage: jetstream.py jskey [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit", 
            "title": "Save API Password as Credential"
        }, 
        {
            "location": "/projects/jetstream_unofficial_cli/#query-current-instances", 
            "text": "As the name stated this allows the user to query instances related to your account. This queries all instances for now and prints certain key parameters(these are a subset that I selected for my use) but can be easily modified in the code to print other information.  usage: jetstream.py instance [-h] [--username USERNAME] [--password PASSWORD]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --username USERNAME  Jetstream API username\n  --password PASSWORD  Jetstream API password:  Optional if you already saved\n                       jetstream key   Incase you have already saved your password a setup would be simply  python jetstream.py instance --username  johndoe   if not  python jetstream.py instance --username  johndoe  --password  pass", 
            "title": "Query Current Instances"
        }, 
        {
            "location": "/projects/jetstream_unofficial_cli/#query-current-volumes", 
            "text": "The current volume options queries current volumes on your project. Note that the current info does not tell you which instance a volume is attached to which might be a nice endpoint readout to have.  usage: jetstream.py volume [-h] [--username USERNAME] [--password PASSWORD]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --username USERNAME  Jetstream API username\n  --password PASSWORD  Jetstream API password:  Optional if you already saved\n                       jetstream key   Incase you have already saved your password a setup would be simply  python jetstream.py volume --username  johndoe   if not  python jetstream.py volume --username  johndoe  --password  pass", 
            "title": "Query Current Volumes"
        }, 
        {
            "location": "/projects/jetstream_unofficial_cli/#perform-instance-actions", 
            "text": "This is perhaps one of the most important aspect of building this tool, the idea was to get a read out or a email message when a system has been idle for sometime and then auto shutdown or initiate a shutdown remotely. This command allows you to choose the instance you want to perform an instance action using the instance ID mentioned on your instance profile page. The next thing you need to know is what kind of action you want to perform. There are certain things that will create a conflict, for example: If you are trying to shutdown an already shutoff system or start an already active system. In that case it will print out a conflict message.  usage: jetstream.py action [-h] [--username USERNAME] [--password PASSWORD]\n                           [--id ID] [--action ACTION]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --username USERNAME  Jetstream API username\n  --password PASSWORD  Jetstream API password:  Optional if you already saved\n                       jetstream key \n  --id ID              Jetstream Instance ID on your Instance Detail Page\n  --action ACTION      Jetstream Instance Action,\n                        start|stop|suspend|resume|reboot   Incase you have already saved your password a setup would be simply  python jetstream.py action --username  johndoe  --id  00000  --action  start   if not   python jetstream.py action --username  johndoe  --password  pass  --id  00000  --action  start   I would like to thank my grant from JetStream TG-GEO160014. You can cite this tool as   Samapriya Roy. (2017, August 16). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon. Zenodo. http://doi.org/10.5281/zenodo.844018  And I would like to include Jetstream citations for others to use  Stewart, C.A., Cockerill, T.M., Foster, I., Hancock, D., Merchant, N., Skidmore, E., Stanzione, D., Taylor, J., Tuecke, S., Turner, G., Vaughn, M., and Gaffney, N.I., Jetstream: a self-provisioned, scalable science and engineering cloud environment. 2015, In Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure. St. Louis, Missouri.  ACM: 2792774.  p. 1-8. http://dx.doi.org/10.1145/2792745.2792774  and  John Towns, Timothy Cockerill, Maytal Dahan, Ian Foster, Kelly Gaither, Andrew Grimshaw, Victor Hazlewood, Scott Lathrop, Dave Lifka, Gregory D. Peterson, Ralph Roskies, J. Ray Scott, Nancy Wilkins-Diehr, \"XSEDE: Accelerating Scientific Discovery\", Computing in Science   Engineering, vol.16, no. 5, pp. 62-74, Sept.-Oct. 2014, doi:10.1109/MCSE.2014.80", 
            "title": "Perform Instance Actions"
        }, 
        {
            "location": "/projects/arcticdem_download/", 
            "text": "ArcticDEM Batch Download \n Processing Tools\n\n\n\n\n\n\nArcticDEM project was a joint project supported by both the National Geospatial-Intelligence Agency(NGA) and the National Science Foundation(NSF) with the idea of creating a high resolution and high quality digital surface model(DSM). The product is distributed free of cost as time-dependent DEM strips and is hosted as https links that a user can use to download each strip. As per their policy\n\n\nThe seamless terrain mosaic can be distributed without restriction.\n\n\nThe created product is a 2-by-2 meter elevation cells over an over of over 20 million square kilometers and uses digital globe stereo imagery to create these high resolution DSM. The method used for the 2m derivate is Surface Extraction with TIN-based Search-space Minimization(SETSM).\n\n\nBased on their acknowledgements requests you can use\n\nAcknowledging PGC services(including data access)\n\n\n\n\nGeospatial support for this work provided by the Polar Geospatial Center under NSF OPP awards 1043681 \n 1559691.\n\n\n\n\nAcknowledging DEMS created from the ArcticDEM project\n\n\n\n\nDEMs provided by the Polar Geospatial Center under NSF OPP awards 1043681, 1559691 and 1542736.\n\n\n\n\nYou can find details on the background, scope and methods among other details \nhere\n\nA detailed acknowledgement link can be found \nhere\n\n\nWith this in mind and with the potential applications of using these toolsets there was a need to batch download the DEM files for your area of interest and to be able to extract, clean and process metadata. In all fairness this tool has a motive of extending this as an input to Google Earth Engine and hence the last tool which is the metadata parser is designed to create a metadata manifest in a csv file which GEE can understand and associate during asset upload. \n\n\nTable of contents\n\n\n\n\nInstallation\n\n\nGetting started\n\n\nUsage examples\n\n\nSubset to AOI\n\n\nEstimate Download Size\n\n\nDownload DEM\n\n\nExtract DEM\n\n\nMetadata Parsing for GEE\n\n\n\n\n\n\n\n\nInstallation\n\n\nWe assume that you have installed the requirements files to install all the necessary packages and libraries required to use this tool. To install packages from the requirements.txt file you can simply use\n\npip install -r requirements.txt\n. Remember that installation is an optional step and you can run this program by simply browsing to the pgcdem-cli file and typing \npython arcticdem.py\n. One of the only other requirement for this tool is the Master Shapefile for all DEM footprints(make sure to use the most updated version which can be found \nhere\n)\n\n\nThis toolbox also uses some functionality from GDAL\n\nFor installing GDAL in Ubuntu\n\n\nsudo add-apt-repository ppa:ubuntugis/ppa \n sudo apt-get update\nsudo apt-get install gdal-bin\n\n\n\n\nFor Windows I found this \nguide\n from UCLA\n\n\nTo install \nArcticDEM Batch Download \n Processing Tools:\n\n\ngit clone https://github.com/samapriya/ArcticDEM-Batch-Pipeline.git\ncd ArcticDEM-Batch-Pipeline \n pip install .\n\n\n\n\nThis release also contains a windows installer which bypasses the need for you to have admin permission, it does however require you to have python in the system path meaning when you open up command prompt you should be able to type python and start it within the command prompt window. Post installation using the installer you can just call ppipe using the command prompt similar to calling python. Give it a go post installation type\n\n\narcticdem -h\n\n\n\n\nThe advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. As a extra addon feature I have wrapped the cli into a Graphical User Interface(GUI) so some people will find it easier to use.\n\n\nTo install run\n\n\npython setup.py develop or python setup.py install\n\nIn a linux distribution\nsudo python setup.py develop or sudo python setup.py install\n\n\n\n\nGetting started\n\n\nTo obtain help for a specific functionality, simply call it with \nhelp\n\nswitch, e.g.: \narcticdem demextract -h\n. If you didn't install arcticdem, then you\ncan run it just by going to \narcticdem-cli\n directory and running \npython\narcticdem.py [arguments go here]\n\n\nAs usual, to print help  \narcticdem -h\n:\n\n\nArcticDEM Batch Download \n Processing Tools\npositional arguments:\n  { ,demaoi,demsize,demdownload,demextract,demmeta}\n                        ---------------------------------------\n                        -----Choose from ArcticDEM-Download Tools Below-----\n                        ---------------------------------------\n    demaoi              Allows user to subset Master ArcticDEM to their AOI\n    demsize             Allows users to estimate total download size and space\n                        left in your destination folder\n    demdownload         Allows users to batch download ArcticDEM Strips using\n                        aoi shapefile\n    demextract          Allows users to extract both image and metadata files\n                        from the zipped tar files\n    demmeta             Tool to process metadata files into CSV for all\n                        strips[For use with Google Earth Engine]\n\noptional arguments:\n  -h, --help            show this help message and exit\n\n\n\n\nSubset to AOI\n\n\nThe script clips the master ArcticDEM strip file to a smaller subset usgin an area of interest shapefile. This allows to get the strip DEM(s) for only the area of interest and to use that to download these files. The subset allows the user to limit the total amount of strips to be downloaded and processed. The script will create a new shapefile with the clipped subset of the master ArcticDEM strip file. \n\n\nMake sure you reproject your aoi shapefile to the same projection as the ArcticDEM strip file\n \n\n\nusage: arcticdem.py demaoi [-h] [--source SOURCE] [--target TARGET]\n                           [--output OUTPUT]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --source SOURCE  Choose location of your AOI shapefile\n  --target TARGET  Choose the location of the master ArcticDEM strip file\n  --output OUTPUT  Choose the location of the output shapefile based on your\n                   AOI\n\n\n\n\nAn example setup would be\n\n\narcticdem demaoi --source \nC:\\users\\aoi.shp\n --target \nC:\\users\\masterdem.shp\n --output \nC:\\users\\master_aoi.shp\n\n\n\n\n\nEstimate Download Size\n\n\nOne of the most common things you want to do is to know if the destination where you want to save these files has enough space before you begin the download process. This script allows you to query the total download size for your area of interest and the destination drive where you want to save the compressed files. It also recursively updates overall download size on the screen and print total size needed along with total download size in GB.\n\n\nusage: arcticdem.py demsize [-h] [--infile INFILE] [--path PATH]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --infile INFILE  Choose the clipped aoi file you clipped from demaoi\n                   tool[This is the subset of the master ArcticDEM Strip]\n  --path PATH      Choose the destination folder where you want your dem files\n                   to be saved[This checks available disk space]\n\n\n\n\nAn example setup would be\n\n\narcticdem demsize --infile \nC:\\users\\master_aoi.shp\n --path \nC:\\users\\ArcticDEM\n\n\n\n\n\nThe program might misbehave if the area of interest is extremely large or be sluggish in nature.\n\n\nDownload DEM\n\n\nWhat we were mainly interested after we know that we have enough space to download is to download the files. The script used a multi part download library to download the files quicker and in a more managed style to the destination given by the user.\n\n\nusage: arcticdem.py demdownload [-h] [--subset SUBSET]\n                                [--desination DESINATION]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --subset SUBSET       Choose the location of the output shapefile based on\n                        your AOI[You got this from demaoi tool]\n  --desination DESINATION\n                        Choose the destination where you want to download your\n                        files\n\n\n\n\nAn example setup would be\n\n\narcticdem demdownload --subset \nC:\\users\\master_aoi.shp\n --destination \nC:\\users\\ArcticDEM\n\n\n\n\n\nExtract DEM\n\n\nThis downloaded DEM files are tar or tar gz files and need to be extracted. The important thing to note is that the script retains the dem file, the matchtag file and the metadata text files in separate directories within the destination directory. \n\n\nusage: arcticdem.py demextract [-h] [--folder FOLDER]\n                               [--destination DESTINATION] [--action ACTION]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --folder FOLDER       Choose the download file where you downloaded your tar\n                        zipped files\n  --destination DESTINATION\n                        Choose the destination folder where you want your\n                        images and metadata files to be extracted\n  --action ACTION       Choose if you want your zipped files to be deleted\n                        post extraction \nyes\n|\nno\n\n\n\n\n\nAn example setup would be\n\n\narcticdem demdextract --folder \nC:\\users\\ArcticDEM\n --destination \nC:\\users\\ArcticDEM\\Extract\n --action \nyes\n\n\n\n\n\nMetadata Parsing for GEE\n\n\nOne of my key interest in working on the ArcticDEM parsing was to be able to upload this to Google Earth Engine(GEE). The metadata parser script allows you to parse all metadata into a combined csv file to be used to upload images to GEE. The manifest and upload tools are separate from this package tool and included in my gee_asset_manager_addon.\n\n\nusage: arcticdem.py demmeta [-h] [--folder FOLDER] [--metadata METADATA]\n                            [--error ERROR]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --folder FOLDER      Choose where you unzipped and extracted your DEM and\n                       metadata files\n  --metadata METADATA  Choose a path to the metadata file \nexample:\n                       users/desktop/metadata.csv\n\n  --error ERROR        Choose a path to the errorlog file \nexample:\n                       users/desktop/errorlog.csv\n\n\n\n\n\nAn example setup would be\n\n\narcticdem demmeta --folder \nC:\\users\\ArcticDEM\\Extract\\pgcmeta\n --metadata \nC:\\users\\arcticdem_metadata.csv\n --error \nC:\\users\\arcticdem_errorlog.csv\n\n\n\n\n\nChangelog\n\n\n[0.1.1] - 2017-08-12\n\n\nAdded\n\n\n\n\nCan now handle ogr input and includes instruction to project aoi in same projection as DEM strip.\n\n\nAdded the capability of skipping over already downloaded files and continues with left over downloads.\n\n\nCompleted recompiling executable to include changes.", 
            "title": "ArcticDEM-Batch-Pipeline"
        }, 
        {
            "location": "/projects/arcticdem_download/#arcticdem-batch-download-processing-tools", 
            "text": "ArcticDEM project was a joint project supported by both the National Geospatial-Intelligence Agency(NGA) and the National Science Foundation(NSF) with the idea of creating a high resolution and high quality digital surface model(DSM). The product is distributed free of cost as time-dependent DEM strips and is hosted as https links that a user can use to download each strip. As per their policy  The seamless terrain mosaic can be distributed without restriction.  The created product is a 2-by-2 meter elevation cells over an over of over 20 million square kilometers and uses digital globe stereo imagery to create these high resolution DSM. The method used for the 2m derivate is Surface Extraction with TIN-based Search-space Minimization(SETSM).  Based on their acknowledgements requests you can use Acknowledging PGC services(including data access)   Geospatial support for this work provided by the Polar Geospatial Center under NSF OPP awards 1043681   1559691.   Acknowledging DEMS created from the ArcticDEM project   DEMs provided by the Polar Geospatial Center under NSF OPP awards 1043681, 1559691 and 1542736.   You can find details on the background, scope and methods among other details  here \nA detailed acknowledgement link can be found  here  With this in mind and with the potential applications of using these toolsets there was a need to batch download the DEM files for your area of interest and to be able to extract, clean and process metadata. In all fairness this tool has a motive of extending this as an input to Google Earth Engine and hence the last tool which is the metadata parser is designed to create a metadata manifest in a csv file which GEE can understand and associate during asset upload.", 
            "title": "ArcticDEM Batch Download &amp; Processing Tools"
        }, 
        {
            "location": "/projects/arcticdem_download/#table-of-contents", 
            "text": "Installation  Getting started  Usage examples  Subset to AOI  Estimate Download Size  Download DEM  Extract DEM  Metadata Parsing for GEE", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/arcticdem_download/#installation", 
            "text": "We assume that you have installed the requirements files to install all the necessary packages and libraries required to use this tool. To install packages from the requirements.txt file you can simply use pip install -r requirements.txt . Remember that installation is an optional step and you can run this program by simply browsing to the pgcdem-cli file and typing  python arcticdem.py . One of the only other requirement for this tool is the Master Shapefile for all DEM footprints(make sure to use the most updated version which can be found  here )  This toolbox also uses some functionality from GDAL \nFor installing GDAL in Ubuntu  sudo add-apt-repository ppa:ubuntugis/ppa   sudo apt-get update\nsudo apt-get install gdal-bin  For Windows I found this  guide  from UCLA  To install  ArcticDEM Batch Download   Processing Tools:  git clone https://github.com/samapriya/ArcticDEM-Batch-Pipeline.git\ncd ArcticDEM-Batch-Pipeline   pip install .  This release also contains a windows installer which bypasses the need for you to have admin permission, it does however require you to have python in the system path meaning when you open up command prompt you should be able to type python and start it within the command prompt window. Post installation using the installer you can just call ppipe using the command prompt similar to calling python. Give it a go post installation type  arcticdem -h  The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. As a extra addon feature I have wrapped the cli into a Graphical User Interface(GUI) so some people will find it easier to use.  To install run  python setup.py develop or python setup.py install\n\nIn a linux distribution\nsudo python setup.py develop or sudo python setup.py install", 
            "title": "Installation"
        }, 
        {
            "location": "/projects/arcticdem_download/#getting-started", 
            "text": "To obtain help for a specific functionality, simply call it with  help \nswitch, e.g.:  arcticdem demextract -h . If you didn't install arcticdem, then you\ncan run it just by going to  arcticdem-cli  directory and running  python\narcticdem.py [arguments go here]  As usual, to print help   arcticdem -h :  ArcticDEM Batch Download   Processing Tools\npositional arguments:\n  { ,demaoi,demsize,demdownload,demextract,demmeta}\n                        ---------------------------------------\n                        -----Choose from ArcticDEM-Download Tools Below-----\n                        ---------------------------------------\n    demaoi              Allows user to subset Master ArcticDEM to their AOI\n    demsize             Allows users to estimate total download size and space\n                        left in your destination folder\n    demdownload         Allows users to batch download ArcticDEM Strips using\n                        aoi shapefile\n    demextract          Allows users to extract both image and metadata files\n                        from the zipped tar files\n    demmeta             Tool to process metadata files into CSV for all\n                        strips[For use with Google Earth Engine]\n\noptional arguments:\n  -h, --help            show this help message and exit", 
            "title": "Getting started"
        }, 
        {
            "location": "/projects/arcticdem_download/#subset-to-aoi", 
            "text": "The script clips the master ArcticDEM strip file to a smaller subset usgin an area of interest shapefile. This allows to get the strip DEM(s) for only the area of interest and to use that to download these files. The subset allows the user to limit the total amount of strips to be downloaded and processed. The script will create a new shapefile with the clipped subset of the master ArcticDEM strip file.   Make sure you reproject your aoi shapefile to the same projection as the ArcticDEM strip file    usage: arcticdem.py demaoi [-h] [--source SOURCE] [--target TARGET]\n                           [--output OUTPUT]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --source SOURCE  Choose location of your AOI shapefile\n  --target TARGET  Choose the location of the master ArcticDEM strip file\n  --output OUTPUT  Choose the location of the output shapefile based on your\n                   AOI  An example setup would be  arcticdem demaoi --source  C:\\users\\aoi.shp  --target  C:\\users\\masterdem.shp  --output  C:\\users\\master_aoi.shp", 
            "title": "Subset to AOI"
        }, 
        {
            "location": "/projects/arcticdem_download/#estimate-download-size", 
            "text": "One of the most common things you want to do is to know if the destination where you want to save these files has enough space before you begin the download process. This script allows you to query the total download size for your area of interest and the destination drive where you want to save the compressed files. It also recursively updates overall download size on the screen and print total size needed along with total download size in GB.  usage: arcticdem.py demsize [-h] [--infile INFILE] [--path PATH]\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --infile INFILE  Choose the clipped aoi file you clipped from demaoi\n                   tool[This is the subset of the master ArcticDEM Strip]\n  --path PATH      Choose the destination folder where you want your dem files\n                   to be saved[This checks available disk space]  An example setup would be  arcticdem demsize --infile  C:\\users\\master_aoi.shp  --path  C:\\users\\ArcticDEM   The program might misbehave if the area of interest is extremely large or be sluggish in nature.", 
            "title": "Estimate Download Size"
        }, 
        {
            "location": "/projects/arcticdem_download/#download-dem", 
            "text": "What we were mainly interested after we know that we have enough space to download is to download the files. The script used a multi part download library to download the files quicker and in a more managed style to the destination given by the user.  usage: arcticdem.py demdownload [-h] [--subset SUBSET]\n                                [--desination DESINATION]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --subset SUBSET       Choose the location of the output shapefile based on\n                        your AOI[You got this from demaoi tool]\n  --desination DESINATION\n                        Choose the destination where you want to download your\n                        files  An example setup would be  arcticdem demdownload --subset  C:\\users\\master_aoi.shp  --destination  C:\\users\\ArcticDEM", 
            "title": "Download DEM"
        }, 
        {
            "location": "/projects/arcticdem_download/#extract-dem", 
            "text": "This downloaded DEM files are tar or tar gz files and need to be extracted. The important thing to note is that the script retains the dem file, the matchtag file and the metadata text files in separate directories within the destination directory.   usage: arcticdem.py demextract [-h] [--folder FOLDER]\n                               [--destination DESTINATION] [--action ACTION]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --folder FOLDER       Choose the download file where you downloaded your tar\n                        zipped files\n  --destination DESTINATION\n                        Choose the destination folder where you want your\n                        images and metadata files to be extracted\n  --action ACTION       Choose if you want your zipped files to be deleted\n                        post extraction  yes | no   An example setup would be  arcticdem demdextract --folder  C:\\users\\ArcticDEM  --destination  C:\\users\\ArcticDEM\\Extract  --action  yes", 
            "title": "Extract DEM"
        }, 
        {
            "location": "/projects/arcticdem_download/#metadata-parsing-for-gee", 
            "text": "One of my key interest in working on the ArcticDEM parsing was to be able to upload this to Google Earth Engine(GEE). The metadata parser script allows you to parse all metadata into a combined csv file to be used to upload images to GEE. The manifest and upload tools are separate from this package tool and included in my gee_asset_manager_addon.  usage: arcticdem.py demmeta [-h] [--folder FOLDER] [--metadata METADATA]\n                            [--error ERROR]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --folder FOLDER      Choose where you unzipped and extracted your DEM and\n                       metadata files\n  --metadata METADATA  Choose a path to the metadata file  example:\n                       users/desktop/metadata.csv \n  --error ERROR        Choose a path to the errorlog file  example:\n                       users/desktop/errorlog.csv   An example setup would be  arcticdem demmeta --folder  C:\\users\\ArcticDEM\\Extract\\pgcmeta  --metadata  C:\\users\\arcticdem_metadata.csv  --error  C:\\users\\arcticdem_errorlog.csv", 
            "title": "Metadata Parsing for GEE"
        }, 
        {
            "location": "/projects/arcticdem_download/#changelog", 
            "text": "", 
            "title": "Changelog"
        }, 
        {
            "location": "/projects/arcticdem_download/#011-2017-08-12", 
            "text": "", 
            "title": "[0.1.1] - 2017-08-12"
        }, 
        {
            "location": "/projects/arcticdem_download/#added", 
            "text": "Can now handle ogr input and includes instruction to project aoi in same projection as DEM strip.  Added the capability of skipping over already downloaded files and continues with left over downloads.  Completed recompiling executable to include changes.", 
            "title": "Added"
        }, 
        {
            "location": "/projects/arcmap_addon/", 
            "text": "ArcMap Addons\n\n\n\n\nWhile working on different projects over sometime I had to create and use some tools which acted as valuable addons to arcmap toolbox. I am sharing a few and I will keep on updating these as I keep working on new models.This toolbox was created on ArcMap 10.4 and is not backward compatible. The toolbox can be downloaded and added to existing toolbox to create additional functionalities.\n\n\n\n\nTable of contents\n\n\n\n\nInstallation\n\n\nUsage examples\n\n\nBatch Raster to Point\n\n\nBatch Table to CSV\n\n\nEmail Notification\n\n\nIterative Clip\n\n\nMultiBand to Single Images\n\n\nRaster Properties as CSV\n\n\nRaster Copy Iterative\n\n\nFeature Select and Copy\n\n\nSelect and Calculate Field\n\n\n\n\n\n\nCredits\n\n\n\n\nInstallation\n\n\nWe assume that the user already has a copy of ArcMap \n=10.4. The toolbox can be downloaded along with adjoining script and added onto existing toolbox. You can keep this toolbox as part of the default toolboxes by clicking Save Settings\nSave as Default\n\n\nUsage examples\n\n\nUsage examples will vary and only continue to grow as new tools are added to the toolbox.\n\n\nBatch Raster to Point\n\n\nThis tool allows you to convert all Raster datasets in a folder into a point with the value field converted to GRID code. The tool supports all raster formats supported by ArcMap and renames the files automatically to the source file name. The raster pixel is converted to a centroid value.\n\n\n\n\nBatch Table to CSV\n\n\nThis tool allows you to batch convert all table files(in this case it looks for '.dbf' files) and converts the fields into csv columns. This is an effective way when you want to handle a large number of dbf files to be imported into other softwares and or processing chains.\n\n\n\n\nEmail Notification\n\n\nI found this to be one of the most useful tools when running a long process and not having to wait for your results. The tool makes use of the smtp library and allows the user to set up an email service for an email to be sent out after a process has been completed.\n\n\n\n\nIterative Clip\n\n\nThis tool does exactl what the name suggests it uses an iterator tool to clip a raster to different boundaries using shapefiles. Requires a single raster file and a folder with multiple polygons for clips.\n\n\n\n\nIf using on a private machine the Key is saved as a csv file for all future runs of the tool.\n\n\nMultiBand to Single Images\n\n\nThe aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.\n\n\n\n\nRaster Properties as CSV\n\n\nThe activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier\n\n\n\n\nRaster Copy Iterative\n\n\nHaving metadata helps in organising your asstets, but is not mandatory - you can skip it.\nThe downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types.\n\n\n\n\nFeature Select and Copy\n\n\nThese tools allow the users to iteratively select features using a given SQL expression and then extract that as a new feature class. This can be done iteratively for all feature classes in the folder.\n\n\n\n\nSelect and Calculate Field\n\n\nThis tool allows you to select a field and then recalculate or calculate it based on expressions. The user has the option of simply filling empty fields based on selection of a different field. Meaning I can calculate a field B based on a selection of a seperate field A if I want.\n\n\n\n\nCredits\n\n\nI would like to thank all those who bring me interesting problems to solve, there is little that is needed to keep one inspired.", 
            "title": "ArcMap Addon Tools"
        }, 
        {
            "location": "/projects/arcmap_addon/#arcmap-addons", 
            "text": "While working on different projects over sometime I had to create and use some tools which acted as valuable addons to arcmap toolbox. I am sharing a few and I will keep on updating these as I keep working on new models.This toolbox was created on ArcMap 10.4 and is not backward compatible. The toolbox can be downloaded and added to existing toolbox to create additional functionalities.", 
            "title": "ArcMap Addons"
        }, 
        {
            "location": "/projects/arcmap_addon/#table-of-contents", 
            "text": "Installation  Usage examples  Batch Raster to Point  Batch Table to CSV  Email Notification  Iterative Clip  MultiBand to Single Images  Raster Properties as CSV  Raster Copy Iterative  Feature Select and Copy  Select and Calculate Field    Credits", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/arcmap_addon/#installation", 
            "text": "We assume that the user already has a copy of ArcMap  =10.4. The toolbox can be downloaded along with adjoining script and added onto existing toolbox. You can keep this toolbox as part of the default toolboxes by clicking Save Settings Save as Default", 
            "title": "Installation"
        }, 
        {
            "location": "/projects/arcmap_addon/#usage-examples", 
            "text": "Usage examples will vary and only continue to grow as new tools are added to the toolbox.", 
            "title": "Usage examples"
        }, 
        {
            "location": "/projects/arcmap_addon/#batch-raster-to-point", 
            "text": "This tool allows you to convert all Raster datasets in a folder into a point with the value field converted to GRID code. The tool supports all raster formats supported by ArcMap and renames the files automatically to the source file name. The raster pixel is converted to a centroid value.", 
            "title": "Batch Raster to Point"
        }, 
        {
            "location": "/projects/arcmap_addon/#batch-table-to-csv", 
            "text": "This tool allows you to batch convert all table files(in this case it looks for '.dbf' files) and converts the fields into csv columns. This is an effective way when you want to handle a large number of dbf files to be imported into other softwares and or processing chains.", 
            "title": "Batch Table to CSV"
        }, 
        {
            "location": "/projects/arcmap_addon/#email-notification", 
            "text": "I found this to be one of the most useful tools when running a long process and not having to wait for your results. The tool makes use of the smtp library and allows the user to set up an email service for an email to be sent out after a process has been completed.", 
            "title": "Email Notification"
        }, 
        {
            "location": "/projects/arcmap_addon/#iterative-clip", 
            "text": "This tool does exactl what the name suggests it uses an iterator tool to clip a raster to different boundaries using shapefiles. Requires a single raster file and a folder with multiple polygons for clips.   If using on a private machine the Key is saved as a csv file for all future runs of the tool.", 
            "title": "Iterative Clip"
        }, 
        {
            "location": "/projects/arcmap_addon/#multiband-to-single-images", 
            "text": "The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.", 
            "title": "MultiBand to Single Images"
        }, 
        {
            "location": "/projects/arcmap_addon/#raster-properties-as-csv", 
            "text": "The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier", 
            "title": "Raster Properties as CSV"
        }, 
        {
            "location": "/projects/arcmap_addon/#raster-copy-iterative", 
            "text": "Having metadata helps in organising your asstets, but is not mandatory - you can skip it.\nThe downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types.", 
            "title": "Raster Copy Iterative"
        }, 
        {
            "location": "/projects/arcmap_addon/#feature-select-and-copy", 
            "text": "These tools allow the users to iteratively select features using a given SQL expression and then extract that as a new feature class. This can be done iteratively for all feature classes in the folder.", 
            "title": "Feature Select and Copy"
        }, 
        {
            "location": "/projects/arcmap_addon/#select-and-calculate-field", 
            "text": "This tool allows you to select a field and then recalculate or calculate it based on expressions. The user has the option of simply filling empty fields based on selection of a different field. Meaning I can calculate a field B based on a selection of a seperate field A if I want.", 
            "title": "Select and Calculate Field"
        }, 
        {
            "location": "/projects/arcmap_addon/#credits", 
            "text": "I would like to thank all those who bring me interesting problems to solve, there is little that is needed to keep one inspired.", 
            "title": "Credits"
        }, 
        {
            "location": "/projects/slack_notifier_cli_addon/", 
            "text": "Slack Notifier-CLI Addon\n\n\n\n\nFor those working with team collaborations and notifications slack is a quick alternative to group emails and chats. The need for a notification tool was also met with the use of the API which could be neatly tied up in clients. This CLI add-on was developed simply to function as an additional tool which can reside in an application folder and which can be called upon within a program and act as a notifier for events and updates. The tool combines simple methods in building channels and application bots and uses backends to send messages, files and to handle message history. The notifier was based on a comparison between available methods in tools such as pushbullet, pushover among a few for being able to have cross platform compatibility. Though this was designed as a means for getting process update for specific tools this CLI is essentially a plug and play into any system which can talk and pass arguments to this tool. In time additional and more refined implementation control might be included to handle specific functions.\n\n\nTable of contents\n\n\n\n\nGetting started\n\n\nSlack Credential\n\n\nSlack-Bot Credential\n\n\nSlack Messages\n\n\nSlack Message with Attachment\n\n\nSlack Delete All\n\n\n\n\n\n\n\n\nGetting started\n\n\nTo get started you need a Slack account and you can create one \nhere\n. Once you create the slack team you can further create an application and a bot within your team. This will allow you to get two API keys that you need for your tool. To access both these API tokens \ngo to\n. On the features, tab should be an option called OAuth \n Permissions and should provide you with \nOAuth Access Token\n and \nBot User OAuth Access Token\n. Note that the bot can only post in those channels where you have given it permission. If you add the bot to multiple channels you can specify the channel when posting messages or files.\n\n\nJust browse to the folder and perform \npython slack_addon.py -h\n:\n\n\nusage: slack_addon.py [-h] { ,smain,sbot,botupdate,botfile,slackdelete} ...\n\nSlack API Addon\n\npositional arguments:\n  { ,smain,sbot,botupdate,botfile,slackdelete}\n                        -------------------------------------------\n                        -----Choose from Slack Tools Below-----\n                        -------------------------------------------\n    smain               Allows you to save your Slack Main API Token\n    sbot                Allows you to save your Slack Bot API Token\n    botupdate           Allows your bot to post messages on slack channel\n    botfile             Allows you to post a file along with comments\n    slackdelete         Allows users to delete all messages and files posted\n                        by bots\n\noptional arguments:\n  -h, --help            show this help message and exit\n\n\n\n\nSlack Credential\n\n\nThis tool allows the user to save slack credential(OAuth Access Token) into \nusers/.config/slackkey\n making sure that they are user specific and are not shared on a system resource or location. It uses a getpass implementation and writes the password as a CSV and the other tools first try to read this and if not present asks for your password.\n\n\nusage: slack_addon.py smain [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\n\n\n\nSlack-Bot Credential\n\n\nThis tool allows the user to save slack bot credential(Bot User OAuth Access Token) into \nusers/.config/slackkey\n making sure that they are user specific and are not shared on a system resource or location. It uses a getpass implementation and writes the password as a CSV and the other tools first try to read this and if not present asks for your password.\n\n\nusage: slack_addon.py sbot [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\n\n\n\nSlack Messages\n\n\nThe slack messaging application is the primary tool which uses the \nslacker\n backend and allows the user to send messages as a bot to specific channel(s). The messaging service reads your Bot User OAuth Access Token and allows you to send messages to all channels where the bot has been added or has permission to post. If you do not specify the channel the bot posts to the general channel.\n\n\nusage: slack_addon.py botupdate [-h] [--channel CHANNEL] [--msg MSG]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --channel CHANNEL  Slack Bot update channel\n  --msg MSG          Slack Bot update message\n\n\n\n\nIncase you have already saved your password sending a message is as simple as \n\n\npython slack_addon.py --channel \n#general\n --message \nHello world\n\n\n\n\n\nThe application can simple be added by a call command with any process running as a system and the bot can update you about system processes, about usage, about application status and file sizes. The possibilities are endless.\n\n\nSlack Message with Attachment\n\n\nOne of the most interesting applications for me was to check that not only can I sent system and application updates but I could send snapshots or process outputs such as excel files and zip files and even error logs as needed. This tool allows the slack bot to not only send a message but only to include a file with the message. The filepath points to the location of the file, the fname allows you to name the file accordingly and the cmmt option is used to add a coment of message along with the file. The channel option allows you to choose a specific channel you want to post the message and as earlier it will post to general channel. \n\n\nusage: slack_addon.py botfile [-h] [--channel CHANNEL] [--filepath FILEPATH]\n                              [--cmmt CMMT] [--fname FNAME]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --channel CHANNEL    Slack Bot channel\n  --filepath FILEPATH  Slack Bot file path to upload\n  --cmmt CMMT          Slack Bot file comment\n  --fname FNAME        Slack Bot filename\n\n\n\n\nIncase you have already saved your password a setup would be simply\n\n\npython slack_addon.py --channel \n#general\n --filepath \n/users/myfilepath.csv\n --cmmt \nCheck the error logs\n --fname \nerrorlog\n\n\n\n\n\nSlack Delete All\n\n\nOne of the current non existent methods within Slack is the capability to delete all messages. This is built using a backend \ncli tool\n to delete all messages and files if needed and I integrated that in the current CLI. The current tool is primarily related to deleting all messages and files for cleaning up a channel as needed. The tool uses your main slack channel API token and uses that to delete all messages from all users but can be modified to delete messages from specific bots if needed. For now the tool deletes all messages and files in the general channel.\n\n\nusage: slack_addon.py slackdelete [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit\n\n\n\n\nto use this tool simple type \npython slack_addon.py slackdelete", 
            "title": "Slack Notifier CLI Addon"
        }, 
        {
            "location": "/projects/slack_notifier_cli_addon/#slack-notifier-cli-addon", 
            "text": "For those working with team collaborations and notifications slack is a quick alternative to group emails and chats. The need for a notification tool was also met with the use of the API which could be neatly tied up in clients. This CLI add-on was developed simply to function as an additional tool which can reside in an application folder and which can be called upon within a program and act as a notifier for events and updates. The tool combines simple methods in building channels and application bots and uses backends to send messages, files and to handle message history. The notifier was based on a comparison between available methods in tools such as pushbullet, pushover among a few for being able to have cross platform compatibility. Though this was designed as a means for getting process update for specific tools this CLI is essentially a plug and play into any system which can talk and pass arguments to this tool. In time additional and more refined implementation control might be included to handle specific functions.", 
            "title": "Slack Notifier-CLI Addon"
        }, 
        {
            "location": "/projects/slack_notifier_cli_addon/#table-of-contents", 
            "text": "Getting started  Slack Credential  Slack-Bot Credential  Slack Messages  Slack Message with Attachment  Slack Delete All", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/slack_notifier_cli_addon/#getting-started", 
            "text": "To get started you need a Slack account and you can create one  here . Once you create the slack team you can further create an application and a bot within your team. This will allow you to get two API keys that you need for your tool. To access both these API tokens  go to . On the features, tab should be an option called OAuth   Permissions and should provide you with  OAuth Access Token  and  Bot User OAuth Access Token . Note that the bot can only post in those channels where you have given it permission. If you add the bot to multiple channels you can specify the channel when posting messages or files.  Just browse to the folder and perform  python slack_addon.py -h :  usage: slack_addon.py [-h] { ,smain,sbot,botupdate,botfile,slackdelete} ...\n\nSlack API Addon\n\npositional arguments:\n  { ,smain,sbot,botupdate,botfile,slackdelete}\n                        -------------------------------------------\n                        -----Choose from Slack Tools Below-----\n                        -------------------------------------------\n    smain               Allows you to save your Slack Main API Token\n    sbot                Allows you to save your Slack Bot API Token\n    botupdate           Allows your bot to post messages on slack channel\n    botfile             Allows you to post a file along with comments\n    slackdelete         Allows users to delete all messages and files posted\n                        by bots\n\noptional arguments:\n  -h, --help            show this help message and exit", 
            "title": "Getting started"
        }, 
        {
            "location": "/projects/slack_notifier_cli_addon/#slack-credential", 
            "text": "This tool allows the user to save slack credential(OAuth Access Token) into  users/.config/slackkey  making sure that they are user specific and are not shared on a system resource or location. It uses a getpass implementation and writes the password as a CSV and the other tools first try to read this and if not present asks for your password.  usage: slack_addon.py smain [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit", 
            "title": "Slack Credential"
        }, 
        {
            "location": "/projects/slack_notifier_cli_addon/#slack-bot-credential", 
            "text": "This tool allows the user to save slack bot credential(Bot User OAuth Access Token) into  users/.config/slackkey  making sure that they are user specific and are not shared on a system resource or location. It uses a getpass implementation and writes the password as a CSV and the other tools first try to read this and if not present asks for your password.  usage: slack_addon.py sbot [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit", 
            "title": "Slack-Bot Credential"
        }, 
        {
            "location": "/projects/slack_notifier_cli_addon/#slack-messages", 
            "text": "The slack messaging application is the primary tool which uses the  slacker  backend and allows the user to send messages as a bot to specific channel(s). The messaging service reads your Bot User OAuth Access Token and allows you to send messages to all channels where the bot has been added or has permission to post. If you do not specify the channel the bot posts to the general channel.  usage: slack_addon.py botupdate [-h] [--channel CHANNEL] [--msg MSG]\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --channel CHANNEL  Slack Bot update channel\n  --msg MSG          Slack Bot update message  Incase you have already saved your password sending a message is as simple as   python slack_addon.py --channel  #general  --message  Hello world   The application can simple be added by a call command with any process running as a system and the bot can update you about system processes, about usage, about application status and file sizes. The possibilities are endless.", 
            "title": "Slack Messages"
        }, 
        {
            "location": "/projects/slack_notifier_cli_addon/#slack-message-with-attachment", 
            "text": "One of the most interesting applications for me was to check that not only can I sent system and application updates but I could send snapshots or process outputs such as excel files and zip files and even error logs as needed. This tool allows the slack bot to not only send a message but only to include a file with the message. The filepath points to the location of the file, the fname allows you to name the file accordingly and the cmmt option is used to add a coment of message along with the file. The channel option allows you to choose a specific channel you want to post the message and as earlier it will post to general channel.   usage: slack_addon.py botfile [-h] [--channel CHANNEL] [--filepath FILEPATH]\n                              [--cmmt CMMT] [--fname FNAME]\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --channel CHANNEL    Slack Bot channel\n  --filepath FILEPATH  Slack Bot file path to upload\n  --cmmt CMMT          Slack Bot file comment\n  --fname FNAME        Slack Bot filename  Incase you have already saved your password a setup would be simply  python slack_addon.py --channel  #general  --filepath  /users/myfilepath.csv  --cmmt  Check the error logs  --fname  errorlog", 
            "title": "Slack Message with Attachment"
        }, 
        {
            "location": "/projects/slack_notifier_cli_addon/#slack-delete-all", 
            "text": "One of the current non existent methods within Slack is the capability to delete all messages. This is built using a backend  cli tool  to delete all messages and files if needed and I integrated that in the current CLI. The current tool is primarily related to deleting all messages and files for cleaning up a channel as needed. The tool uses your main slack channel API token and uses that to delete all messages from all users but can be modified to delete messages from specific bots if needed. For now the tool deletes all messages and files in the general channel.  usage: slack_addon.py slackdelete [-h]\n\noptional arguments:\n  -h, --help  show this help message and exit  to use this tool simple type  python slack_addon.py slackdelete", 
            "title": "Slack Delete All"
        }, 
        {
            "location": "/projects/synthetic_models/", 
            "text": "Synthetic LandScape Generation\n\n\nWhile working on synthetic landscape generation or neutral landscape generation, it it interesting to try out different randomizing and clustering algorithms. The ones included here were part of the same experiments. The outputs are generally preceded by a header so that the ASCII files can be read easily in a GIS software such as ArcMap. The output file is ASCII type since this is commonly accepted by multiple tools. The number of rows and columns can be changed as can the no data value which for now is set to a 16 bit signed integer output. There are codes included to generate a video using landscapes as frames within the output file. Further work can include the nlmpy module \n\n\nTable of contents\n\n\n\n\nInstallation\n\n\nPackages\n\n\nClumped matrix algorithms\n\n\nFragmentation Aggregation Algorithms\n\n\nNlmPy applications\n\n\nNoise Function Terrain Generation\n\n\nRandom matrix algorithms\n\n\nRandom matrix to video\n\n\n\n\n\n\nCredits\n\n\n\n\nInstallation\n\n\nWe assume that the user already has a copy of Matlab and has installed necessary libraries in python such as nlmpy and numpy+mlk and scipy. The toolbox can be modified to accomodate varying sizes of images to be produced and varying number of classes. The codes are mostly written in matlab and some implementation in NlmPy produced by \n Etherington et al\n are included as well.\n\n\nPackages\n\n\nEach folder has a variety of packages and tools available to run different kinds of analysis. Tools and methods include and are not limited to the following\n\n\nClumped matrix algorithms\n\n\nThis allows for clumping to occur in random matrix algorithms such that there is an element of non random allocation of class\n\n\n\n\n\n\n\n\nfilename\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nclumped_randi_land.m\n\n\nclumping algorithm applied to Uniformly distributed pseudorandom integers\n\n\n\n\n\n\nclumped_sprand_land\n\n\nclumping algorithm applied to Sparse uniformly distributed random matrix\n\n\n\n\n\n\n\n\nFragmentation Aggregation Algorithms\n\n\nThis one is more specific looking at different fragmentation and aggregation algorithms and allows the user to set convergence time for solution.It includes Gibbs model and Metropolis-Hastings algorithm.\n\n\nNlmPy applications\n\n\nNlmPy was created as a python library which allows the user to user different algorithms \n Etherington et al\n. The output files as ASCII to allow for easy read.\n\n\nNoise Function Terrain Generation\n\n\nThese Matlab functions allows you to test noise functions to generate artificial terrains using noise function executables.Source codes for the c++ noise functions files are included in the adjoining folder and were provided kindly by Travis Archer for open use.The codes create a bmp output which are then modified to create binned responses and landscape classes as needed. Noise types include\n\n Cell Noise\n\n Diamond Square\n\n Erosion\n\n Midpoint Displacement\n\n Perline Noise\n\n Simplex Noise\n* Value Noise\n\n\nRandom matrix algorithms\n\n\nThis uses the random matrix functions within matlab to extract different landscape types. Once again they are exported as ASCII.\n\n\n\n\n\n\n\n\nfilename\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nrandi_land.m\n\n\nUniformly distributed pseudorandom integers\n\n\n\n\n\n\nsprand_land.m\n\n\nSparse uniformly distributed random matrix\n\n\n\n\n\n\nrand_land.m\n\n\nUniformly distributed random numbers\n\n\n\n\n\n\nrng_seed_land\n\n\nRandom Seed Generation with Random Number Generator\n\n\n\n\n\n\n\n\nRandom matrix to video\n\n\nThese are experiments within matlab to use the landscapes generated as frames in a video output file from the landscapes. The video output is generally in avi format and includes slices of the landscapes produced.\n\n\nCredits\n\n\nI would like to thank Dr. Scott Robeson my mentor among others who got me interested in some topics on Landscape ecology.", 
            "title": "Synthetic Landscape Generation Models"
        }, 
        {
            "location": "/projects/synthetic_models/#synthetic-landscape-generation", 
            "text": "While working on synthetic landscape generation or neutral landscape generation, it it interesting to try out different randomizing and clustering algorithms. The ones included here were part of the same experiments. The outputs are generally preceded by a header so that the ASCII files can be read easily in a GIS software such as ArcMap. The output file is ASCII type since this is commonly accepted by multiple tools. The number of rows and columns can be changed as can the no data value which for now is set to a 16 bit signed integer output. There are codes included to generate a video using landscapes as frames within the output file. Further work can include the nlmpy module", 
            "title": "Synthetic LandScape Generation"
        }, 
        {
            "location": "/projects/synthetic_models/#table-of-contents", 
            "text": "Installation  Packages  Clumped matrix algorithms  Fragmentation Aggregation Algorithms  NlmPy applications  Noise Function Terrain Generation  Random matrix algorithms  Random matrix to video    Credits", 
            "title": "Table of contents"
        }, 
        {
            "location": "/projects/synthetic_models/#installation", 
            "text": "We assume that the user already has a copy of Matlab and has installed necessary libraries in python such as nlmpy and numpy+mlk and scipy. The toolbox can be modified to accomodate varying sizes of images to be produced and varying number of classes. The codes are mostly written in matlab and some implementation in NlmPy produced by   Etherington et al  are included as well.", 
            "title": "Installation"
        }, 
        {
            "location": "/projects/synthetic_models/#packages", 
            "text": "Each folder has a variety of packages and tools available to run different kinds of analysis. Tools and methods include and are not limited to the following", 
            "title": "Packages"
        }, 
        {
            "location": "/projects/synthetic_models/#clumped-matrix-algorithms", 
            "text": "This allows for clumping to occur in random matrix algorithms such that there is an element of non random allocation of class     filename  Description      clumped_randi_land.m  clumping algorithm applied to Uniformly distributed pseudorandom integers    clumped_sprand_land  clumping algorithm applied to Sparse uniformly distributed random matrix", 
            "title": "Clumped matrix algorithms"
        }, 
        {
            "location": "/projects/synthetic_models/#fragmentation-aggregation-algorithms", 
            "text": "This one is more specific looking at different fragmentation and aggregation algorithms and allows the user to set convergence time for solution.It includes Gibbs model and Metropolis-Hastings algorithm.", 
            "title": "Fragmentation Aggregation Algorithms"
        }, 
        {
            "location": "/projects/synthetic_models/#nlmpy-applications", 
            "text": "NlmPy was created as a python library which allows the user to user different algorithms   Etherington et al . The output files as ASCII to allow for easy read.", 
            "title": "NlmPy applications"
        }, 
        {
            "location": "/projects/synthetic_models/#noise-function-terrain-generation", 
            "text": "These Matlab functions allows you to test noise functions to generate artificial terrains using noise function executables.Source codes for the c++ noise functions files are included in the adjoining folder and were provided kindly by Travis Archer for open use.The codes create a bmp output which are then modified to create binned responses and landscape classes as needed. Noise types include  Cell Noise  Diamond Square  Erosion  Midpoint Displacement  Perline Noise  Simplex Noise\n* Value Noise", 
            "title": "Noise Function Terrain Generation"
        }, 
        {
            "location": "/projects/synthetic_models/#random-matrix-algorithms", 
            "text": "This uses the random matrix functions within matlab to extract different landscape types. Once again they are exported as ASCII.     filename  Description      randi_land.m  Uniformly distributed pseudorandom integers    sprand_land.m  Sparse uniformly distributed random matrix    rand_land.m  Uniformly distributed random numbers    rng_seed_land  Random Seed Generation with Random Number Generator", 
            "title": "Random matrix algorithms"
        }, 
        {
            "location": "/projects/synthetic_models/#random-matrix-to-video", 
            "text": "These are experiments within matlab to use the landscapes generated as frames in a video output file from the landscapes. The video output is generally in avi format and includes slices of the landscapes produced.", 
            "title": "Random matrix to video"
        }, 
        {
            "location": "/projects/synthetic_models/#credits", 
            "text": "I would like to thank Dr. Scott Robeson my mentor among others who got me interested in some topics on Landscape ecology.", 
            "title": "Credits"
        }, 
        {
            "location": "/projects/open-impact/", 
            "text": "Open Impact Training and Hackathons\n\n\nOpen Impact projects include hackathons, training to support developer interaction and enegagement of Education and Research users.\n\n\nStanford Big Earth Hackathon\n\n\nOver 100 students gathered with industry and faculty mentors on April 14-15, 2018 to hack for planet earth. Congrats to the participants for an exciting weekend of planetary solutions.\n\n\n\n\nCSDMS 2018-Introduction to Google Earth Engine\n\n\nPresentations and more of the joined CSDMS - SEN 2018 annual meeting Geoprocesses, geohazards - CSDMS 2018. Held May 22-24th 2018 in Boulder Colorado, USA. The presentation was Introduction to Google Earth Engine", 
            "title": "Open Impact"
        }, 
        {
            "location": "/projects/open-impact/#open-impact-training-and-hackathons", 
            "text": "Open Impact projects include hackathons, training to support developer interaction and enegagement of Education and Research users.", 
            "title": "Open Impact Training and Hackathons"
        }, 
        {
            "location": "/projects/open-impact/#stanford-big-earth-hackathon", 
            "text": "Over 100 students gathered with industry and faculty mentors on April 14-15, 2018 to hack for planet earth. Congrats to the participants for an exciting weekend of planetary solutions.", 
            "title": "Stanford Big Earth Hackathon"
        }, 
        {
            "location": "/projects/open-impact/#csdms-2018-introduction-to-google-earth-engine", 
            "text": "Presentations and more of the joined CSDMS - SEN 2018 annual meeting Geoprocesses, geohazards - CSDMS 2018. Held May 22-24th 2018 in Boulder Colorado, USA. The presentation was Introduction to Google Earth Engine", 
            "title": "CSDMS 2018-Introduction to Google Earth Engine"
        }, 
        {
            "location": "/project-citations/", 
            "text": "Project Citations\n\n\nGoogle Earth Engine Account Transfer Tool\n\n\n\n\nSamapriya Roy. (2018, February 27). samapriya/gee-takeout: Google Earth Engine Account Transfer Tool (Version 0.1).\nZenodo. http://doi.org/10.5281/zenodo.1185158\n\n\n\n\nPlanet-Batch-Slack-Pipeline-CLI\n\n\n\n\nSamapriya Roy. (2017, December 4). samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI (Version 0.1.4).\nZenodo. http://doi.org/10.5281/zenodo.1079887\n\n\n\n\n\n\nClip-Ship-Planet-CLI\n\n\n\n\nSamapriya Roy. (2017, October 9). samapriya/Clip-Ship-Planet-CLI: Clip-Ship-Batch Planet Command Line Interface (Version 0.2.0).\nZenodo. http://doi.org/10.5281/zenodo.1005752\n\n\n\n\n\n\nSlack-Notifier-CLI-Addon\n\n\n\n\nSamapriya Roy. (2017, September 5). samapriya/Slack-Notifier-CLI-Addon: Slack Notifier-CLI Addon (Version 0.1.0).\nZenodo. http://doi.org/10.5281/zenodo.885505\n\n\n\n\n\n\nPlanet-GEE-Pipeline-CLI\n\n\n\n\nSamapriya Roy. (2018, June 30). samapriya/Planet-GEE-Pipeline-CLI: Planet-GEE-Pipeline-CLI (Version 0.3.5).\nZenodo. http://doi.org/10.5281/zenodo.1302069\n\n\n\n\n\n\nPlanet-GEE-Pipeline-GUI\n\n\n\n\nSamapriya Roy. (2017, June 25). samapriya/Planet-GEE-Pipeline-GUI:Planet-GEE-Pipeline-GUI (Version 0.1.4).\nZenodo. http://doi.org/10.5281/zenodo.817739\n\n\n\n\n\n\nPlanet-Pipeline-GUI\n\n\n\n\nSamapriya Roy. (2017, August 17). samapriya/Planet-Pipeline-GUI: Planet-Pipeline-GUI (Version 0.3).\nZenodo. http://doi.org/10.5281/zenodo.844149\n\n\n\n\n\n\nGEE Asset Manager Addons\n\n\n\n\nSamapriya Roy. (2017, November 29). samapriya/gee_asset_manager_addon: GEE Asset Manager with Addons (Version 0.2.2).\nZenodo. http://doi.org/10.5281/zenodo.1068184\n\n\n\n\n\n\nSynthetic LandScape Models\n\n\n\n\nSamapriya Roy. (2017, May 21). samapriya/synthetic-landScape-generation-nlm: Synthetic LandScape Generation (Version 0.1.1).\nZenodo. http://doi.org/10.5281/zenodo.581701\n\n\n\n\n\n\nArcMap Addons\n\n\n\n\nSamapriya Roy. (2017, October 12). samapriya/arcmap-addons: ArcMap Addons (Version 0.2).\nZenodo. http://doi.org/10.5281/zenodo.1009210\n\n\n\n\n\n\nArcticDEM-Batch-Pipeline\n\n\n\n\nSamapriya Roy. (2017, August 12). samapriya/ArcticDEM-Batch-Pipeline: ArcticDEM-Batch-Pipeline (Version 0.1.1).\nZenodo. http://doi.org/10.5281/zenodo.842056\n\n\n\n\n\n\nJetstream-Unofficial-addon\n\n\n\n\nSamapriya Roy. (2017, August 16). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon (Version 0.1.1).\nZenodo. http://doi.org/10.5281/zenodo.844018", 
            "title": "Project Citations"
        }, 
        {
            "location": "/project-citations/#project-citations", 
            "text": "Google Earth Engine Account Transfer Tool   Samapriya Roy. (2018, February 27). samapriya/gee-takeout: Google Earth Engine Account Transfer Tool (Version 0.1).\nZenodo. http://doi.org/10.5281/zenodo.1185158  Planet-Batch-Slack-Pipeline-CLI   Samapriya Roy. (2017, December 4). samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI (Version 0.1.4).\nZenodo. http://doi.org/10.5281/zenodo.1079887   Clip-Ship-Planet-CLI   Samapriya Roy. (2017, October 9). samapriya/Clip-Ship-Planet-CLI: Clip-Ship-Batch Planet Command Line Interface (Version 0.2.0).\nZenodo. http://doi.org/10.5281/zenodo.1005752   Slack-Notifier-CLI-Addon   Samapriya Roy. (2017, September 5). samapriya/Slack-Notifier-CLI-Addon: Slack Notifier-CLI Addon (Version 0.1.0).\nZenodo. http://doi.org/10.5281/zenodo.885505   Planet-GEE-Pipeline-CLI   Samapriya Roy. (2018, June 30). samapriya/Planet-GEE-Pipeline-CLI: Planet-GEE-Pipeline-CLI (Version 0.3.5).\nZenodo. http://doi.org/10.5281/zenodo.1302069   Planet-GEE-Pipeline-GUI   Samapriya Roy. (2017, June 25). samapriya/Planet-GEE-Pipeline-GUI:Planet-GEE-Pipeline-GUI (Version 0.1.4).\nZenodo. http://doi.org/10.5281/zenodo.817739   Planet-Pipeline-GUI   Samapriya Roy. (2017, August 17). samapriya/Planet-Pipeline-GUI: Planet-Pipeline-GUI (Version 0.3).\nZenodo. http://doi.org/10.5281/zenodo.844149   GEE Asset Manager Addons   Samapriya Roy. (2017, November 29). samapriya/gee_asset_manager_addon: GEE Asset Manager with Addons (Version 0.2.2).\nZenodo. http://doi.org/10.5281/zenodo.1068184   Synthetic LandScape Models   Samapriya Roy. (2017, May 21). samapriya/synthetic-landScape-generation-nlm: Synthetic LandScape Generation (Version 0.1.1).\nZenodo. http://doi.org/10.5281/zenodo.581701   ArcMap Addons   Samapriya Roy. (2017, October 12). samapriya/arcmap-addons: ArcMap Addons (Version 0.2).\nZenodo. http://doi.org/10.5281/zenodo.1009210   ArcticDEM-Batch-Pipeline   Samapriya Roy. (2017, August 12). samapriya/ArcticDEM-Batch-Pipeline: ArcticDEM-Batch-Pipeline (Version 0.1.1).\nZenodo. http://doi.org/10.5281/zenodo.842056   Jetstream-Unofficial-addon   Samapriya Roy. (2017, August 16). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon (Version 0.1.1).\nZenodo. http://doi.org/10.5281/zenodo.844018", 
            "title": "Project Citations"
        }, 
        {
            "location": "/release-notes/", 
            "text": "Release notes\n\n\n\n\n\n\n\n\nProject Name\n\n\nDigital Object Identification Number(DOI)\n\n\nRelease\n\n\n\n\n\n\n\n\n\n\nGoogle Earth Engine Takeout\n\n\n\n\n0.1\n\n\n\n\n\n\nPlanet-Batch-Slack-Pipeline\n\n\n\n\n0.1.4\n\n\n\n\n\n\nClip-Ship-Planet-CLI\n\n\n\n\n0.2.5\n\n\n\n\n\n\nSlack-Notifier-CLI-Addon\n\n\n\n\n0.1.0\n\n\n\n\n\n\nPlanet-GEE-Pipeline-CLI\n\n\n\n\n0.3.5\n\n\n\n\n\n\nPlanet-GEE-Pipeline-GUI\n\n\n\n\n0.1.4\n\n\n\n\n\n\nPlanet-Pipeline-GUI\n\n\n\n\n0.3\n\n\n\n\n\n\nGEE Asset Manager Addons\n\n\n\n\n0.2.3\n\n\n\n\n\n\nSynthetic LandScape Models\n\n\n\n\n0.1.1\n\n\n\n\n\n\nArcMap Addons\n\n\n\n\n0.2\n\n\n\n\n\n\nArcticDEM-Batch-Pipeline\n\n\n\n\n0.1.1\n\n\n\n\n\n\nJetstream-Unofficial-addon\n\n\n\n\n0.1.1", 
            "title": "Release notes"
        }, 
        {
            "location": "/release-notes/#release-notes", 
            "text": "Project Name  Digital Object Identification Number(DOI)  Release      Google Earth Engine Takeout   0.1    Planet-Batch-Slack-Pipeline   0.1.4    Clip-Ship-Planet-CLI   0.2.5    Slack-Notifier-CLI-Addon   0.1.0    Planet-GEE-Pipeline-CLI   0.3.5    Planet-GEE-Pipeline-GUI   0.1.4    Planet-Pipeline-GUI   0.3    GEE Asset Manager Addons   0.2.3    Synthetic LandScape Models   0.1.1    ArcMap Addons   0.2    ArcticDEM-Batch-Pipeline   0.1.1    Jetstream-Unofficial-addon   0.1.1", 
            "title": "Release notes"
        }, 
        {
            "location": "/license/", 
            "text": "Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n\n\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n\n\n\n\n\nDefinitions.\n\n\n\"License\" shall mean the terms and conditions for use, reproduction,\n  and distribution as defined by Sections 1 through 9 of this document.\n\n\n\"Licensor\" shall mean the copyright owner or entity authorized by\n  the copyright owner that is granting the License.\n\n\n\"Legal Entity\" shall mean the union of the acting entity and all\n  other entities that control, are controlled by, or are under common\n  control with that entity. For the purposes of this definition,\n  \"control\" means (i) the power, direct or indirect, to cause the\n  direction or management of such entity, whether by contract or\n  otherwise, or (ii) ownership of fifty percent (50%) or more of the\n  outstanding shares, or (iii) beneficial ownership of such entity.\n\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity\n  exercising permissions granted by this License.\n\n\n\"Source\" form shall mean the preferred form for making modifications,\n  including but not limited to software source code, documentation\n  source, and configuration files.\n\n\n\"Object\" form shall mean any form resulting from mechanical\n  transformation or translation of a Source form, including but\n  not limited to compiled object code, generated documentation,\n  and conversions to other media types.\n\n\n\"Work\" shall mean the work of authorship, whether in Source or\n  Object form, made available under the License, as indicated by a\n  copyright notice that is included in or attached to the work\n  (an example is provided in the Appendix below).\n\n\n\"Derivative Works\" shall mean any work, whether in Source or Object\n  form, that is based on (or derived from) the Work and for which the\n  editorial revisions, annotations, elaborations, or other modifications\n  represent, as a whole, an original work of authorship. For the purposes\n  of this License, Derivative Works shall not include works that remain\n  separable from, or merely link (or bind by name) to the interfaces of,\n  the Work and Derivative Works thereof.\n\n\n\"Contribution\" shall mean any work of authorship, including\n  the original version of the Work and any modifications or additions\n  to that Work or Derivative Works thereof, that is intentionally\n  submitted to Licensor for inclusion in the Work by the copyright owner\n  or by an individual or Legal Entity authorized to submit on behalf of\n  the copyright owner. For the purposes of this definition, \"submitted\"\n  means any form of electronic, verbal, or written communication sent\n  to the Licensor or its representatives, including but not limited to\n  communication on electronic mailing lists, source code control systems,\n  and issue tracking systems that are managed by, or on behalf of, the\n  Licensor for the purpose of discussing and improving the Work, but\n  excluding communication that is conspicuously marked or otherwise\n  designated in writing by the copyright owner as \"Not a Contribution.\"\n\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity\n  on behalf of whom a Contribution has been received by Licensor and\n  subsequently incorporated within the Work.\n\n\n\n\n\n\nGrant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n\n\n\n\n\nGrant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n\n\n\n\n\nRedistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n\n(a) You must give any other recipients of the Work or\n      Derivative Works a copy of this License; and\n\n\n(b) You must cause any modified files to carry prominent notices\n      stating that You changed the files; and\n\n\n(c) You must retain, in the Source form of any Derivative Works\n      that You distribute, all copyright, patent, trademark, and\n      attribution notices from the Source form of the Work,\n      excluding those notices that do not pertain to any part of\n      the Derivative Works; and\n\n\n(d) If the Work includes a \"NOTICE\" text file as part of its\n      distribution, then any Derivative Works that You distribute must\n      include a readable copy of the attribution notices contained\n      within such NOTICE file, excluding those notices that do not\n      pertain to any part of the Derivative Works, in at least one\n      of the following places: within a NOTICE text file distributed\n      as part of the Derivative Works; within the Source form or\n      documentation, if provided along with the Derivative Works; or,\n      within a display generated by the Derivative Works, if and\n      wherever such third-party notices normally appear. The contents\n      of the NOTICE file are for informational purposes only and\n      do not modify the License. You may add Your own attribution\n      notices within Derivative Works that You distribute, alongside\n      or as an addendum to the NOTICE text from the Work, provided\n      that such additional attribution notices cannot be construed\n      as modifying the License.\n\n\nYou may add Your own copyright statement to Your modifications and\n  may provide additional or different license terms and conditions\n  for use, reproduction, or distribution of Your modifications, or\n  for any such Derivative Works as a whole, provided Your use,\n  reproduction, and distribution of the Work otherwise complies with\n  the conditions stated in this License.\n\n\n\n\n\n\nSubmission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n\n\n\n\n\nTrademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n\n\n\n\n\nDisclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n\n\n\n\n\nLimitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n\n\n\n\n\nAccepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n\n\n\n\n\nEND OF TERMS AND CONDITIONS\n\n\nAPPENDIX: How to apply the Apache License to your work.\n\n\n  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"{}\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n\n\n\nCopyright {2017} {Samapriya Roy}\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n\n\nUnless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.", 
            "title": "License"
        }, 
        {
            "location": "/credits/", 
            "text": "Credits\n\n\n\n\nJetStream\n A portion of the work is suported by JetStream Grant TG-GEO160014.\n\n\nArcticDEM data provided by Polar Geospatial Center \nPGC\n\n\nAlso supported by \nPlanet Labs Ambassador Program", 
            "title": "Credits"
        }, 
        {
            "location": "/credits/#credits", 
            "text": "JetStream  A portion of the work is suported by JetStream Grant TG-GEO160014.  ArcticDEM data provided by Polar Geospatial Center  PGC  Also supported by  Planet Labs Ambassador Program", 
            "title": "Credits"
        }
    ]
}