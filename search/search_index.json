{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 These github projects are a collection of tools and improvements made to accomplish tasks as well as potential tools and applications designed for future applications. Core capabilities include tools that interact with Google Earth Engine, Planet Labs API, Arctic DEM datasets and coupling large scale data pipelines using satadd projects. These projects extends tools and stand alone CLI(s) and GUI(s) for testing and implementation on various platforms. Some tools are designed to allow for easy intergation with cloud services such as Digital Ocean along with standalone hackathon tools and collaborations. Projects Include work on following Platforms Two projects included looking at different aspects of raster analysis and landscape modeling and hence I decided to create an ArcMap toolbox with tools that I have developed and continue to develop further. The toolbox can be downloaded and added to existing toolbox to create additional functionalities. The synthetic landscape models are designed to try different randomizing and clustering algorithms. An additional project was completed during the Polar Geospatial Bootcamp which allows users to bulk download and process 2m high resolution ArcticDEM provided by the Polar Geospatial Center. The list now also includes a notifier application using the Slack interface along with including an application of the Clips API from Planet. Recent Project Names Digital Object Identification Number(DOI) GitHub Release PyPI Stats porder: Simple ordersv2 CLI Planet-GEE-Pipeline-CLI GEE Asset Manager Addons satadd: MultiSource CLI geeup: GEE Upload CLI gee2drive: GEE Download CLI pydrop: Digital Ocean CLI Planet BaseMap CLI","title":"Introduction"},{"location":"#introduction","text":"These github projects are a collection of tools and improvements made to accomplish tasks as well as potential tools and applications designed for future applications. Core capabilities include tools that interact with Google Earth Engine, Planet Labs API, Arctic DEM datasets and coupling large scale data pipelines using satadd projects. These projects extends tools and stand alone CLI(s) and GUI(s) for testing and implementation on various platforms. Some tools are designed to allow for easy intergation with cloud services such as Digital Ocean along with standalone hackathon tools and collaborations. Projects Include work on following Platforms Two projects included looking at different aspects of raster analysis and landscape modeling and hence I decided to create an ArcMap toolbox with tools that I have developed and continue to develop further. The toolbox can be downloaded and added to existing toolbox to create additional functionalities. The synthetic landscape models are designed to try different randomizing and clustering algorithms. An additional project was completed during the Polar Geospatial Bootcamp which allows users to bulk download and process 2m high resolution ArcticDEM provided by the Polar Geospatial Center. The list now also includes a notifier application using the Slack interface along with including an application of the Clips API from Planet. Recent Project Names Digital Object Identification Number(DOI) GitHub Release PyPI Stats porder: Simple ordersv2 CLI Planet-GEE-Pipeline-CLI GEE Asset Manager Addons satadd: MultiSource CLI geeup: GEE Upload CLI gee2drive: GEE Download CLI pydrop: Digital Ocean CLI Planet BaseMap CLI","title":"Introduction"},{"location":"aboutme/","text":"My current work involves big data analysis and large scale data science applications utilizing geospatial technology. I work with different imagery types and I am currently involved with looking at time series data & analysis of natural systems coupled with system modeling and periodicity. Most of my work includes but is not limited to remote sensing applications, large scale data processing and management, API support along with network analysis and geostatistical methods. I like solving puzzles and problems, building tools and plugins and working from the perspective of the end user with limited access. Contact me: samapriya.roy[at]gmail.com","title":"About me"},{"location":"credits/","text":"Credits \u00b6 JetStream A portion of the work is suported by JetStream Grant TG-GEO160014. ArcticDEM data provided by Polar Geospatial Center PGC Also supported by Planet Labs Ambassador Program","title":"Credits"},{"location":"credits/#credits","text":"JetStream A portion of the work is suported by JetStream Grant TG-GEO160014. ArcticDEM data provided by Polar Geospatial Center PGC Also supported by Planet Labs Ambassador Program","title":"Credits"},{"location":"cv/","text":"Samapriya Roy Github Projects : github . com / samapriya Github Site : samapriya . github . io Linkedin Profile : https : //www.linkedin.com/in/samapriya/ City : Peoria email : samapriya . roy @ gmail . com last updated : 2n d May 2021 Research Specialization and Interest \u00b6 Remote Sensing and GIS, Urban systems,patterns and hydrology, Land Change Science Education \u00b6 Degree University Year Research Interest or Thesis PhD Geography Indiana University 2019 Thesis: Delta Dynamics: Understanding Process, Pattern, and People Using Remote Sensing and Systems Analysis in Coastal Louisiana and Amazon River Delta MS Earth Sciences Indiana University 2013 Thesis: Remote sensing & GIS applications for drainage detection and Modeling in agricultural watersheds B.Tech Civil Engineering VNIT 2010 Thesis: Social Network Analysis for Mapping Segmented Growth in Urban Cities in India. Technical \u00b6 Extensive experience with Google Earth Engine, ESRI ArcGIS, ERDAS IMAGINE, ENVI. Intermediate experience with Javascript, Python, Shell Scripts Experience with QGIS, eCognition, MATLAB, Fragstats, Patch Analyst, SPSS, QUAL2Kw, SocNetV, Expert Choice 2000, GRASS, ILWIS. Employment \u00b6 October 2019- present: Customer and Researcher Engagement & Senior Solutions Engineer,Planet Labs End to end design of customer solutions and engagement while working towards overall goals and milestones. Responsibilities include but are not limited to i) championing Planet\u2019s customer products and offerings in-person and via online assistance, by presenting at conferences, writing technical tutorials, and publishing articles and videos. ii) Building tools, demos and sample applications, and continuously improving the applications that customers and developers experience using Planet\u2019s APIs and analytics endpoints. iii) Identifying strategic partnership opportunities and growing Planet\u2019s customer base and research community across multiple scales, domain expertise, and applications. Influencing the direction of Planet\u2019s developer products and offerings by gathering insights from customer engagement and the developer community. iv) interacting and engaging with users in Planet\u2019s Education and Research program to source novel customer applications and unique insights. December 2018- September 2019: Customer and Researcher Engagement and Solutions Engineer,Planet Labs Responsibilities include but are not limited to i) championing Planet\u2019s customer products and offerings in-person and via online assistance, by presenting at conferences, writing technical tutorials, and publishing articles and videos. ii) Building tools, demos and sample applications, and continuously improving the applications that customers and developers experience using Planet\u2019s APIs and analytics endpoints. iii) Identifying strategic partnership opportunities and growing Planet\u2019s customer base and research community across multiple scales, domain expertise, and applications. Influencing the direction of Planet\u2019s developer products and offerings by gathering insights from customer engagement and the developer community. iv) interacting and engaging with users in Planet\u2019s Education and Research program to source novel customer applications and unique insights. May 2018- September 2018: Senior Developer Advocate Intern, Planet Labs Responsibilites include Growing and supporting Planet\u2019s technical user communities and developing new analytical tools and tutorials. Teaching workshops and delivering conference talks to technologists in academic communities and to developers in the geospatial and cloud industries. Collaborating on remote sensing science, including primary research on the evolution, geomorphology, and long-term welfare of the world\u2019s coastal ecosystems. January 2018- May 2018: Developer Advocate Intern, Planet Labs Responsibilities include growing and supporting user communities for Planet\u2019s Developer Center and the Education and Research Program. Developing new analytical tools, tutorials, and workshops for technical users of Planet data and tools. 2017-2018: Research Assistant, Indiana University funded by National Science Foundation(NSF) Coastal SEES Collaborative Research: Changes in actual and perceived coastal flood risks due to river management strategies (NSF: 1426997). Partner-PI. National Science Foundation. Responsible for looking at land loss models and remote sensing application to coastal land loss. Includes model building and assessment along with hydrological model based vulnerability assessment of same area looking at landscape pattern and progress. 2015-2016: Research Assistant, Indiana University funded by Deltas Belmont Forum and National Science Foundation(NSF) Catalyzing action towards sustainability of deltaic systems with an integrated modeling framework for risk assessment (DELTAS: 1342946). Partner-PI. National Science Foundation, Belmont Forum. Partner PI [International collaborative network of 24 institutions] (2015-2016) Responsible for looking at physical models of vulnerability and risk assessment in deltaic areas and among populated regions within Brazil. Includes model building and assessment along with hydrological model based vulnerability assessment of same area. 2013-2014: Research Assistant , Indiana University funded by National Aeronautics and Space Administration(NASA) National Aeronautics and Space Administration(NASA: NNX11AF50G) (01/01/2013\u201312/31/2014) Carbon Dynamics, Land Cover Change, and Vulnerability of the World's Largest Coastal Mangrove Ecosystem to Climate Change, with Dr. Rinku Roy Chowdhury. Responsibilities includes review of existing datasets and literature. Evaluation of existing LULC classes using Landsat ETM+, and Geoeye classified raster datasets. Assessment of existing census and socio ecological data. 2013-2015: Research Assistant, Indiana University funded by National Science Foundation(NSF) National Science Foundation (NSF: 1065785) (07/01/2013\u201306/30/2015). Collaborative Research: Ecological Homogenization of Urban America, with Dr. Rinku Roy Chowdhury Responsibilities include analysis of land cover data at census block group and parcel levels using landscape metrics and analysis of land cover heterogeneity or homogeneity at both levels. This includes developing batch models for running large datasets and creating consistent data and analytical output for all 6 urban sites in the project. 2011-2012: Research Assistant, Indiana University Purdue University at Indinapolis funded by United States Department of Agriculture & Natural Resource Conservation(USDA & NRCS) United States Department of Agriculture & Natural Resource Conservation (USDA & NRCS: 68-52KY-11-058) project for Creating and Automating Potential Polygon location and Site Characterization for Upland Storage in Watersheds, using Arc Map. A CTI (Compound Topographic Index) based Toolbox was developed using Model Builder (\u00ae ESRI) as a part of project deliverables to automate location of these potential sites. (September 2011- August 2012) January 2011-June 2011: Research Assistant, Indian Institute of Technology (IIT) funder by Ministry of Environment and Forests Research Associate with Water Resource Management Group at IIT Kanpur, for the Ganga River Basin Management Plan (GRBMP), under Ministry of Environment and Forests Responsibilities: Included assessment of the hydrology of the Upper Ganga Basin and working on flow simulation and scenario development for flow calculation. Organization: Indian Institute of Technology (IIT), Kanpur, January to June 2011. June 2010- August 2010: Research Intern at Risk Management Solution India (RMSI) Research Intern at the Core Geospatial and Utilities(CGO) business Unit Responsibilities at Risk Management Solution in India (RMSI), Dehradun: Created a characteristic model called SMCPM (Scenario Modeling with Catchment Priority Model) which utilizes the SCN method accompanied with SMART and MAUT utilization for user based criterion input and priority output. 22 nd June to 21 st August 2010 \\newpage Peer-Reviewed Journal Articles \u00b6 Swetnam, Tyson L., Stephen R. Yool, Samapriya Roy, and Donald A. Falk. \"On the Use of Standardized Multi-Temporal Indices for Monitoring Disturbance and Ecosystem Moisture Stress across Multiple Earth Observation Systems in the Google Earth Engine.\" Remote Sensing 13, no. 8 (2021): 1448. link Roy, Samapriya, Scott M. Robeson, Alejandra C. Ortiz, and Douglas A. Edmonds. \"Spatial and temporal patterns of land loss in the Lower Mississippi River Delta from 1983 to 2016.\" Remote Sensing of Environment 250 (2020): 112046. link Valenza, J. M., D. A. Edmonds, T. Hwang, and S. Roy. \"Downstream changes in river avulsion style are related to channel morphology.\" Nature communications 11, no. 1 (2020): 1-8. link Donchyts, Gennadii, Dirk Eilander, Antonio Moreno-Rodenas, Maarten Pronk, Samapriya Roy, and Hessel Winsemius. On the applicability of ICESat-2 and off-nadir SkySAT satellite datasets for the estimation of water storage in medium and small reservoirs at the global scale. No. EGU2020-18116. Copernicus Meetings, 2020. Caldwell, R. L., Edmonds, D. A., Baumgardner, S., Paola, C., Roy, S., & Nienhuis, J. H. (2019). A global delta dataset and the environmental variables that predict delta formation on marine coastlines. Earth Surface Dynamics, 7(3), 773-787. link Ortiz, A. C., S. Roy, and D. A. Edmonds (2017), Land loss by pond expansion on the Mississippi River Delta Plain, Geophys. Res. Lett., 44, doi:10.1002/2017GL073079. link Mansur, A. V., Brondizio, E. S., Roy, S., Soares, P. P. D. M. A., & Newton, A. (2018). Adapting to urban challenges in the Amazon: flood risk and infrastructure deficiencies in Bel\u00e9m, Brazil. Regional Environmental Change, 18(5), 1411-1426. link Mansur, A. V., Brond\u00edzio, E. S., Roy, S., Hetrick, S., Vogt, N. D., & Newton, A. (2016). An assessment of urban vulnerability in the Amazon Delta and Estuary: a multi-criterion index of flood exposure, socio-economic conditions and infrastructure. Sustainability Science, 11(4), 625-643. link Roy, S., & Katpatal, Y. B. (2011). Cyclical Hierarchical Modeling for Water Quality Model\u2013Based DSS Module in an Urban River System. Journal of Environmental Engineering, 137(12), 1176-1184. link Open Source Tools & Products \u00b6 Samapriya Roy. (2018, October 6). samapriya/satadd: satadd: CLI pipeline for Planet, Satellogic, Google Earth Engine and Digital Globe Imagery (Version 0.0.3). Zenodo. http://doi.org/10.5281/zenodo.1450622 Samapriya Roy. (2018, October 4). samapriya/pygbdx: pygbdx: Simple CLI for GBDX (Version 0.0.2). Zenodo. http://doi.org/10.5281/zenodo.1445734 Samapriya Roy. (2018, September 22). samapriya/Planet-Mosaic-Quads-Download-CLI: Planet Mosaic Quads Download CLI (Version 0.0.4). Zenodo. http://doi.org/10.5281/zenodo.1432872 Samapriya Roy. (2018, August 12). samapriya/geeup: geeup: Simple CLI for Earth Engine Uploads (Version 0.0.3). Zenodo. http://doi.org/10.5281/zenodo.1344130 Samapriya Roy. (2018, July 31). samapriya/gee2drive: gee2drive: Google Earth Engine to Drive Export Manager (Version 0.0.5). Zenodo. http://doi.org/10.5281/zenodo.132447 Samapriya Roy. (2018, July 23). samapriya/pydrop: pydrop: Minimal Python Client for Digital Ocean Droplets (Version 0.0.2). Zenodo. http://doi.org/10.5281/zenodo.1319799 Google Earth Engine Account Transfer Tool,Samapriya Roy. (2018, February 27). samapriya/gee-takeout: Google Earth Engine Account Transfer Tool (Version 0.1). Zenodo. http://doi.org/10.5281/zenodo.1185158 Clip-Ship-Planet-CLI,Samapriya Roy. (2017, December 20). samapriya/Clip-Ship-Planet-CLI: Clip-Ship-Batch Planet Command Line Interface (Version 0.2.1). Zenodo. http://doi.org/10.5281/zenodo.1119192 Slack-Notifier-CLI-Addon,Samapriya Roy. (2017, September 5). samapriya/Slack-Notifier-CLI-Addon: Slack Notifier-CLI Addon (Version 0.1.0). Zenodo. http://doi.org/10.5281/zenodo.885505 Planet-GEE-Pipeline-CLI,Samapriya Roy. (2018, March 8). samapriya/Planet-GEE-Pipeline-CLI: Planet-GEE-Pipeline-CLI (Version 0.2.1). Zenodo. http://doi.org/10.5281/zenodo.1194323 Planet-GEE-Pipeline-GUI,Samapriya Roy. (2017, June 25). samapriya/Planet-GEE-Pipeline-GUI: Planet-GEE-Pipeline-GUI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.817739 Planet-Pipeline-GUI,Samapriya Roy. (2017, August 17). samapriya/Planet-Pipeline-GUI: Planet-Pipeline-GUI (Version 0.3). Zenodo. http://doi.org/10.5281/zenodo.844149 GEE Asset Manager Addons,Samapriya Roy. (2018, March 8). samapriya/gee_asset_manager_addon: GEE Asset Manager with Addons (Version 0.2.3). Zenodo. http://doi.org/10.5281/zenodo.1194308 ArcMap Addons,Samapriya Roy. (2017, October 12). samapriya/arcmap-addons: ArcMap Addons (Version 0.2). Zenodo. http://doi.org/10.5281/zenodo.1009210 ArcticDEM-Batch-Pipeline,Samapriya Roy. (2018, May 3). samapriya/ArcticDEM-Batch-Pipeline: ArcticDEM-Batch-Pipeline (Version 0.1.2). Zenodo. http://doi.org/10.5281/zenodo.1240456 Jetstream-Unofficial-addon,Samapriya Roy. (2018, March 12). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon (Version 0.1.2). Zenodo. http://doi.org/10.5281/zenodo.1196653 Planet-Batch-Slack-CLI,Samapriya Roy. (2017, December 4). samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.1079887 Articles, Blogposts and Publications \u00b6 Roy, Samapriya, \"Search, Batch, and Upload: Planet Basemaps to Descartes Labs\" , February 17, 2021, Read Here Roy, Samapriya, \"Community Datasets & Data Commons in Google Earth Engine\" , September 30, 2020, Read Here Roy, Samapriya, \"Cloud Filter the Cloud-native Way: Planet UDM2 in Google Earth Engine\" , May 11, 2020, Read Here Roy, Samapriya, \"Getting Git Right on Google Earth Engine\" , Feb 27, 2020, Read Here Roy, Samapriya, \"Community Datasets in Google Earth Engine: An experiment\" , February 21 st 2020, Read Here Roy, Samapriya, \"The Scoop on Planet Basemaps\" , Jan 9 th 2020, Read Here Roy, Samapriya, \"All about that Base-map\" , 7 th May 2019, Read Here Roy, Samapriya, \"Demystifying Planet\u2019s Command Line Tool\" , 20 th March 2019, Read Here Roy, Samapriya, \"Google Fusion Table Migration with & within Google Earth Engine\" , 28 th January 2019, Read Here Roy, Samapriya, \"porder for Planet ordersv2: Cheat Sheet\" , 19 th December 2018, Read Here Roy, Samapriya, \"Order Up: Using and Building with Planet \u2019s new Ordersv2 API\" , 26 th November 2018, Read Here Roy, Samapriya, \"From Analysis Ready Data to Analysis Engines and Everything in between\" , 7 th November 2018, Read Here Roy, Samapriya, \"Places, Planet & Earth Engine: Hacktober 2018\" , 17 th October 2018, Read Here Roy, Samapriya, \"Google Earth Engine Takeout: Tools and Guide for Code and Asset Transfer\" , 4 th December 2017, Read Here Roy, Samapriya, \"Talk Slack to Me: Integrating Planet and Slack API for Automation & Batch Notifications\" , 4 th December 2017, Read Here Roy, Samapriya, \"Baking API Clients in a Raspberry Pi: Planet and Earth Engine in a Box\" , 22 nd November 2017, Read Here Roy, Samapriya, \"Planet, People and Pixels: A Data Pipeline to link Planet API to Google Earth Engine\" , 10 July 2017, Read Here Roy, Samapriya, \"Clip and Ship: Batch Clips using Planet\u2019s Clips API\" , 15 September 2017, Read Here Roy, Samapriya, \"Google Earth Engine Asset Manager and Addons: Building Tools of the Trade\" , 19 October 2017, Read Here Invited Talks and Trainings \u00b6 American Geophysical Union Fall Meeting 2018:Invited Talk: Sensor Fusion and Analysis Ready Data with the Planet Platform (WS32) December 9 th 2018. Github Link Stanford Big Earth Hackathon October 6-November 30, 2018 Invited talk how to download data from Planet data API and integration for local analysis and analysis in Google Earth Engine. Github Link SatSummit September 19-20 2018: Hands-on Satellite Imagery Processing and Analysis at Scale . This was a hands-on introduction to Planet Data API and application within Google Earth Engine plus an introduction to Planet\u2019s open data products and offerings. Github Link Terra 2018 Pointcloud And Remote Sensing Workshop: Invited talk about Planet data, API mechanics and working with Planet Data inside Google Earth Engine . This was an online workshop along with Joseph Mascaro held at Ensenada, B.C. Github Link NEON Data Institute 2018: Remote Sensing with Reproducible Workflows using Python: Invited talk about Planet data, API mechanics and working with Google Earth Engine . This was an online talk as I joined remotely with participants. Github Link Stanford Big Earth Hackathon April 14-15 th 2018, Stanford University: Invited to coordinate use of Planet data and API mechanics to formulate and work on Earth Sciences and big data problems with students as Developer advocate intern. Github Link CSDMS 2018 Annual Meeting, May 22 -24 th 2018, Boulder Colorado, USA: Organized Clinic Introduction to Google Earth Engine Github Link American Geophysical Union Fall Meeting 2017: Invited Talk: Earth Science in Real Time with the Planet SmallSat Constellation. December 11-15 th 2017 Conference presentations, Trainings and Attendance \u00b6 Cyverse Container Camp 7 th to 9 th March: Container Technology for Scientific Research Introduction to Docker and Singularity images in High Performance Computing environments. American Geophysical Union Annual Meeting 2017(AGU 2017) Spatial and Temporal Patterns of Land Loss in Mississipi River Delta. December 11-15 th 2017. New Orleans, Louisiana Polar Geospatial BootCamp 2017: Focus on looking at high resolution satellite data along with using stereoscopic method to create high resolution digital elevation model. August 7-10 th 2017, Minneapolis, Minnesota Google Earth Engine Summit 2017: Focus on methodology and large scale data analysis. Mountain View, California. June 12-14 th 2017. Indiana Geographic Information Council Conference 2017. Deep Time Stack Analysis of Coastal Land loss: Case Study of Mississippi Delta using Earth Engine. May 9-11 th May 2017 Edmonds,Douglas A., Caldwell,Rebecca L., Baumgardner,Sarah, Paola,Chris, Roy ,Samapriya, Nelson,Amelia: A global analysis of human habitation on river deltas. European Geosciences Union General Assembly , Vienna, 23-28 th April 2017 Roy, Samapriya., Edmonds, Douglas., Visualizing land loss across high spatio-temporal scales across Louisiana Coast, Planet Labs Headquarters December 2016. Deltas Belmont Forum End of Project Meeting August 2016: Large Scale data analysis and visualization application to specific delta cases along with urban land cover change. Google Earth Engine Summit 2016: Focus on methodology and large scale data analysis. Mountain View, California. June 13-16 th 2016. Community Surface Dynamics Modeling System (CSDMS) Annual Meeting , A Composite Vulnerability for Urban Areas in Deltaic Regions: An application in the Amazon Delta, Colorado. 17-19 th May 2016. Indiana Geographic Information Council Conference 2016. Locally & Globally Applied Classification Algorithms for Urban Land Cover Detection using Earth Engine. May 9-11 th May 2016. Summer Training at the United States Army Core of Engineers (USACE) and South Florida Water Management District (SFWMD) joint Modeling Group (2015): South Florida Water Management Model, West Palm Beach, 26 th July to 31 st July 2015. Roy, Samapriya (2015) Hydrological Modeling for studying impacts of Urbanization: A cross site assessment at Community Surface Dynamics Modeling System (CSDMS) Annual Meeting , Colorado. 26-28nd May 2015 Roy, Samapriya et.al (2015) Urban Watersheds and urbanizing hydrology: Assessment through dynamic modeling, Oral presentation at the Association of American Geographers(AAG) Annual Meeting , April 21-April 25 th , 2015 Roy, Samapriya, Roy Chowdhury, Rinku and Ficklin, Darren (2015) Dynamic Hydrological Modeling using SWAT within Florida Coastal Everglades. Florida Coastal Everglades LTER Mid Term Review Meeting , March 11-13, 2015. Roy, Samapriya, Darren L. Ficklin, Rinku Roy Chowdhury, Scott Robeson, Jarlath O Neil Dunn, James B Heffernan, Meredith K Steele, Peter M Groffman (2014) Dynamic Assessment of Urban Hydrologic Component using SWAT. International Long Term Ecological Research (ILTER): All Scientist Meeting of the Americas , Valdivia, Chile, December 1-3 , 2014 Field Research Experience \u00b6 Summer 2014 (05/15/2014-06/18/2014). Field research in Bangladesh: Conducted 140 household surveys, with data collection aimed at assessing socio-economic-political drivers of land use and livelihood vulnerability in 4 villages of the Sundarbans mangrove forest region. Grants and Fellowship \u00b6 College of Arts and Sciences Graduate Student Travel Award for attending Google Earth Engine Summit 2017 $200 April 2017. Awarded the Graduate & Professional Student Government Travel Award for attending Google Earth Engine Summit 2017 $500 April 2017. Awarded the John Odland Graduate Research Fellowship to support graduate research $500 March 2017 Awarded the Lester Spicer Department of Geography Poster Award Fellowship to support graduate research $250 March 2017 Awarded the William R. Black Leadership Memorial Fellowship for $500 March 2017 Co-PI on Extreme Science and Engineering Discovery Environment (XSEDE) Allocation Grant along with Douglas Edmonds for 50,000 Super-computing Units and 500 GB storage volume for project \"Automated API based Pipeline for high volume satellite data\", grant number TG-GEO160014 at Indiana University. Co PI on the Planet Labs Ambassador program providing us unprecedented daily satellite data for entire Louisiana coast an estimated value of over 40,000 square kilometer and estimated access to over 500,000 square kilometer daily (approximately) maximum access value considering RapidEye is (approx. 500,000 x $1.28/sqkm = $640,000) Digital Globe Imagery Grant for an area of over 1500 square kilometer (estimated value at $25,500) April 2016. Awarded the John Odland Graduate Research Fellowship to support graduate research $500 March 2016. Awarded the IndianaView Student Scholarship Program to participate and conduct research $750 March 2016 Awarded the Partners of America (POA) 100,000 Strong award, for $1000 towards travel to the ILTER All Scientist Meeting of the Americas, Valdivia, Chile December 2014 Awarded the ILTER Travel Award, for $500 towards travel to the ILTER All Scientist Meeting of the Americas, Valdivia, Chile December 2014 Teaching Experience \u00b6 Lead Instructor, G338: Introduction to Geographic Information Systems, Summer 2017, Indiana University, Bloomington Lead Instructor, G237: Mapping our World, Spring 2017, Indiana University, Bloomington Lead Instructor, G237: Mapping our World, Fall 2016, Indiana University, Bloomington Teaching Assistant, G237: Mapping our World, Spring 2016, Indiana University, Bloomington Lead Instructor, G237: Mapping our World, Fall 2015, Indiana University, Bloomington Guest Lecture I202 Lecture Topic: Spatial Epidemiology September 25 th 2014 at Indiana University, Bloomington Teaching Assistant: Environmental Geology Course, G117 Spring 2013, Indiana University Purdue University, Indianapolis Teaching Assistant: Environmental Geology Course, G117 Fall 2012, Indiana University Purdue University, Indianapolis Teaching Assistant: Environmental Geology Course, G117 Spring 2012, Indiana University Purdue University, Indianapolis Memberships and committees \u00b6 Co-Chair for GIS Day Day at Bloomington, Indiana University, 2016 College Committee on Graduate Education, Graduate Student Representative (2015-2016) Planning Committee for GIS Day at Bloomington, Indiana University 2015. Student Member, American Society of Civil Engineers (ASCE) 2008-present Environmental and Water Resources Institute (EWRI) 2010-present Student Member, American Association of Geographer (AAG) 2014-present Certifications and Trainings \u00b6 Collaborative Institutional Training Initiative (CITI) Human Research 2014 Trimble Geospatial Training: eCognition- analysis strategies August 14 th - 15 th , 2014 References \u00b6 Available on request.","title":"Current CV"},{"location":"cv/#research-specialization-and-interest","text":"Remote Sensing and GIS, Urban systems,patterns and hydrology, Land Change Science","title":"Research Specialization and Interest"},{"location":"cv/#education","text":"Degree University Year Research Interest or Thesis PhD Geography Indiana University 2019 Thesis: Delta Dynamics: Understanding Process, Pattern, and People Using Remote Sensing and Systems Analysis in Coastal Louisiana and Amazon River Delta MS Earth Sciences Indiana University 2013 Thesis: Remote sensing & GIS applications for drainage detection and Modeling in agricultural watersheds B.Tech Civil Engineering VNIT 2010 Thesis: Social Network Analysis for Mapping Segmented Growth in Urban Cities in India.","title":"Education"},{"location":"cv/#technical","text":"Extensive experience with Google Earth Engine, ESRI ArcGIS, ERDAS IMAGINE, ENVI. Intermediate experience with Javascript, Python, Shell Scripts Experience with QGIS, eCognition, MATLAB, Fragstats, Patch Analyst, SPSS, QUAL2Kw, SocNetV, Expert Choice 2000, GRASS, ILWIS.","title":"Technical"},{"location":"cv/#employment","text":"October 2019- present: Customer and Researcher Engagement & Senior Solutions Engineer,Planet Labs End to end design of customer solutions and engagement while working towards overall goals and milestones. Responsibilities include but are not limited to i) championing Planet\u2019s customer products and offerings in-person and via online assistance, by presenting at conferences, writing technical tutorials, and publishing articles and videos. ii) Building tools, demos and sample applications, and continuously improving the applications that customers and developers experience using Planet\u2019s APIs and analytics endpoints. iii) Identifying strategic partnership opportunities and growing Planet\u2019s customer base and research community across multiple scales, domain expertise, and applications. Influencing the direction of Planet\u2019s developer products and offerings by gathering insights from customer engagement and the developer community. iv) interacting and engaging with users in Planet\u2019s Education and Research program to source novel customer applications and unique insights. December 2018- September 2019: Customer and Researcher Engagement and Solutions Engineer,Planet Labs Responsibilities include but are not limited to i) championing Planet\u2019s customer products and offerings in-person and via online assistance, by presenting at conferences, writing technical tutorials, and publishing articles and videos. ii) Building tools, demos and sample applications, and continuously improving the applications that customers and developers experience using Planet\u2019s APIs and analytics endpoints. iii) Identifying strategic partnership opportunities and growing Planet\u2019s customer base and research community across multiple scales, domain expertise, and applications. Influencing the direction of Planet\u2019s developer products and offerings by gathering insights from customer engagement and the developer community. iv) interacting and engaging with users in Planet\u2019s Education and Research program to source novel customer applications and unique insights. May 2018- September 2018: Senior Developer Advocate Intern, Planet Labs Responsibilites include Growing and supporting Planet\u2019s technical user communities and developing new analytical tools and tutorials. Teaching workshops and delivering conference talks to technologists in academic communities and to developers in the geospatial and cloud industries. Collaborating on remote sensing science, including primary research on the evolution, geomorphology, and long-term welfare of the world\u2019s coastal ecosystems. January 2018- May 2018: Developer Advocate Intern, Planet Labs Responsibilities include growing and supporting user communities for Planet\u2019s Developer Center and the Education and Research Program. Developing new analytical tools, tutorials, and workshops for technical users of Planet data and tools. 2017-2018: Research Assistant, Indiana University funded by National Science Foundation(NSF) Coastal SEES Collaborative Research: Changes in actual and perceived coastal flood risks due to river management strategies (NSF: 1426997). Partner-PI. National Science Foundation. Responsible for looking at land loss models and remote sensing application to coastal land loss. Includes model building and assessment along with hydrological model based vulnerability assessment of same area looking at landscape pattern and progress. 2015-2016: Research Assistant, Indiana University funded by Deltas Belmont Forum and National Science Foundation(NSF) Catalyzing action towards sustainability of deltaic systems with an integrated modeling framework for risk assessment (DELTAS: 1342946). Partner-PI. National Science Foundation, Belmont Forum. Partner PI [International collaborative network of 24 institutions] (2015-2016) Responsible for looking at physical models of vulnerability and risk assessment in deltaic areas and among populated regions within Brazil. Includes model building and assessment along with hydrological model based vulnerability assessment of same area. 2013-2014: Research Assistant , Indiana University funded by National Aeronautics and Space Administration(NASA) National Aeronautics and Space Administration(NASA: NNX11AF50G) (01/01/2013\u201312/31/2014) Carbon Dynamics, Land Cover Change, and Vulnerability of the World's Largest Coastal Mangrove Ecosystem to Climate Change, with Dr. Rinku Roy Chowdhury. Responsibilities includes review of existing datasets and literature. Evaluation of existing LULC classes using Landsat ETM+, and Geoeye classified raster datasets. Assessment of existing census and socio ecological data. 2013-2015: Research Assistant, Indiana University funded by National Science Foundation(NSF) National Science Foundation (NSF: 1065785) (07/01/2013\u201306/30/2015). Collaborative Research: Ecological Homogenization of Urban America, with Dr. Rinku Roy Chowdhury Responsibilities include analysis of land cover data at census block group and parcel levels using landscape metrics and analysis of land cover heterogeneity or homogeneity at both levels. This includes developing batch models for running large datasets and creating consistent data and analytical output for all 6 urban sites in the project. 2011-2012: Research Assistant, Indiana University Purdue University at Indinapolis funded by United States Department of Agriculture & Natural Resource Conservation(USDA & NRCS) United States Department of Agriculture & Natural Resource Conservation (USDA & NRCS: 68-52KY-11-058) project for Creating and Automating Potential Polygon location and Site Characterization for Upland Storage in Watersheds, using Arc Map. A CTI (Compound Topographic Index) based Toolbox was developed using Model Builder (\u00ae ESRI) as a part of project deliverables to automate location of these potential sites. (September 2011- August 2012) January 2011-June 2011: Research Assistant, Indian Institute of Technology (IIT) funder by Ministry of Environment and Forests Research Associate with Water Resource Management Group at IIT Kanpur, for the Ganga River Basin Management Plan (GRBMP), under Ministry of Environment and Forests Responsibilities: Included assessment of the hydrology of the Upper Ganga Basin and working on flow simulation and scenario development for flow calculation. Organization: Indian Institute of Technology (IIT), Kanpur, January to June 2011. June 2010- August 2010: Research Intern at Risk Management Solution India (RMSI) Research Intern at the Core Geospatial and Utilities(CGO) business Unit Responsibilities at Risk Management Solution in India (RMSI), Dehradun: Created a characteristic model called SMCPM (Scenario Modeling with Catchment Priority Model) which utilizes the SCN method accompanied with SMART and MAUT utilization for user based criterion input and priority output. 22 nd June to 21 st August 2010 \\newpage","title":"Employment"},{"location":"cv/#peer-reviewed-journal-articles","text":"Swetnam, Tyson L., Stephen R. Yool, Samapriya Roy, and Donald A. Falk. \"On the Use of Standardized Multi-Temporal Indices for Monitoring Disturbance and Ecosystem Moisture Stress across Multiple Earth Observation Systems in the Google Earth Engine.\" Remote Sensing 13, no. 8 (2021): 1448. link Roy, Samapriya, Scott M. Robeson, Alejandra C. Ortiz, and Douglas A. Edmonds. \"Spatial and temporal patterns of land loss in the Lower Mississippi River Delta from 1983 to 2016.\" Remote Sensing of Environment 250 (2020): 112046. link Valenza, J. M., D. A. Edmonds, T. Hwang, and S. Roy. \"Downstream changes in river avulsion style are related to channel morphology.\" Nature communications 11, no. 1 (2020): 1-8. link Donchyts, Gennadii, Dirk Eilander, Antonio Moreno-Rodenas, Maarten Pronk, Samapriya Roy, and Hessel Winsemius. On the applicability of ICESat-2 and off-nadir SkySAT satellite datasets for the estimation of water storage in medium and small reservoirs at the global scale. No. EGU2020-18116. Copernicus Meetings, 2020. Caldwell, R. L., Edmonds, D. A., Baumgardner, S., Paola, C., Roy, S., & Nienhuis, J. H. (2019). A global delta dataset and the environmental variables that predict delta formation on marine coastlines. Earth Surface Dynamics, 7(3), 773-787. link Ortiz, A. C., S. Roy, and D. A. Edmonds (2017), Land loss by pond expansion on the Mississippi River Delta Plain, Geophys. Res. Lett., 44, doi:10.1002/2017GL073079. link Mansur, A. V., Brondizio, E. S., Roy, S., Soares, P. P. D. M. A., & Newton, A. (2018). Adapting to urban challenges in the Amazon: flood risk and infrastructure deficiencies in Bel\u00e9m, Brazil. Regional Environmental Change, 18(5), 1411-1426. link Mansur, A. V., Brond\u00edzio, E. S., Roy, S., Hetrick, S., Vogt, N. D., & Newton, A. (2016). An assessment of urban vulnerability in the Amazon Delta and Estuary: a multi-criterion index of flood exposure, socio-economic conditions and infrastructure. Sustainability Science, 11(4), 625-643. link Roy, S., & Katpatal, Y. B. (2011). Cyclical Hierarchical Modeling for Water Quality Model\u2013Based DSS Module in an Urban River System. Journal of Environmental Engineering, 137(12), 1176-1184. link","title":"Peer-Reviewed Journal Articles"},{"location":"cv/#open-source-tools-products","text":"Samapriya Roy. (2018, October 6). samapriya/satadd: satadd: CLI pipeline for Planet, Satellogic, Google Earth Engine and Digital Globe Imagery (Version 0.0.3). Zenodo. http://doi.org/10.5281/zenodo.1450622 Samapriya Roy. (2018, October 4). samapriya/pygbdx: pygbdx: Simple CLI for GBDX (Version 0.0.2). Zenodo. http://doi.org/10.5281/zenodo.1445734 Samapriya Roy. (2018, September 22). samapriya/Planet-Mosaic-Quads-Download-CLI: Planet Mosaic Quads Download CLI (Version 0.0.4). Zenodo. http://doi.org/10.5281/zenodo.1432872 Samapriya Roy. (2018, August 12). samapriya/geeup: geeup: Simple CLI for Earth Engine Uploads (Version 0.0.3). Zenodo. http://doi.org/10.5281/zenodo.1344130 Samapriya Roy. (2018, July 31). samapriya/gee2drive: gee2drive: Google Earth Engine to Drive Export Manager (Version 0.0.5). Zenodo. http://doi.org/10.5281/zenodo.132447 Samapriya Roy. (2018, July 23). samapriya/pydrop: pydrop: Minimal Python Client for Digital Ocean Droplets (Version 0.0.2). Zenodo. http://doi.org/10.5281/zenodo.1319799 Google Earth Engine Account Transfer Tool,Samapriya Roy. (2018, February 27). samapriya/gee-takeout: Google Earth Engine Account Transfer Tool (Version 0.1). Zenodo. http://doi.org/10.5281/zenodo.1185158 Clip-Ship-Planet-CLI,Samapriya Roy. (2017, December 20). samapriya/Clip-Ship-Planet-CLI: Clip-Ship-Batch Planet Command Line Interface (Version 0.2.1). Zenodo. http://doi.org/10.5281/zenodo.1119192 Slack-Notifier-CLI-Addon,Samapriya Roy. (2017, September 5). samapriya/Slack-Notifier-CLI-Addon: Slack Notifier-CLI Addon (Version 0.1.0). Zenodo. http://doi.org/10.5281/zenodo.885505 Planet-GEE-Pipeline-CLI,Samapriya Roy. (2018, March 8). samapriya/Planet-GEE-Pipeline-CLI: Planet-GEE-Pipeline-CLI (Version 0.2.1). Zenodo. http://doi.org/10.5281/zenodo.1194323 Planet-GEE-Pipeline-GUI,Samapriya Roy. (2017, June 25). samapriya/Planet-GEE-Pipeline-GUI: Planet-GEE-Pipeline-GUI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.817739 Planet-Pipeline-GUI,Samapriya Roy. (2017, August 17). samapriya/Planet-Pipeline-GUI: Planet-Pipeline-GUI (Version 0.3). Zenodo. http://doi.org/10.5281/zenodo.844149 GEE Asset Manager Addons,Samapriya Roy. (2018, March 8). samapriya/gee_asset_manager_addon: GEE Asset Manager with Addons (Version 0.2.3). Zenodo. http://doi.org/10.5281/zenodo.1194308 ArcMap Addons,Samapriya Roy. (2017, October 12). samapriya/arcmap-addons: ArcMap Addons (Version 0.2). Zenodo. http://doi.org/10.5281/zenodo.1009210 ArcticDEM-Batch-Pipeline,Samapriya Roy. (2018, May 3). samapriya/ArcticDEM-Batch-Pipeline: ArcticDEM-Batch-Pipeline (Version 0.1.2). Zenodo. http://doi.org/10.5281/zenodo.1240456 Jetstream-Unofficial-addon,Samapriya Roy. (2018, March 12). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon (Version 0.1.2). Zenodo. http://doi.org/10.5281/zenodo.1196653 Planet-Batch-Slack-CLI,Samapriya Roy. (2017, December 4). samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.1079887","title":"Open Source Tools &amp; Products"},{"location":"cv/#articles-blogposts-and-publications","text":"Roy, Samapriya, \"Search, Batch, and Upload: Planet Basemaps to Descartes Labs\" , February 17, 2021, Read Here Roy, Samapriya, \"Community Datasets & Data Commons in Google Earth Engine\" , September 30, 2020, Read Here Roy, Samapriya, \"Cloud Filter the Cloud-native Way: Planet UDM2 in Google Earth Engine\" , May 11, 2020, Read Here Roy, Samapriya, \"Getting Git Right on Google Earth Engine\" , Feb 27, 2020, Read Here Roy, Samapriya, \"Community Datasets in Google Earth Engine: An experiment\" , February 21 st 2020, Read Here Roy, Samapriya, \"The Scoop on Planet Basemaps\" , Jan 9 th 2020, Read Here Roy, Samapriya, \"All about that Base-map\" , 7 th May 2019, Read Here Roy, Samapriya, \"Demystifying Planet\u2019s Command Line Tool\" , 20 th March 2019, Read Here Roy, Samapriya, \"Google Fusion Table Migration with & within Google Earth Engine\" , 28 th January 2019, Read Here Roy, Samapriya, \"porder for Planet ordersv2: Cheat Sheet\" , 19 th December 2018, Read Here Roy, Samapriya, \"Order Up: Using and Building with Planet \u2019s new Ordersv2 API\" , 26 th November 2018, Read Here Roy, Samapriya, \"From Analysis Ready Data to Analysis Engines and Everything in between\" , 7 th November 2018, Read Here Roy, Samapriya, \"Places, Planet & Earth Engine: Hacktober 2018\" , 17 th October 2018, Read Here Roy, Samapriya, \"Google Earth Engine Takeout: Tools and Guide for Code and Asset Transfer\" , 4 th December 2017, Read Here Roy, Samapriya, \"Talk Slack to Me: Integrating Planet and Slack API for Automation & Batch Notifications\" , 4 th December 2017, Read Here Roy, Samapriya, \"Baking API Clients in a Raspberry Pi: Planet and Earth Engine in a Box\" , 22 nd November 2017, Read Here Roy, Samapriya, \"Planet, People and Pixels: A Data Pipeline to link Planet API to Google Earth Engine\" , 10 July 2017, Read Here Roy, Samapriya, \"Clip and Ship: Batch Clips using Planet\u2019s Clips API\" , 15 September 2017, Read Here Roy, Samapriya, \"Google Earth Engine Asset Manager and Addons: Building Tools of the Trade\" , 19 October 2017, Read Here","title":"Articles, Blogposts and Publications"},{"location":"cv/#invited-talks-and-trainings","text":"American Geophysical Union Fall Meeting 2018:Invited Talk: Sensor Fusion and Analysis Ready Data with the Planet Platform (WS32) December 9 th 2018. Github Link Stanford Big Earth Hackathon October 6-November 30, 2018 Invited talk how to download data from Planet data API and integration for local analysis and analysis in Google Earth Engine. Github Link SatSummit September 19-20 2018: Hands-on Satellite Imagery Processing and Analysis at Scale . This was a hands-on introduction to Planet Data API and application within Google Earth Engine plus an introduction to Planet\u2019s open data products and offerings. Github Link Terra 2018 Pointcloud And Remote Sensing Workshop: Invited talk about Planet data, API mechanics and working with Planet Data inside Google Earth Engine . This was an online workshop along with Joseph Mascaro held at Ensenada, B.C. Github Link NEON Data Institute 2018: Remote Sensing with Reproducible Workflows using Python: Invited talk about Planet data, API mechanics and working with Google Earth Engine . This was an online talk as I joined remotely with participants. Github Link Stanford Big Earth Hackathon April 14-15 th 2018, Stanford University: Invited to coordinate use of Planet data and API mechanics to formulate and work on Earth Sciences and big data problems with students as Developer advocate intern. Github Link CSDMS 2018 Annual Meeting, May 22 -24 th 2018, Boulder Colorado, USA: Organized Clinic Introduction to Google Earth Engine Github Link American Geophysical Union Fall Meeting 2017: Invited Talk: Earth Science in Real Time with the Planet SmallSat Constellation. December 11-15 th 2017","title":"Invited Talks and Trainings"},{"location":"cv/#conference-presentations-trainings-and-attendance","text":"Cyverse Container Camp 7 th to 9 th March: Container Technology for Scientific Research Introduction to Docker and Singularity images in High Performance Computing environments. American Geophysical Union Annual Meeting 2017(AGU 2017) Spatial and Temporal Patterns of Land Loss in Mississipi River Delta. December 11-15 th 2017. New Orleans, Louisiana Polar Geospatial BootCamp 2017: Focus on looking at high resolution satellite data along with using stereoscopic method to create high resolution digital elevation model. August 7-10 th 2017, Minneapolis, Minnesota Google Earth Engine Summit 2017: Focus on methodology and large scale data analysis. Mountain View, California. June 12-14 th 2017. Indiana Geographic Information Council Conference 2017. Deep Time Stack Analysis of Coastal Land loss: Case Study of Mississippi Delta using Earth Engine. May 9-11 th May 2017 Edmonds,Douglas A., Caldwell,Rebecca L., Baumgardner,Sarah, Paola,Chris, Roy ,Samapriya, Nelson,Amelia: A global analysis of human habitation on river deltas. European Geosciences Union General Assembly , Vienna, 23-28 th April 2017 Roy, Samapriya., Edmonds, Douglas., Visualizing land loss across high spatio-temporal scales across Louisiana Coast, Planet Labs Headquarters December 2016. Deltas Belmont Forum End of Project Meeting August 2016: Large Scale data analysis and visualization application to specific delta cases along with urban land cover change. Google Earth Engine Summit 2016: Focus on methodology and large scale data analysis. Mountain View, California. June 13-16 th 2016. Community Surface Dynamics Modeling System (CSDMS) Annual Meeting , A Composite Vulnerability for Urban Areas in Deltaic Regions: An application in the Amazon Delta, Colorado. 17-19 th May 2016. Indiana Geographic Information Council Conference 2016. Locally & Globally Applied Classification Algorithms for Urban Land Cover Detection using Earth Engine. May 9-11 th May 2016. Summer Training at the United States Army Core of Engineers (USACE) and South Florida Water Management District (SFWMD) joint Modeling Group (2015): South Florida Water Management Model, West Palm Beach, 26 th July to 31 st July 2015. Roy, Samapriya (2015) Hydrological Modeling for studying impacts of Urbanization: A cross site assessment at Community Surface Dynamics Modeling System (CSDMS) Annual Meeting , Colorado. 26-28nd May 2015 Roy, Samapriya et.al (2015) Urban Watersheds and urbanizing hydrology: Assessment through dynamic modeling, Oral presentation at the Association of American Geographers(AAG) Annual Meeting , April 21-April 25 th , 2015 Roy, Samapriya, Roy Chowdhury, Rinku and Ficklin, Darren (2015) Dynamic Hydrological Modeling using SWAT within Florida Coastal Everglades. Florida Coastal Everglades LTER Mid Term Review Meeting , March 11-13, 2015. Roy, Samapriya, Darren L. Ficklin, Rinku Roy Chowdhury, Scott Robeson, Jarlath O Neil Dunn, James B Heffernan, Meredith K Steele, Peter M Groffman (2014) Dynamic Assessment of Urban Hydrologic Component using SWAT. International Long Term Ecological Research (ILTER): All Scientist Meeting of the Americas , Valdivia, Chile, December 1-3 , 2014","title":"Conference presentations, Trainings and Attendance"},{"location":"cv/#field-research-experience","text":"Summer 2014 (05/15/2014-06/18/2014). Field research in Bangladesh: Conducted 140 household surveys, with data collection aimed at assessing socio-economic-political drivers of land use and livelihood vulnerability in 4 villages of the Sundarbans mangrove forest region.","title":"Field Research Experience"},{"location":"cv/#grants-and-fellowship","text":"College of Arts and Sciences Graduate Student Travel Award for attending Google Earth Engine Summit 2017 $200 April 2017. Awarded the Graduate & Professional Student Government Travel Award for attending Google Earth Engine Summit 2017 $500 April 2017. Awarded the John Odland Graduate Research Fellowship to support graduate research $500 March 2017 Awarded the Lester Spicer Department of Geography Poster Award Fellowship to support graduate research $250 March 2017 Awarded the William R. Black Leadership Memorial Fellowship for $500 March 2017 Co-PI on Extreme Science and Engineering Discovery Environment (XSEDE) Allocation Grant along with Douglas Edmonds for 50,000 Super-computing Units and 500 GB storage volume for project \"Automated API based Pipeline for high volume satellite data\", grant number TG-GEO160014 at Indiana University. Co PI on the Planet Labs Ambassador program providing us unprecedented daily satellite data for entire Louisiana coast an estimated value of over 40,000 square kilometer and estimated access to over 500,000 square kilometer daily (approximately) maximum access value considering RapidEye is (approx. 500,000 x $1.28/sqkm = $640,000) Digital Globe Imagery Grant for an area of over 1500 square kilometer (estimated value at $25,500) April 2016. Awarded the John Odland Graduate Research Fellowship to support graduate research $500 March 2016. Awarded the IndianaView Student Scholarship Program to participate and conduct research $750 March 2016 Awarded the Partners of America (POA) 100,000 Strong award, for $1000 towards travel to the ILTER All Scientist Meeting of the Americas, Valdivia, Chile December 2014 Awarded the ILTER Travel Award, for $500 towards travel to the ILTER All Scientist Meeting of the Americas, Valdivia, Chile December 2014","title":"Grants and Fellowship"},{"location":"cv/#teaching-experience","text":"Lead Instructor, G338: Introduction to Geographic Information Systems, Summer 2017, Indiana University, Bloomington Lead Instructor, G237: Mapping our World, Spring 2017, Indiana University, Bloomington Lead Instructor, G237: Mapping our World, Fall 2016, Indiana University, Bloomington Teaching Assistant, G237: Mapping our World, Spring 2016, Indiana University, Bloomington Lead Instructor, G237: Mapping our World, Fall 2015, Indiana University, Bloomington Guest Lecture I202 Lecture Topic: Spatial Epidemiology September 25 th 2014 at Indiana University, Bloomington Teaching Assistant: Environmental Geology Course, G117 Spring 2013, Indiana University Purdue University, Indianapolis Teaching Assistant: Environmental Geology Course, G117 Fall 2012, Indiana University Purdue University, Indianapolis Teaching Assistant: Environmental Geology Course, G117 Spring 2012, Indiana University Purdue University, Indianapolis","title":"Teaching Experience"},{"location":"cv/#memberships-and-committees","text":"Co-Chair for GIS Day Day at Bloomington, Indiana University, 2016 College Committee on Graduate Education, Graduate Student Representative (2015-2016) Planning Committee for GIS Day at Bloomington, Indiana University 2015. Student Member, American Society of Civil Engineers (ASCE) 2008-present Environmental and Water Resources Institute (EWRI) 2010-present Student Member, American Association of Geographer (AAG) 2014-present","title":"Memberships and committees"},{"location":"cv/#certifications-and-trainings","text":"Collaborative Institutional Training Initiative (CITI) Human Research 2014 Trimble Geospatial Training: eCognition- analysis strategies August 14 th - 15 th , 2014","title":"Certifications and Trainings"},{"location":"cv/#references","text":"Available on request.","title":"References"},{"location":"porder/","text":"","title":"Porder"},{"location":"project-citations/","text":"Project Citations \u00b6 porder: Simple CLI for Planet ordersV2 API \"Samapriya Roy. (2019, October 24). samapriya/porder: porder: Simple CLI for Planet ordersV2 API (Version 0.5.2). Zenodo. http://doi.org/10.5281/zenodo.3518295\" neon-science-summit-2019 \"Samapriya Roy. (2019, October 14). samapriya/neon-science-summit-2019: neon-science-summit-2019 (Version 0.1). Zenodo. http://doi.org/10.5281/zenodo.3484160\" Planet-GEE-Pipeline-CLI \"Samapriya Roy. (2019, October 10). samapriya/Planet-GEE-Pipeline-CLI: Planet-GEE-Pipeline-CLI (Version 0.4.8). Zenodo. http://doi.org/10.5281/zenodo.3479525\" GEE Asset Manager Addons \"Samapriya Roy. (2019, October 6). samapriya/gee_asset_manager_addon: GEE Asset Manager with Addons (Version 0.3.3). Zenodo. http://doi.org/10.5281/zenodo.3474295\" geeup: GEE Upload CLI \"Samapriya Roy. (2019, October 4). samapriya/geeup: geeup: Simple CLI for Earth Engine Uploads (Version 0.3.1). Zenodo. http://doi.org/10.5281/zenodo.3473534\" pobatch: porder wrapper for Ordersv2 Batch Client \"Samapriya Roy. (2019, August 26). samapriya/pobatch: pobatch: porder wrapper for Ordersv2 Batch Client (Version 0.0.7). Zenodo. http://doi.org/10.5281/zenodo.3376975\" pbasemap: Planet Mosaic Quads Download CLI \"Samapriya Roy. (2019, June 25). samapriya/Planet-Mosaic-Quads-Download-CLI: Planet Mosaic Quads Download CLI (Version 0.1.0). Zenodo. http://doi.org/10.5281/zenodo.3255274\" gee2drive: GEE Download CLI \"Samapriya Roy. (2018, August 25). samapriya/gee2drive: gee2drive: Google Earth Engine to Drive Export Manager (Version 0.0.7). Zenodo. http://doi.org/10.5281/zenodo.1403657\" pydrop: Digital Ocean CLI \"Samapriya Roy. (2018, July 29). samapriya/pydrop: pydrop: Minimal Python Client for Digital Ocean Droplets (Version 0.0.3). Zenodo. http://doi.org/10.5281/zenodo.1323340\" Google Earth Engine Account Transfer Tool \"Samapriya Roy. (2018, February 27). samapriya/gee-takeout: Google Earth Engine Account Transfer Tool (Version 0.1). Zenodo. http://doi.org/10.5281/zenodo.1185158\" Clip-Ship-Planet-CLI \"Samapriya Roy. (2018, June 30). samapriya/Clip-Ship-Planet-CLI: Clip-Ship-Batch Planet Command Line Interface (Version 0.2.5). Zenodo. http://doi.org/10.5281/zenodo.1302068\" Slack-Notifier-CLI-Addon \"Samapriya Roy. (2017, September 5). samapriya/Slack-Notifier-CLI-Addon: Slack Notifier-CLI Addon (Version 0.1.0). Zenodo. http://doi.org/10.5281/zenodo.885505\" Planet-GEE-Pipeline-GUI \"Samapriya Roy. (2017, June 25). samapriya/Planet-GEE-Pipeline-GUI: Planet-GEE-Pipeline-GUI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.817739\" Planet-Pipeline-GUI \"Samapriya Roy. (2017, August 17). samapriya/Planet-Pipeline-GUI: Planet-Pipeline-GUI (Version 0.3). Zenodo. http://doi.org/10.5281/zenodo.844149\" ArcMap Addons \"Samapriya Roy. (2017, October 12). samapriya/arcmap-addons: ArcMap Addons (Version 0.2). Zenodo. http://doi.org/10.5281/zenodo.1009210\" ArcticDEM-Batch-Pipeline,\"Samapriya Roy. (2018, May 3). samapriya/ArcticDEM-Batch-Pipeline: ArcticDEM-Batch-Pipeline (Version 0.1.2). Zenodo. http://doi.org/10.5281/zenodo.1240456\" Jetstream-Unofficial-addon \"Samapriya Roy. (2018, March 12). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon (Version 0.1.2). Zenodo. http://doi.org/10.5281/zenodo.1196653\" Planet-Batch-Slack-CLI \"Samapriya Roy. (2017, December 4). samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.1079887\" satadd: CLI pipeline for Planet, Satellogic, Google Earth Engine and Digital Globe Imagery \"Samapriya Roy. (2018, October 6). samapriya/satadd: satadd: CLI pipeline for Planet, Satellogic, Google Earth Engine and Digital Globe Imagery (Version 0.0.3). Zenodo. http://doi.org/10.5281/zenodo.1450622\" pygbdx: Simple CLI for Digital Globe GBDX \"Samapriya Roy. (2018, October 4). samapriya/pygbdx: pygbdx: Simple CLI for GBDX (Version 0.0.2). Zenodo. http://doi.org/10.5281/zenodo.1445734\"","title":"Project Citations"},{"location":"project-citations/#project-citations","text":"porder: Simple CLI for Planet ordersV2 API \"Samapriya Roy. (2019, October 24). samapriya/porder: porder: Simple CLI for Planet ordersV2 API (Version 0.5.2). Zenodo. http://doi.org/10.5281/zenodo.3518295\" neon-science-summit-2019 \"Samapriya Roy. (2019, October 14). samapriya/neon-science-summit-2019: neon-science-summit-2019 (Version 0.1). Zenodo. http://doi.org/10.5281/zenodo.3484160\" Planet-GEE-Pipeline-CLI \"Samapriya Roy. (2019, October 10). samapriya/Planet-GEE-Pipeline-CLI: Planet-GEE-Pipeline-CLI (Version 0.4.8). Zenodo. http://doi.org/10.5281/zenodo.3479525\" GEE Asset Manager Addons \"Samapriya Roy. (2019, October 6). samapriya/gee_asset_manager_addon: GEE Asset Manager with Addons (Version 0.3.3). Zenodo. http://doi.org/10.5281/zenodo.3474295\" geeup: GEE Upload CLI \"Samapriya Roy. (2019, October 4). samapriya/geeup: geeup: Simple CLI for Earth Engine Uploads (Version 0.3.1). Zenodo. http://doi.org/10.5281/zenodo.3473534\" pobatch: porder wrapper for Ordersv2 Batch Client \"Samapriya Roy. (2019, August 26). samapriya/pobatch: pobatch: porder wrapper for Ordersv2 Batch Client (Version 0.0.7). Zenodo. http://doi.org/10.5281/zenodo.3376975\" pbasemap: Planet Mosaic Quads Download CLI \"Samapriya Roy. (2019, June 25). samapriya/Planet-Mosaic-Quads-Download-CLI: Planet Mosaic Quads Download CLI (Version 0.1.0). Zenodo. http://doi.org/10.5281/zenodo.3255274\" gee2drive: GEE Download CLI \"Samapriya Roy. (2018, August 25). samapriya/gee2drive: gee2drive: Google Earth Engine to Drive Export Manager (Version 0.0.7). Zenodo. http://doi.org/10.5281/zenodo.1403657\" pydrop: Digital Ocean CLI \"Samapriya Roy. (2018, July 29). samapriya/pydrop: pydrop: Minimal Python Client for Digital Ocean Droplets (Version 0.0.3). Zenodo. http://doi.org/10.5281/zenodo.1323340\" Google Earth Engine Account Transfer Tool \"Samapriya Roy. (2018, February 27). samapriya/gee-takeout: Google Earth Engine Account Transfer Tool (Version 0.1). Zenodo. http://doi.org/10.5281/zenodo.1185158\" Clip-Ship-Planet-CLI \"Samapriya Roy. (2018, June 30). samapriya/Clip-Ship-Planet-CLI: Clip-Ship-Batch Planet Command Line Interface (Version 0.2.5). Zenodo. http://doi.org/10.5281/zenodo.1302068\" Slack-Notifier-CLI-Addon \"Samapriya Roy. (2017, September 5). samapriya/Slack-Notifier-CLI-Addon: Slack Notifier-CLI Addon (Version 0.1.0). Zenodo. http://doi.org/10.5281/zenodo.885505\" Planet-GEE-Pipeline-GUI \"Samapriya Roy. (2017, June 25). samapriya/Planet-GEE-Pipeline-GUI: Planet-GEE-Pipeline-GUI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.817739\" Planet-Pipeline-GUI \"Samapriya Roy. (2017, August 17). samapriya/Planet-Pipeline-GUI: Planet-Pipeline-GUI (Version 0.3). Zenodo. http://doi.org/10.5281/zenodo.844149\" ArcMap Addons \"Samapriya Roy. (2017, October 12). samapriya/arcmap-addons: ArcMap Addons (Version 0.2). Zenodo. http://doi.org/10.5281/zenodo.1009210\" ArcticDEM-Batch-Pipeline,\"Samapriya Roy. (2018, May 3). samapriya/ArcticDEM-Batch-Pipeline: ArcticDEM-Batch-Pipeline (Version 0.1.2). Zenodo. http://doi.org/10.5281/zenodo.1240456\" Jetstream-Unofficial-addon \"Samapriya Roy. (2018, March 12). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon (Version 0.1.2). Zenodo. http://doi.org/10.5281/zenodo.1196653\" Planet-Batch-Slack-CLI \"Samapriya Roy. (2017, December 4). samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.1079887\" satadd: CLI pipeline for Planet, Satellogic, Google Earth Engine and Digital Globe Imagery \"Samapriya Roy. (2018, October 6). samapriya/satadd: satadd: CLI pipeline for Planet, Satellogic, Google Earth Engine and Digital Globe Imagery (Version 0.0.3). Zenodo. http://doi.org/10.5281/zenodo.1450622\" pygbdx: Simple CLI for Digital Globe GBDX \"Samapriya Roy. (2018, October 4). samapriya/pygbdx: pygbdx: Simple CLI for GBDX (Version 0.0.2). Zenodo. http://doi.org/10.5281/zenodo.1445734\"","title":"Project Citations"},{"location":"pypi/","text":"PyPI Releases \u00b6 I have successfully converted some of the github projects into PyPI projects for allowing easier installation and usage. Some of these have been tested for compatibility with Google Colaboratory and for building within a Docker image. Currently maintained pypi projects include porder: Simple CLI for Planet ordersV2 API \u00b6 Ordersv2 is the next iteration of Planet's API in getting Analysis Ready Data (ARD) delivered to you. Orders v2 allows you to improved functionality in this domain, including capability to submit an number of images in a batch order, and perform operations such as top of atmospheric reflectance, compression, coregistration and also enhanced notifications such as email and webhooks. Based on your access you can use this tool to chain together a sequence of operations. This tool is a command line interface that allows you to interact with the ordersv2 API along with place orders and download orders as needed. The tool also allows you to chain multiple processes together and additional functionalities will be added as needed. pip install porder satadd: CLI pipeline for Planet, Satellogic, Google Earth Engine and Digital Globe Imagery \u00b6 This tool was built with a focus on the same issues and borrow parts from my other projects such as ppipe for handling Planet's datasets, gee2drive to handle download collections already available in Google Earth Engine (GEE), pygbdx which is a relatively new project to explore Digital Globe assets and I have now integrated tools to access and download Satellogic imagery. Core components from a lot of these tools have gone into building satadd based on the idea of adding satelite data as needed. These tools include authentications setups for every account, and access to datasets, metadata among other tools. This was not build however for heavy lifting though I have tested this on hundreds and thousands of assets delivery so it behaves robustly for now. The tool is build and rebuilt as companies change their authentication protocal and delivery mechanisms and allow for improving many aspects of data delivery and preprocessing in the next iterations. pip install satadd Planet-Mosaic-Quads-Download-CLI \u00b6 Planet creates global monthly mosaics apart from creating mosaics at different frequencies, monthly mosaics are of interest to a lot of people who would like to do a consistent time series analysis using these mosaics and would like to apply them to an existing analytical pipeline. I created this tool to allow you pass single or multiple geometries in a folder for the tool to find the mosaic quads and then process and download it. For now the geometry is passed as a geojson file, but I have included a tool for you to convert any shapefile into geojson files so you can use this tool. In the future I will add support for kml and json files as well. pip install pbasemap geeup: Simple CLI for Google Earth Engine Uploads \u00b6 This came of the simple need to handle batch uploads of both image assets to collections but also thanks to the new table feature the possibility of batch uploading shapefiles into a folder. Though a lot of these tools including batch image uploader is part of my other project geeadd which also includes additional features to add to the python CLI, this tool was designed to be minimal so as to allow the user to simply query their quota, upload images or tables and also to query ongoing tasks and delete assets. I am hoping this tool with a simple objective proves useful to a few users of Google Earth Engine. pip install geeup gee2drive: Google Earth Engine to Drive Export Manager \u00b6 I created this tool which allows you to run a terminal environment where your personal and general catalog images are part of a autosuggest feature. This tool allows you to look for images based on names for example \" you can search for Sentinel and it will show you full path of images which have the word sentinel in the title\". It also creates a report for your image collections and images so apart from the public datasets this can also find your own datasets as well. You can then generate bandlist to make sure all bands you are exporting are of the same type and then export all images that intersect you aoi. pip install gee2drive Planet API Pipeline & Google Earth Engine Batch Assets Manager with Addons \u00b6 The tool allows the user to upload Planet assets to Google Earth Engine from your local system. The tool has been modified and tested in Docker environment and Colab environments. It makes use of the manifest feature of image upload so that it can choose image and asset type appropriately. It also figures out property type automatically to avoid conflict in metadata property type in successive images. You can read more about the project here pip install ppipe pydrop: Minimal Python Client for Digital Ocean Droplets \u00b6 This is a minimal tool designed to interact with the Digital Ocean API. This does not translate all functionalities of the API but is a template I created for some of the most common operations I could perform. New tools will be added in the future as I familiarize myself further with the API structure and use as a student. For now this tool allows you to summarize all your droplets running, including and necessarily a price summary to keep tabs on your droplets monthly and hourly rates. The tool also allows you to seach by tags, delete a drop or perfrom actions such as start, stop or shutdown a droplet. pip install pydrop Planet Clip-Ship Tools CLI \u00b6 The Clips API has been deprecated and will no longer be supporting the standalone Clip and Ship service. Deprecation means that there is intention to remove the service at some point; it DOES NOT mean end-of-life for the service, yet. (Deprecate and end-of-life are two distinct terms.) There should be a plan to replace Clips API with new pre-processing functionality, but no new announcement or timetable for the launch of the new service and the removal (end-of-life) for Clips API has been made. Planet\u2019s Clip API was a compute API designed to allowed users to clip the images to their area of interest. This would save them time in preprocessing and also allow the user to save on their area quota which might have restrictions. You can read more about the project here pip install pclip Google Earth Engine Batch Assets Manager with Addons \u00b6 Google Earth Engine Batch Asset Manager with Addons is an extension of the existing CLI and additional tools were added to include functionality that was missing from the EarthEngine CLI. This includes printing quota, moving and batch copying images, print asset report and task lists to name a few of the features. It is developed case by case basis to include more features in the future as it becomes available or as need arises.You can read more about the project here pip install geeadd Google Takeout and Transfer: Tools and Guide for Code and Asset Transfer \u00b6 This tool replicates a Google Earth Engine account and transfer everything over to a new account. Even if you are not replicating your account, think of this as an add on which allows you to download your entire code repositories, create an asset report and best of all iteratively change permissions to all image-collection and images whether or not within a folder.You can read more about the project here pip install geetakeout","title":"PyPI Projects"},{"location":"pypi/#pypi-releases","text":"I have successfully converted some of the github projects into PyPI projects for allowing easier installation and usage. Some of these have been tested for compatibility with Google Colaboratory and for building within a Docker image. Currently maintained pypi projects include","title":"PyPI Releases"},{"location":"pypi/#porder-simple-cli-for-planet-ordersv2-api","text":"Ordersv2 is the next iteration of Planet's API in getting Analysis Ready Data (ARD) delivered to you. Orders v2 allows you to improved functionality in this domain, including capability to submit an number of images in a batch order, and perform operations such as top of atmospheric reflectance, compression, coregistration and also enhanced notifications such as email and webhooks. Based on your access you can use this tool to chain together a sequence of operations. This tool is a command line interface that allows you to interact with the ordersv2 API along with place orders and download orders as needed. The tool also allows you to chain multiple processes together and additional functionalities will be added as needed. pip install porder","title":"porder: Simple CLI for Planet ordersV2 API"},{"location":"pypi/#satadd-cli-pipeline-for-planet-satellogic-google-earth-engine-and-digital-globe-imagery","text":"This tool was built with a focus on the same issues and borrow parts from my other projects such as ppipe for handling Planet's datasets, gee2drive to handle download collections already available in Google Earth Engine (GEE), pygbdx which is a relatively new project to explore Digital Globe assets and I have now integrated tools to access and download Satellogic imagery. Core components from a lot of these tools have gone into building satadd based on the idea of adding satelite data as needed. These tools include authentications setups for every account, and access to datasets, metadata among other tools. This was not build however for heavy lifting though I have tested this on hundreds and thousands of assets delivery so it behaves robustly for now. The tool is build and rebuilt as companies change their authentication protocal and delivery mechanisms and allow for improving many aspects of data delivery and preprocessing in the next iterations. pip install satadd","title":"satadd: CLI pipeline for Planet, Satellogic, Google Earth Engine and Digital Globe Imagery"},{"location":"pypi/#planet-mosaic-quads-download-cli","text":"Planet creates global monthly mosaics apart from creating mosaics at different frequencies, monthly mosaics are of interest to a lot of people who would like to do a consistent time series analysis using these mosaics and would like to apply them to an existing analytical pipeline. I created this tool to allow you pass single or multiple geometries in a folder for the tool to find the mosaic quads and then process and download it. For now the geometry is passed as a geojson file, but I have included a tool for you to convert any shapefile into geojson files so you can use this tool. In the future I will add support for kml and json files as well. pip install pbasemap","title":"Planet-Mosaic-Quads-Download-CLI"},{"location":"pypi/#geeup-simple-cli-for-google-earth-engine-uploads","text":"This came of the simple need to handle batch uploads of both image assets to collections but also thanks to the new table feature the possibility of batch uploading shapefiles into a folder. Though a lot of these tools including batch image uploader is part of my other project geeadd which also includes additional features to add to the python CLI, this tool was designed to be minimal so as to allow the user to simply query their quota, upload images or tables and also to query ongoing tasks and delete assets. I am hoping this tool with a simple objective proves useful to a few users of Google Earth Engine. pip install geeup","title":"geeup: Simple CLI for Google Earth Engine Uploads"},{"location":"pypi/#gee2drive-google-earth-engine-to-drive-export-manager","text":"I created this tool which allows you to run a terminal environment where your personal and general catalog images are part of a autosuggest feature. This tool allows you to look for images based on names for example \" you can search for Sentinel and it will show you full path of images which have the word sentinel in the title\". It also creates a report for your image collections and images so apart from the public datasets this can also find your own datasets as well. You can then generate bandlist to make sure all bands you are exporting are of the same type and then export all images that intersect you aoi. pip install gee2drive","title":"gee2drive: Google Earth Engine to Drive Export Manager"},{"location":"pypi/#planet-api-pipeline-google-earth-engine-batch-assets-manager-with-addons","text":"The tool allows the user to upload Planet assets to Google Earth Engine from your local system. The tool has been modified and tested in Docker environment and Colab environments. It makes use of the manifest feature of image upload so that it can choose image and asset type appropriately. It also figures out property type automatically to avoid conflict in metadata property type in successive images. You can read more about the project here pip install ppipe","title":"Planet API Pipeline &amp; Google Earth Engine Batch Assets Manager with Addons"},{"location":"pypi/#pydrop-minimal-python-client-for-digital-ocean-droplets","text":"This is a minimal tool designed to interact with the Digital Ocean API. This does not translate all functionalities of the API but is a template I created for some of the most common operations I could perform. New tools will be added in the future as I familiarize myself further with the API structure and use as a student. For now this tool allows you to summarize all your droplets running, including and necessarily a price summary to keep tabs on your droplets monthly and hourly rates. The tool also allows you to seach by tags, delete a drop or perfrom actions such as start, stop or shutdown a droplet. pip install pydrop","title":"pydrop: Minimal Python Client for Digital Ocean Droplets"},{"location":"pypi/#planet-clip-ship-tools-cli","text":"The Clips API has been deprecated and will no longer be supporting the standalone Clip and Ship service. Deprecation means that there is intention to remove the service at some point; it DOES NOT mean end-of-life for the service, yet. (Deprecate and end-of-life are two distinct terms.) There should be a plan to replace Clips API with new pre-processing functionality, but no new announcement or timetable for the launch of the new service and the removal (end-of-life) for Clips API has been made. Planet\u2019s Clip API was a compute API designed to allowed users to clip the images to their area of interest. This would save them time in preprocessing and also allow the user to save on their area quota which might have restrictions. You can read more about the project here pip install pclip","title":"Planet Clip-Ship Tools CLI"},{"location":"pypi/#google-earth-engine-batch-assets-manager-with-addons","text":"Google Earth Engine Batch Asset Manager with Addons is an extension of the existing CLI and additional tools were added to include functionality that was missing from the EarthEngine CLI. This includes printing quota, moving and batch copying images, print asset report and task lists to name a few of the features. It is developed case by case basis to include more features in the future as it becomes available or as need arises.You can read more about the project here pip install geeadd","title":"Google Earth Engine Batch Assets Manager with Addons"},{"location":"pypi/#google-takeout-and-transfer-tools-and-guide-for-code-and-asset-transfer","text":"This tool replicates a Google Earth Engine account and transfer everything over to a new account. Even if you are not replicating your account, think of this as an add on which allows you to download your entire code repositories, create an asset report and best of all iteratively change permissions to all image-collection and images whether or not within a folder.You can read more about the project here pip install geetakeout","title":"Google Takeout and Transfer: Tools and Guide for Code and Asset Transfer"},{"location":"release-notes/","text":"Recent Projects: Release notes \u00b6 Project Name Digital Object Identification Number(DOI) GitHub Release PyPI Stats porder: Simple ordersv2 CLI Planet-GEE-Pipeline-CLI GEE Asset Manager Addons satadd: MultiSource CLI geeup: GEE Upload CLI gee2drive: GEE Download CLI pydrop: Digital Ocean CLI Planet BaseMap CLI","title":"Release notes"},{"location":"release-notes/#recent-projects-release-notes","text":"Project Name Digital Object Identification Number(DOI) GitHub Release PyPI Stats porder: Simple ordersv2 CLI Planet-GEE-Pipeline-CLI GEE Asset Manager Addons satadd: MultiSource CLI geeup: GEE Upload CLI gee2drive: GEE Download CLI pydrop: Digital Ocean CLI Planet BaseMap CLI","title":"Recent Projects: Release notes"},{"location":"projects/arcmap_addon/","text":"ArcMap Addons \u00b6 While working on different projects over sometime I had to create and use some tools which acted as valuable addons to arcmap toolbox. I am sharing a few and I will keep on updating these as I keep working on new models.This toolbox was created on ArcMap 10.4 and is not backward compatible. The toolbox can be downloaded and added to existing toolbox to create additional functionalities. Table of contents \u00b6 Installation Usage examples Batch Raster to Point Batch Table to CSV Email Notification Iterative Clip MultiBand to Single Images Raster Properties as CSV Raster Copy Iterative Feature Select and Copy Select and Calculate Field Credits Installation \u00b6 We assume that the user already has a copy of ArcMap >=10.4. The toolbox can be downloaded along with adjoining script and added onto existing toolbox. You can keep this toolbox as part of the default toolboxes by clicking Save Settings>Save as Default Usage examples \u00b6 Usage examples will vary and only continue to grow as new tools are added to the toolbox. Batch Raster to Point \u00b6 This tool allows you to convert all Raster datasets in a folder into a point with the value field converted to GRID code. The tool supports all raster formats supported by ArcMap and renames the files automatically to the source file name. The raster pixel is converted to a centroid value. Batch Table to CSV \u00b6 This tool allows you to batch convert all table files(in this case it looks for '.dbf' files) and converts the fields into csv columns. This is an effective way when you want to handle a large number of dbf files to be imported into other softwares and or processing chains. Email Notification \u00b6 I found this to be one of the most useful tools when running a long process and not having to wait for your results. The tool makes use of the smtp library and allows the user to set up an email service for an email to be sent out after a process has been completed. Iterative Clip \u00b6 This tool does exactl what the name suggests it uses an iterator tool to clip a raster to different boundaries using shapefiles. Requires a single raster file and a folder with multiple polygons for clips. If using on a private machine the Key is saved as a csv file for all future runs of the tool. MultiBand to Single Images \u00b6 The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you. Raster Properties as CSV \u00b6 The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier Raster Copy Iterative \u00b6 Having metadata helps in organising your asstets, but is not mandatory - you can skip it. The downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types. Feature Select and Copy \u00b6 These tools allow the users to iteratively select features using a given SQL expression and then extract that as a new feature class. This can be done iteratively for all feature classes in the folder. Select and Calculate Field \u00b6 This tool allows you to select a field and then recalculate or calculate it based on expressions. The user has the option of simply filling empty fields based on selection of a different field. Meaning I can calculate a field B based on a selection of a seperate field A if I want. Credits \u00b6 I would like to thank all those who bring me interesting problems to solve, there is little that is needed to keep one inspired. Changelog \u00b6 v0.2 \u00b6 Batch Table to CSV Batch Raster to Points","title":"ArcMap Addon Tools"},{"location":"projects/arcmap_addon/#arcmap-addons","text":"While working on different projects over sometime I had to create and use some tools which acted as valuable addons to arcmap toolbox. I am sharing a few and I will keep on updating these as I keep working on new models.This toolbox was created on ArcMap 10.4 and is not backward compatible. The toolbox can be downloaded and added to existing toolbox to create additional functionalities.","title":"ArcMap Addons"},{"location":"projects/arcmap_addon/#table-of-contents","text":"Installation Usage examples Batch Raster to Point Batch Table to CSV Email Notification Iterative Clip MultiBand to Single Images Raster Properties as CSV Raster Copy Iterative Feature Select and Copy Select and Calculate Field Credits","title":"Table of contents"},{"location":"projects/arcmap_addon/#installation","text":"We assume that the user already has a copy of ArcMap >=10.4. The toolbox can be downloaded along with adjoining script and added onto existing toolbox. You can keep this toolbox as part of the default toolboxes by clicking Save Settings>Save as Default","title":"Installation"},{"location":"projects/arcmap_addon/#usage-examples","text":"Usage examples will vary and only continue to grow as new tools are added to the toolbox.","title":"Usage examples"},{"location":"projects/arcmap_addon/#batch-raster-to-point","text":"This tool allows you to convert all Raster datasets in a folder into a point with the value field converted to GRID code. The tool supports all raster formats supported by ArcMap and renames the files automatically to the source file name. The raster pixel is converted to a centroid value.","title":"Batch Raster to Point"},{"location":"projects/arcmap_addon/#batch-table-to-csv","text":"This tool allows you to batch convert all table files(in this case it looks for '.dbf' files) and converts the fields into csv columns. This is an effective way when you want to handle a large number of dbf files to be imported into other softwares and or processing chains.","title":"Batch Table to CSV"},{"location":"projects/arcmap_addon/#email-notification","text":"I found this to be one of the most useful tools when running a long process and not having to wait for your results. The tool makes use of the smtp library and allows the user to set up an email service for an email to be sent out after a process has been completed.","title":"Email Notification"},{"location":"projects/arcmap_addon/#iterative-clip","text":"This tool does exactl what the name suggests it uses an iterator tool to clip a raster to different boundaries using shapefiles. Requires a single raster file and a folder with multiple polygons for clips. If using on a private machine the Key is saved as a csv file for all future runs of the tool.","title":"Iterative Clip"},{"location":"projects/arcmap_addon/#multiband-to-single-images","text":"The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.","title":"MultiBand to Single Images"},{"location":"projects/arcmap_addon/#raster-properties-as-csv","text":"The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier","title":"Raster Properties as CSV"},{"location":"projects/arcmap_addon/#raster-copy-iterative","text":"Having metadata helps in organising your asstets, but is not mandatory - you can skip it. The downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types.","title":"Raster Copy Iterative"},{"location":"projects/arcmap_addon/#feature-select-and-copy","text":"These tools allow the users to iteratively select features using a given SQL expression and then extract that as a new feature class. This can be done iteratively for all feature classes in the folder.","title":"Feature Select and Copy"},{"location":"projects/arcmap_addon/#select-and-calculate-field","text":"This tool allows you to select a field and then recalculate or calculate it based on expressions. The user has the option of simply filling empty fields based on selection of a different field. Meaning I can calculate a field B based on a selection of a seperate field A if I want.","title":"Select and Calculate Field"},{"location":"projects/arcmap_addon/#credits","text":"I would like to thank all those who bring me interesting problems to solve, there is little that is needed to keep one inspired.","title":"Credits"},{"location":"projects/arcmap_addon/#changelog","text":"","title":"Changelog"},{"location":"projects/arcmap_addon/#v02","text":"Batch Table to CSV Batch Raster to Points","title":"v0.2"},{"location":"projects/arcticdem_download/","text":"ArcticDEM Batch Download & Processing Tools \u00b6 ArcticDEM project was a joint project supported by both the National Geospatial-Intelligence Agency(NGA) and the National Science Foundation(NSF) with the idea of creating a high resolution and high quality digital surface model(DSM). The product is distributed free of cost as time-dependent DEM strips and is hosted as https links that a user can use to download each strip. As per their policy The seamless terrain mosaic can be distributed without restriction. The created product is a 2-by-2 meter elevation cells over an over of over 20 million square kilometers and uses digital globe stereo imagery to create these high resolution DSM. The method used for the 2m derivate is Surface Extraction with TIN-based Search-space Minimization(SETSM). Based on their acknowledgements requests you can use Acknowledging PGC services(including data access) Geospatial support for this work provided by the Polar Geospatial Center under NSF OPP awards 1043681 & 1559691. Acknowledging DEMS created from the ArcticDEM project DEMs provided by the Polar Geospatial Center under NSF OPP awards 1043681, 1559691 and 1542736. You can find details on the background, scope and methods among other details here A detailed acknowledgement link can be found here With this in mind and with the potential applications of using these toolsets there was a need to batch download the DEM files for your area of interest and to be able to extract, clean and process metadata. In all fairness this tool has a motive of extending this as an input to Google Earth Engine and hence the last tool which is the metadata parser is designed to create a metadata manifest in a csv file which GEE can understand and associate during asset upload. Table of contents \u00b6 Installation Getting started Usage examples Subset to AOI Estimate Download Size Download DEM Extract DEM Metadata Parsing for GEE Installation \u00b6 We assume that you have installed the requirements files to install all the necessary packages and libraries required to use this tool. To install packages from the requirements.txt file you can simply use pip install -r requirements.txt . Remember that installation is an optional step and you can run this program by simply browsing to the pgcdem-cli file and typing python arcticdem.py . One of the only other requirement for this tool is the Master Shapefile for all DEM footprints(make sure to use the most updated version which can be found here ) This toolbox also uses some functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa && sudo apt-get update sudo apt-get install gdal-bin For Windows I found this guide from UCLA To install ArcticDEM Batch Download & Processing Tools: git clone https://github.com/samapriya/ArcticDEM-Batch-Pipeline.git cd ArcticDEM-Batch-Pipeline && pip install . This release also contains a windows installer which bypasses the need for you to have admin permission, it does however require you to have python in the system path meaning when you open up command prompt you should be able to type python and start it within the command prompt window. Post installation using the installer you can just call ppipe using the command prompt similar to calling python. Give it a go post installation type arcticdem -h The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. As a extra addon feature I have wrapped the cli into a Graphical User Interface(GUI) so some people will find it easier to use. To install run python setup.py develop or python setup.py install In a linux distribution sudo python setup.py develop or sudo python setup.py install Getting started \u00b6 To obtain help for a specific functionality, simply call it with help switch, e.g.: arcticdem demextract -h . If you didn't install arcticdem, then you can run it just by going to arcticdem-cli directory and running python arcticdem.py [arguments go here] As usual, to print help arcticdem -h : ArcticDEM Batch Download & Processing Tools positional arguments: { ,demaoi,demsize,demdownload,demextract,demmeta} --------------------------------------- -----Choose from ArcticDEM-Download Tools Below----- --------------------------------------- demaoi Allows user to subset Master ArcticDEM to their AOI demsize Allows users to estimate total download size and space left in your destination folder demdownload Allows users to batch download ArcticDEM Strips using aoi shapefile demextract Allows users to extract both image and metadata files from the zipped tar files demmeta Tool to process metadata files into CSV for all strips[For use with Google Earth Engine] optional arguments: -h, --help show this help message and exit Subset to AOI \u00b6 The script clips the master ArcticDEM strip file to a smaller subset usgin an area of interest shapefile. This allows to get the strip DEM(s) for only the area of interest and to use that to download these files. The subset allows the user to limit the total amount of strips to be downloaded and processed. The script will create a new shapefile with the clipped subset of the master ArcticDEM strip file. Make sure you reproject your aoi shapefile to the same projection as the ArcticDEM strip file usage: arcticdem.py demaoi [-h] [--source SOURCE] [--target TARGET] [--output OUTPUT] optional arguments: -h, --help show this help message and exit --source SOURCE Choose location of your AOI shapefile --target TARGET Choose the location of the master ArcticDEM strip file --output OUTPUT Choose the location of the output shapefile based on your AOI An example setup would be arcticdem demaoi --source \"C:\\users\\aoi.shp\" --target \"C:\\users\\masterdem.shp\" --output \"C:\\users\\master_aoi.shp\" Estimate Download Size \u00b6 One of the most common things you want to do is to know if the destination where you want to save these files has enough space before you begin the download process. This script allows you to query the total download size for your area of interest and the destination drive where you want to save the compressed files. It also recursively updates overall download size on the screen and print total size needed along with total download size in GB. usage: arcticdem.py demsize [-h] [--infile INFILE] [--path PATH] optional arguments: -h, --help show this help message and exit --infile INFILE Choose the clipped aoi file you clipped from demaoi tool[This is the subset of the master ArcticDEM Strip] --path PATH Choose the destination folder where you want your dem files to be saved[This checks available disk space] An example setup would be arcticdem demsize --infile \"C:\\users\\master_aoi.shp\" --path \"C:\\users\\ArcticDEM\" The program might misbehave if the area of interest is extremely large or be sluggish in nature. Download DEM \u00b6 What we were mainly interested after we know that we have enough space to download is to download the files. The script used a multi part download library to download the files quicker and in a more managed style to the destination given by the user. usage: arcticdem.py demdownload [-h] [--subset SUBSET] [--desination DESINATION] optional arguments: -h, --help show this help message and exit --subset SUBSET Choose the location of the output shapefile based on your AOI[You got this from demaoi tool] --desination DESINATION Choose the destination where you want to download your files An example setup would be arcticdem demdownload --subset \"C:\\users\\master_aoi.shp\" --destination \"C:\\users\\ArcticDEM\" Extract DEM \u00b6 This downloaded DEM files are tar or tar gz files and need to be extracted. The important thing to note is that the script retains the dem file, the matchtag file and the metadata text files in separate directories within the destination directory. usage: arcticdem.py demextract [-h] [--folder FOLDER] [--destination DESTINATION] [--action ACTION] optional arguments: -h, --help show this help message and exit --folder FOLDER Choose the download file where you downloaded your tar zipped files --destination DESTINATION Choose the destination folder where you want your images and metadata files to be extracted --action ACTION Choose if you want your zipped files to be deleted post extraction \"yes\"|\"no\" An example setup would be arcticdem demdextract --folder \"C:\\users\\ArcticDEM\" --destination \"C:\\users\\ArcticDEM\\Extract\" --action \"yes\" Metadata Parsing for GEE \u00b6 One of my key interest in working on the ArcticDEM parsing was to be able to upload this to Google Earth Engine(GEE). The metadata parser script allows you to parse all metadata into a combined csv file to be used to upload images to GEE. The manifest and upload tools are separate from this package tool and included in my gee_asset_manager_addon. usage: arcticdem.py demmeta [-h] [--folder FOLDER] [--metadata METADATA] [--error ERROR] optional arguments: -h, --help show this help message and exit --folder FOLDER Choose where you unzipped and extracted your DEM and metadata files --metadata METADATA Choose a path to the metadata file \"example: users/desktop/metadata.csv\" --error ERROR Choose a path to the errorlog file \"example: users/desktop/errorlog.csv\" An example setup would be arcticdem demmeta --folder \"C:\\users\\ArcticDEM\\Extract\\pgcmeta\" --metadata \"C:\\users\\arcticdem_metadata.csv\" --error \"C:\\users\\arcticdem_errorlog.csv\" Changelog \u00b6 [0.1.1] - 2017-08-12 \u00b6 Added \u00b6 Can now handle ogr input and includes instruction to project aoi in same projection as DEM strip. Added the capability of skipping over already downloaded files and continues with left over downloads. Completed recompiling executable to include changes.","title":"ArcticDEM-Batch-Pipeline"},{"location":"projects/arcticdem_download/#arcticdem-batch-download-processing-tools","text":"ArcticDEM project was a joint project supported by both the National Geospatial-Intelligence Agency(NGA) and the National Science Foundation(NSF) with the idea of creating a high resolution and high quality digital surface model(DSM). The product is distributed free of cost as time-dependent DEM strips and is hosted as https links that a user can use to download each strip. As per their policy The seamless terrain mosaic can be distributed without restriction. The created product is a 2-by-2 meter elevation cells over an over of over 20 million square kilometers and uses digital globe stereo imagery to create these high resolution DSM. The method used for the 2m derivate is Surface Extraction with TIN-based Search-space Minimization(SETSM). Based on their acknowledgements requests you can use Acknowledging PGC services(including data access) Geospatial support for this work provided by the Polar Geospatial Center under NSF OPP awards 1043681 & 1559691. Acknowledging DEMS created from the ArcticDEM project DEMs provided by the Polar Geospatial Center under NSF OPP awards 1043681, 1559691 and 1542736. You can find details on the background, scope and methods among other details here A detailed acknowledgement link can be found here With this in mind and with the potential applications of using these toolsets there was a need to batch download the DEM files for your area of interest and to be able to extract, clean and process metadata. In all fairness this tool has a motive of extending this as an input to Google Earth Engine and hence the last tool which is the metadata parser is designed to create a metadata manifest in a csv file which GEE can understand and associate during asset upload.","title":"ArcticDEM Batch Download &amp; Processing Tools"},{"location":"projects/arcticdem_download/#table-of-contents","text":"Installation Getting started Usage examples Subset to AOI Estimate Download Size Download DEM Extract DEM Metadata Parsing for GEE","title":"Table of contents"},{"location":"projects/arcticdem_download/#installation","text":"We assume that you have installed the requirements files to install all the necessary packages and libraries required to use this tool. To install packages from the requirements.txt file you can simply use pip install -r requirements.txt . Remember that installation is an optional step and you can run this program by simply browsing to the pgcdem-cli file and typing python arcticdem.py . One of the only other requirement for this tool is the Master Shapefile for all DEM footprints(make sure to use the most updated version which can be found here ) This toolbox also uses some functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa && sudo apt-get update sudo apt-get install gdal-bin For Windows I found this guide from UCLA To install ArcticDEM Batch Download & Processing Tools: git clone https://github.com/samapriya/ArcticDEM-Batch-Pipeline.git cd ArcticDEM-Batch-Pipeline && pip install . This release also contains a windows installer which bypasses the need for you to have admin permission, it does however require you to have python in the system path meaning when you open up command prompt you should be able to type python and start it within the command prompt window. Post installation using the installer you can just call ppipe using the command prompt similar to calling python. Give it a go post installation type arcticdem -h The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. As a extra addon feature I have wrapped the cli into a Graphical User Interface(GUI) so some people will find it easier to use. To install run python setup.py develop or python setup.py install In a linux distribution sudo python setup.py develop or sudo python setup.py install","title":"Installation"},{"location":"projects/arcticdem_download/#getting-started","text":"To obtain help for a specific functionality, simply call it with help switch, e.g.: arcticdem demextract -h . If you didn't install arcticdem, then you can run it just by going to arcticdem-cli directory and running python arcticdem.py [arguments go here] As usual, to print help arcticdem -h : ArcticDEM Batch Download & Processing Tools positional arguments: { ,demaoi,demsize,demdownload,demextract,demmeta} --------------------------------------- -----Choose from ArcticDEM-Download Tools Below----- --------------------------------------- demaoi Allows user to subset Master ArcticDEM to their AOI demsize Allows users to estimate total download size and space left in your destination folder demdownload Allows users to batch download ArcticDEM Strips using aoi shapefile demextract Allows users to extract both image and metadata files from the zipped tar files demmeta Tool to process metadata files into CSV for all strips[For use with Google Earth Engine] optional arguments: -h, --help show this help message and exit","title":"Getting started"},{"location":"projects/arcticdem_download/#subset-to-aoi","text":"The script clips the master ArcticDEM strip file to a smaller subset usgin an area of interest shapefile. This allows to get the strip DEM(s) for only the area of interest and to use that to download these files. The subset allows the user to limit the total amount of strips to be downloaded and processed. The script will create a new shapefile with the clipped subset of the master ArcticDEM strip file. Make sure you reproject your aoi shapefile to the same projection as the ArcticDEM strip file usage: arcticdem.py demaoi [-h] [--source SOURCE] [--target TARGET] [--output OUTPUT] optional arguments: -h, --help show this help message and exit --source SOURCE Choose location of your AOI shapefile --target TARGET Choose the location of the master ArcticDEM strip file --output OUTPUT Choose the location of the output shapefile based on your AOI An example setup would be arcticdem demaoi --source \"C:\\users\\aoi.shp\" --target \"C:\\users\\masterdem.shp\" --output \"C:\\users\\master_aoi.shp\"","title":"Subset to AOI"},{"location":"projects/arcticdem_download/#estimate-download-size","text":"One of the most common things you want to do is to know if the destination where you want to save these files has enough space before you begin the download process. This script allows you to query the total download size for your area of interest and the destination drive where you want to save the compressed files. It also recursively updates overall download size on the screen and print total size needed along with total download size in GB. usage: arcticdem.py demsize [-h] [--infile INFILE] [--path PATH] optional arguments: -h, --help show this help message and exit --infile INFILE Choose the clipped aoi file you clipped from demaoi tool[This is the subset of the master ArcticDEM Strip] --path PATH Choose the destination folder where you want your dem files to be saved[This checks available disk space] An example setup would be arcticdem demsize --infile \"C:\\users\\master_aoi.shp\" --path \"C:\\users\\ArcticDEM\" The program might misbehave if the area of interest is extremely large or be sluggish in nature.","title":"Estimate Download Size"},{"location":"projects/arcticdem_download/#download-dem","text":"What we were mainly interested after we know that we have enough space to download is to download the files. The script used a multi part download library to download the files quicker and in a more managed style to the destination given by the user. usage: arcticdem.py demdownload [-h] [--subset SUBSET] [--desination DESINATION] optional arguments: -h, --help show this help message and exit --subset SUBSET Choose the location of the output shapefile based on your AOI[You got this from demaoi tool] --desination DESINATION Choose the destination where you want to download your files An example setup would be arcticdem demdownload --subset \"C:\\users\\master_aoi.shp\" --destination \"C:\\users\\ArcticDEM\"","title":"Download DEM"},{"location":"projects/arcticdem_download/#extract-dem","text":"This downloaded DEM files are tar or tar gz files and need to be extracted. The important thing to note is that the script retains the dem file, the matchtag file and the metadata text files in separate directories within the destination directory. usage: arcticdem.py demextract [-h] [--folder FOLDER] [--destination DESTINATION] [--action ACTION] optional arguments: -h, --help show this help message and exit --folder FOLDER Choose the download file where you downloaded your tar zipped files --destination DESTINATION Choose the destination folder where you want your images and metadata files to be extracted --action ACTION Choose if you want your zipped files to be deleted post extraction \"yes\"|\"no\" An example setup would be arcticdem demdextract --folder \"C:\\users\\ArcticDEM\" --destination \"C:\\users\\ArcticDEM\\Extract\" --action \"yes\"","title":"Extract DEM"},{"location":"projects/arcticdem_download/#metadata-parsing-for-gee","text":"One of my key interest in working on the ArcticDEM parsing was to be able to upload this to Google Earth Engine(GEE). The metadata parser script allows you to parse all metadata into a combined csv file to be used to upload images to GEE. The manifest and upload tools are separate from this package tool and included in my gee_asset_manager_addon. usage: arcticdem.py demmeta [-h] [--folder FOLDER] [--metadata METADATA] [--error ERROR] optional arguments: -h, --help show this help message and exit --folder FOLDER Choose where you unzipped and extracted your DEM and metadata files --metadata METADATA Choose a path to the metadata file \"example: users/desktop/metadata.csv\" --error ERROR Choose a path to the errorlog file \"example: users/desktop/errorlog.csv\" An example setup would be arcticdem demmeta --folder \"C:\\users\\ArcticDEM\\Extract\\pgcmeta\" --metadata \"C:\\users\\arcticdem_metadata.csv\" --error \"C:\\users\\arcticdem_errorlog.csv\"","title":"Metadata Parsing for GEE"},{"location":"projects/arcticdem_download/#changelog","text":"","title":"Changelog"},{"location":"projects/arcticdem_download/#011-2017-08-12","text":"","title":"[0.1.1] - 2017-08-12"},{"location":"projects/arcticdem_download/#added","text":"Can now handle ogr input and includes instruction to project aoi in same projection as DEM strip. Added the capability of skipping over already downloaded files and continues with left over downloads. Completed recompiling executable to include changes.","title":"Added"},{"location":"projects/clip_ship_planet_cli/","text":"Clip Ship Planet CLI addon \u00b6 Note: The Clips API has been deprecated - and will no longer be supporting the standalone Clip and Ship service. Deprecation means that there is intention to remove the service at some point; it DOES NOT mean end-of-life for the service, yet. (Deprecate and end-of-life are two distinct terms.) There should be a plan to replace Clips API with new pre-processing functionality, but no new announcement or timetable for the launch of the new service and the removal (end-of-life) for Clips API has been made. Planet's Clip API was a compute API designed to allowed users to clip the images to their area of interest. This would save them time in preprocessing and also allow the user to save on their area quota which might have restrictions. Based on Planet's Education and Research Program this quota is set at 10,000 square kilometers a month, which means saving up on quota is very useful. The discussion also led to an important clarification that users are in fact charged only for the area downloaded post clip if using the clip operation and hence this tool. This tool takes a sequential approach from activation to generating a clip request for multiple images activated and then processing the download tokens to actually download the clipped image files. The tool also consists of a sort function which allows the user to extract the files and sort them by type and deleting the original files to save on space. Installation \u00b6 To install the Clip-Ship-Planet-CLI you can simply perform the following action with Linux(Tested on Ubuntu 16): git clone https://github.com/samapriya/Clip-Ship-Planet-CLI.git cd Clip-Ship-Planet-CLI && pip install -r requirements.txt On a windows as well as a linux machine, installation is an optional step; the application can also be run directly by executing pclip.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment but you can also install it to system python and should not create any conflicts. To install on windows download the setup files as a zip package, unpack and run python setup.py develop or python setup.py install In a linux distribution sudo python setup.py develop or sudo python setup.py install pclip -h Table of contents \u00b6 Getting started Usage examples Planet Key Planet Quota AOI JSON Activate or Check Asset List IDs Clipping with GeoJSON Clipping with JSON Downloading Clipped Imagery Sorting Getting started \u00b6 As usual, to print help: Planet Clip Tools CLI positional arguments: { ,planetkey,aoijson,activate,aoiupdate,idlist,geojsonc,jsonc,downloadclips,sort} ------------------------------------------- -----Choose from Planet Clip Tools----- ------------------------------------------- planetkey Enter your planet API Key quota Prints your quota details aoijson Tool to convert KML, Shapefile,WKT,GeoJSON or Landsat WRS PathRow file to AreaOfInterest.JSON file with structured query for use with Planet API 1.0 activate Tool to query and/or activate Planet Assets idlist Allows users to generate an id list for the selected item and asset type for example item_asset= PSOrthoTile analytic/PSScene3Band visual. This is used with the clip tool geojsonc Allows users to batch submit clipping request to the Planet Clip API using geometry in geojson file jsonc Allows users to batch submit clipping request to the Planet Clip API using geometry in structured json file. This is preferred because the structured JSON allows the activate tool to stream line asset ids being requested and to extract geometry from the same file downloadclips Allows users to batch download clipped assets post computation using a directory path(Requires you to first activate and run geojson or json tool) sort Allows users to unzip downloaded files to new folder and sorts into images and metadata optional arguments: -h, --help show this help message and exit Usage examples \u00b6 The tools have been designed to follow a sequential setup from activation, clip, download and even sort and includes steps that help resolve additional issues a user might face trying to download clipped area of interests instead of entire scenes. The system will ask you to enter your API key before the CLI starts(this will prompt you only once to change API key use the Planet Key tool). Planet Key \u00b6 This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools. Ites makes use of the Planet client and esentially executes planet init usage: pclip planetkey [-h] optional arguments: -h, --help show this help message and exit Planet Quota \u00b6 This tool prints details on your existing quota and your area remaining usage: pclip quota optional arguments: -h, --help show this help message and exit AOI JSON \u00b6 The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you. usage: pclip aoijson [-h] [--start START] [--end END] [--cloud CLOUD] [--inputfile INPUTFILE] [--geo GEO] [--loc LOC] optional arguments: -h, --help show this help message and exit --start START Start date in YYYY-MM-DD? --end END End date in YYYY-MM-DD? --cloud CLOUD Maximum Cloud Cover(0-1) representing 0-100 --inputfile INPUTFILE Choose a kml/shapefile/geojson or WKT file for AOI(KML/SHP/GJSON/WKT) or WRS (6 digit RowPath Example: 023042) --geo GEO map.geojson/aoi.kml/aoi.shp/aoi.wkt file --loc LOC Location where aoi.json file is to be stored As with the Planet-GEE-Pipeline-CLI the aoijson tool allows the user to bring any filetype of interest, which includes GEOJSON, WKT, KML or SHP file including but not limited to WRS rowpath setup and structures it to enable filtered query using Planet's data API. A simple setup would be pclip aoijson --start \"2017-06-01\" --end \"2017-12-31\" --cloud \"0.15\" --inputfile \"GJSON\" --geo \"C:\\planet\\myarea.geojson\" --loc \"C:\\planet\" the output is always named as aoi.json. Activate or Check Asset \u00b6 The activate tool allows the users to either check or activate planet assets. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier. This is a necessary step since the clip API can only work with those ID(s) which have been activated. In the future the list ID tool will check for number of activated id and wait for all of them to be activated before generating an ID list. usage: pclip activate [-h] [--aoi AOI] [--action ACTION] [--asst ASST] optional arguments: -h, --help show this help message and exit --aoi AOI Choose aoi.json file created earlier --action ACTION choose between check/activate --asset ASST Choose between planet asset types (PSOrthoTile analytic/PSOrthoTile analytic_dn/PSOrthoTile visual/PSScene4Band analytic/PSScene4Band analytic_dn/PSScene3Band analytic/PSScene3Band analytic_dn/PSScene3Band visual/REOrthoTile analytic/REOrthoTile visual An example setup for asset activation is the following pclip activate --aoi \"C:\\planet\\aoi.json\" --action \"activate\" --asset \"PSOrthoTile analytic\" List IDs \u00b6 The next step is to list ID(s) that you have activated, this creates a temporary file containing the list of ID(s) which can be used to iteratively call the clips API. This is a modification of the activation function to use only the item id instead of item type and asset id and write to file for future use. usage: pclip idlist [-h] [--aoi AOI] [--asset ASSET] optional arguments: -h, --help show this help message and exit --aoi AOI Input path to the structured json file from which we will generate the clips --asset ASSET Choose from asset type for example:\"PSOrthoTile analytic\"|\"REOrthoTile analytic\" The example setup for this command is the following pclip idlist --aoi \u201cC:\\planet\\aoi.json\u201d --asset \u201cPSOrthoTile analytic\u201d Clipping with GeoJSON \u00b6 A geejson file can be used directly to clip and query the area of interest and then submit clip process. I added this is a functionality but want to make clear that this does not take into consideration any other filters such as cloud cover or start and end date, and hence should be used only when you do not need to apply any filter. usage: pclip geojsonc [-h] [--path PATH] [--item ITEM] [--asset ASSET] optional arguments: -h, --help show this help message and exit --path PATH Path to the geojson file including filename (Example: C:\\users ile.geojson) --item ITEM Choose from item type for example:\"PSOrthoTile\",\"REOrthoTile\" --asset ASSET Choose from asset type for example: \"visual\",\"analytic\" ``` A simple setup for the JSON tool is the following ```pclip geojsonc --path \u201cC:\\planet\\aoi.geojson\u201d --item \u201cPSOrthoTile\u201d --asset \u201canalytic\"``` ### Clipping with JSON This is the preferred style of submitting the clip requests using the IDlist we generated earlier. This is already structured before even activating assets and includes the additional filters you might have used for selecting the images. usage: pclip jsonc [-h] [--path PATH] [--item ITEM] [--asset ASSET] optional arguments: -h, --help show this help message and exit --path PATH Path to the json file including filename (Example: C:\\users ile.json) --item ITEM Choose from item type for example:\"PSOrthoTile\",\"REOrthoTile\" --asset ASSET Choose from asset type for example: \"visual\",\"analytic\" ``` A simple setup for the JSON tool is the following pclip jsonc --path \u201cC:\\planet\\aoi.json\u201d --item \u201cPSOrthoTile\u201d --asset \u201canalytic\" Downloading Clipped Imagery \u00b6 The last step includes providing a location where the clipped imagery can be downloaded. This includes the zip files that are generated from the earlier step and include a download token that expires over time. This batch downloads the clipped zip files to destination directory usage: pclip downloadclips [-h] [--dir DIR] optional arguments: -h, --help show this help message and exit --dir DIR Output directory to save the assets. All files are zipped and include metadata A simple setup includes just the location to the download directory for the zipped & clipped files to be downloaded pclip downloadclips --dir \u201cC:\\planet\\zipped\" Sorting \u00b6 As an additional measure and because it makes arranging and handling datasets easily, this setup comes completed with a sort tool. If a output directory is provided for the unzipped files, the tool unzips all files, moves the images and metadata to seperate directories and then deletes the original zipped files to save space. usage: pclip sort [-h] [--zipped ZIPPED] [--unzipped UNZIPPED] optional arguments: -h, --help show this help message and exit --zipped ZIPPED Folder containing downloaded clipped files which are zipped --unzipped UNZIPPED Folder where you want your files to be unzipped and sorted A simple would be the following (Images and metadata are sorted into an image and metadata folder inside the unzipped files folder) pclip sort --zipped \u201cC:\\planet\\zipped\u201d --unzipped \u201cC:\\planet\\unzipped\u201d Changelog \u00b6 v0.2.2 \u00b6 Improved Planet Key Handler Added new tool to insepect planet account quota v0.2.1 \u00b6 Thanks to commit suggested by Rabscuttler Fixed issues with help text and installer v0.2.0 \u00b6 Fixed issues with config files v0.1.9 \u00b6 Now handles running and succeeded status better Now enumerates during clip and download to allow user estimates on number of assets clipped and/or downloaded v0.1.8 \u00b6 Includes required packages list within installer Robust GEOJSON Parsing v0.1.7 \u00b6 Fixed issues with processing visual asset types The Clip function now handles error codes if the post response code is not 202(accepted for processing) then the error code and item and asset type is printed. v0.1.6 \u00b6 Handles single time input API Key, this is needed only once to start the program Fixed issue with base metadata folder during sort Updated asset argument for asset activation to match styles v0.1.5 \u00b6 Updated Requirements.txt to include pyshp Fixed subprocess shell error, for now shell=True v0.1.4 \u00b6 General Improvements v0.1.3 \u00b6 General Improvements v0.1.2 \u00b6 Tested on Ubuntu 16.04 and now handles permissions problem Temporary files now written to config folders to avoid admin permission v0.1.1 \u00b6 General Improvements","title":"Clip Ship Planet CLI"},{"location":"projects/clip_ship_planet_cli/#clip-ship-planet-cli-addon","text":"Note: The Clips API has been deprecated - and will no longer be supporting the standalone Clip and Ship service. Deprecation means that there is intention to remove the service at some point; it DOES NOT mean end-of-life for the service, yet. (Deprecate and end-of-life are two distinct terms.) There should be a plan to replace Clips API with new pre-processing functionality, but no new announcement or timetable for the launch of the new service and the removal (end-of-life) for Clips API has been made. Planet's Clip API was a compute API designed to allowed users to clip the images to their area of interest. This would save them time in preprocessing and also allow the user to save on their area quota which might have restrictions. Based on Planet's Education and Research Program this quota is set at 10,000 square kilometers a month, which means saving up on quota is very useful. The discussion also led to an important clarification that users are in fact charged only for the area downloaded post clip if using the clip operation and hence this tool. This tool takes a sequential approach from activation to generating a clip request for multiple images activated and then processing the download tokens to actually download the clipped image files. The tool also consists of a sort function which allows the user to extract the files and sort them by type and deleting the original files to save on space.","title":"Clip Ship Planet CLI addon"},{"location":"projects/clip_ship_planet_cli/#installation","text":"To install the Clip-Ship-Planet-CLI you can simply perform the following action with Linux(Tested on Ubuntu 16): git clone https://github.com/samapriya/Clip-Ship-Planet-CLI.git cd Clip-Ship-Planet-CLI && pip install -r requirements.txt On a windows as well as a linux machine, installation is an optional step; the application can also be run directly by executing pclip.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment but you can also install it to system python and should not create any conflicts. To install on windows download the setup files as a zip package, unpack and run python setup.py develop or python setup.py install In a linux distribution sudo python setup.py develop or sudo python setup.py install pclip -h","title":"Installation"},{"location":"projects/clip_ship_planet_cli/#table-of-contents","text":"Getting started Usage examples Planet Key Planet Quota AOI JSON Activate or Check Asset List IDs Clipping with GeoJSON Clipping with JSON Downloading Clipped Imagery Sorting","title":"Table of contents"},{"location":"projects/clip_ship_planet_cli/#getting-started","text":"As usual, to print help: Planet Clip Tools CLI positional arguments: { ,planetkey,aoijson,activate,aoiupdate,idlist,geojsonc,jsonc,downloadclips,sort} ------------------------------------------- -----Choose from Planet Clip Tools----- ------------------------------------------- planetkey Enter your planet API Key quota Prints your quota details aoijson Tool to convert KML, Shapefile,WKT,GeoJSON or Landsat WRS PathRow file to AreaOfInterest.JSON file with structured query for use with Planet API 1.0 activate Tool to query and/or activate Planet Assets idlist Allows users to generate an id list for the selected item and asset type for example item_asset= PSOrthoTile analytic/PSScene3Band visual. This is used with the clip tool geojsonc Allows users to batch submit clipping request to the Planet Clip API using geometry in geojson file jsonc Allows users to batch submit clipping request to the Planet Clip API using geometry in structured json file. This is preferred because the structured JSON allows the activate tool to stream line asset ids being requested and to extract geometry from the same file downloadclips Allows users to batch download clipped assets post computation using a directory path(Requires you to first activate and run geojson or json tool) sort Allows users to unzip downloaded files to new folder and sorts into images and metadata optional arguments: -h, --help show this help message and exit","title":"Getting started"},{"location":"projects/clip_ship_planet_cli/#usage-examples","text":"The tools have been designed to follow a sequential setup from activation, clip, download and even sort and includes steps that help resolve additional issues a user might face trying to download clipped area of interests instead of entire scenes. The system will ask you to enter your API key before the CLI starts(this will prompt you only once to change API key use the Planet Key tool).","title":"Usage examples"},{"location":"projects/clip_ship_planet_cli/#planet-key","text":"This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools. Ites makes use of the Planet client and esentially executes planet init usage: pclip planetkey [-h] optional arguments: -h, --help show this help message and exit","title":"Planet Key"},{"location":"projects/clip_ship_planet_cli/#planet-quota","text":"This tool prints details on your existing quota and your area remaining usage: pclip quota optional arguments: -h, --help show this help message and exit","title":"Planet Quota"},{"location":"projects/clip_ship_planet_cli/#aoi-json","text":"The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you. usage: pclip aoijson [-h] [--start START] [--end END] [--cloud CLOUD] [--inputfile INPUTFILE] [--geo GEO] [--loc LOC] optional arguments: -h, --help show this help message and exit --start START Start date in YYYY-MM-DD? --end END End date in YYYY-MM-DD? --cloud CLOUD Maximum Cloud Cover(0-1) representing 0-100 --inputfile INPUTFILE Choose a kml/shapefile/geojson or WKT file for AOI(KML/SHP/GJSON/WKT) or WRS (6 digit RowPath Example: 023042) --geo GEO map.geojson/aoi.kml/aoi.shp/aoi.wkt file --loc LOC Location where aoi.json file is to be stored As with the Planet-GEE-Pipeline-CLI the aoijson tool allows the user to bring any filetype of interest, which includes GEOJSON, WKT, KML or SHP file including but not limited to WRS rowpath setup and structures it to enable filtered query using Planet's data API. A simple setup would be pclip aoijson --start \"2017-06-01\" --end \"2017-12-31\" --cloud \"0.15\" --inputfile \"GJSON\" --geo \"C:\\planet\\myarea.geojson\" --loc \"C:\\planet\" the output is always named as aoi.json.","title":"AOI JSON"},{"location":"projects/clip_ship_planet_cli/#activate-or-check-asset","text":"The activate tool allows the users to either check or activate planet assets. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier. This is a necessary step since the clip API can only work with those ID(s) which have been activated. In the future the list ID tool will check for number of activated id and wait for all of them to be activated before generating an ID list. usage: pclip activate [-h] [--aoi AOI] [--action ACTION] [--asst ASST] optional arguments: -h, --help show this help message and exit --aoi AOI Choose aoi.json file created earlier --action ACTION choose between check/activate --asset ASST Choose between planet asset types (PSOrthoTile analytic/PSOrthoTile analytic_dn/PSOrthoTile visual/PSScene4Band analytic/PSScene4Band analytic_dn/PSScene3Band analytic/PSScene3Band analytic_dn/PSScene3Band visual/REOrthoTile analytic/REOrthoTile visual An example setup for asset activation is the following pclip activate --aoi \"C:\\planet\\aoi.json\" --action \"activate\" --asset \"PSOrthoTile analytic\"","title":"Activate or Check Asset"},{"location":"projects/clip_ship_planet_cli/#list-ids","text":"The next step is to list ID(s) that you have activated, this creates a temporary file containing the list of ID(s) which can be used to iteratively call the clips API. This is a modification of the activation function to use only the item id instead of item type and asset id and write to file for future use. usage: pclip idlist [-h] [--aoi AOI] [--asset ASSET] optional arguments: -h, --help show this help message and exit --aoi AOI Input path to the structured json file from which we will generate the clips --asset ASSET Choose from asset type for example:\"PSOrthoTile analytic\"|\"REOrthoTile analytic\" The example setup for this command is the following pclip idlist --aoi \u201cC:\\planet\\aoi.json\u201d --asset \u201cPSOrthoTile analytic\u201d","title":"List IDs"},{"location":"projects/clip_ship_planet_cli/#clipping-with-geojson","text":"A geejson file can be used directly to clip and query the area of interest and then submit clip process. I added this is a functionality but want to make clear that this does not take into consideration any other filters such as cloud cover or start and end date, and hence should be used only when you do not need to apply any filter. usage: pclip geojsonc [-h] [--path PATH] [--item ITEM] [--asset ASSET] optional arguments: -h, --help show this help message and exit --path PATH Path to the geojson file including filename (Example: C:\\users ile.geojson) --item ITEM Choose from item type for example:\"PSOrthoTile\",\"REOrthoTile\" --asset ASSET Choose from asset type for example: \"visual\",\"analytic\" ``` A simple setup for the JSON tool is the following ```pclip geojsonc --path \u201cC:\\planet\\aoi.geojson\u201d --item \u201cPSOrthoTile\u201d --asset \u201canalytic\"``` ### Clipping with JSON This is the preferred style of submitting the clip requests using the IDlist we generated earlier. This is already structured before even activating assets and includes the additional filters you might have used for selecting the images. usage: pclip jsonc [-h] [--path PATH] [--item ITEM] [--asset ASSET] optional arguments: -h, --help show this help message and exit --path PATH Path to the json file including filename (Example: C:\\users ile.json) --item ITEM Choose from item type for example:\"PSOrthoTile\",\"REOrthoTile\" --asset ASSET Choose from asset type for example: \"visual\",\"analytic\" ``` A simple setup for the JSON tool is the following pclip jsonc --path \u201cC:\\planet\\aoi.json\u201d --item \u201cPSOrthoTile\u201d --asset \u201canalytic\"","title":"Clipping with GeoJSON"},{"location":"projects/clip_ship_planet_cli/#downloading-clipped-imagery","text":"The last step includes providing a location where the clipped imagery can be downloaded. This includes the zip files that are generated from the earlier step and include a download token that expires over time. This batch downloads the clipped zip files to destination directory usage: pclip downloadclips [-h] [--dir DIR] optional arguments: -h, --help show this help message and exit --dir DIR Output directory to save the assets. All files are zipped and include metadata A simple setup includes just the location to the download directory for the zipped & clipped files to be downloaded pclip downloadclips --dir \u201cC:\\planet\\zipped\"","title":"Downloading Clipped Imagery"},{"location":"projects/clip_ship_planet_cli/#sorting","text":"As an additional measure and because it makes arranging and handling datasets easily, this setup comes completed with a sort tool. If a output directory is provided for the unzipped files, the tool unzips all files, moves the images and metadata to seperate directories and then deletes the original zipped files to save space. usage: pclip sort [-h] [--zipped ZIPPED] [--unzipped UNZIPPED] optional arguments: -h, --help show this help message and exit --zipped ZIPPED Folder containing downloaded clipped files which are zipped --unzipped UNZIPPED Folder where you want your files to be unzipped and sorted A simple would be the following (Images and metadata are sorted into an image and metadata folder inside the unzipped files folder) pclip sort --zipped \u201cC:\\planet\\zipped\u201d --unzipped \u201cC:\\planet\\unzipped\u201d","title":"Sorting"},{"location":"projects/clip_ship_planet_cli/#changelog","text":"","title":"Changelog"},{"location":"projects/clip_ship_planet_cli/#v022","text":"Improved Planet Key Handler Added new tool to insepect planet account quota","title":"v0.2.2"},{"location":"projects/clip_ship_planet_cli/#v021","text":"Thanks to commit suggested by Rabscuttler Fixed issues with help text and installer","title":"v0.2.1"},{"location":"projects/clip_ship_planet_cli/#v020","text":"Fixed issues with config files","title":"v0.2.0"},{"location":"projects/clip_ship_planet_cli/#v019","text":"Now handles running and succeeded status better Now enumerates during clip and download to allow user estimates on number of assets clipped and/or downloaded","title":"v0.1.9"},{"location":"projects/clip_ship_planet_cli/#v018","text":"Includes required packages list within installer Robust GEOJSON Parsing","title":"v0.1.8"},{"location":"projects/clip_ship_planet_cli/#v017","text":"Fixed issues with processing visual asset types The Clip function now handles error codes if the post response code is not 202(accepted for processing) then the error code and item and asset type is printed.","title":"v0.1.7"},{"location":"projects/clip_ship_planet_cli/#v016","text":"Handles single time input API Key, this is needed only once to start the program Fixed issue with base metadata folder during sort Updated asset argument for asset activation to match styles","title":"v0.1.6"},{"location":"projects/clip_ship_planet_cli/#v015","text":"Updated Requirements.txt to include pyshp Fixed subprocess shell error, for now shell=True","title":"v0.1.5"},{"location":"projects/clip_ship_planet_cli/#v014","text":"General Improvements","title":"v0.1.4"},{"location":"projects/clip_ship_planet_cli/#v013","text":"General Improvements","title":"v0.1.3"},{"location":"projects/clip_ship_planet_cli/#v012","text":"Tested on Ubuntu 16.04 and now handles permissions problem Temporary files now written to config folders to avoid admin permission","title":"v0.1.2"},{"location":"projects/clip_ship_planet_cli/#v011","text":"General Improvements","title":"v0.1.1"},{"location":"projects/ee_data/","text":"Earth-Engine-Datasets-List \u00b6 The idea is to make this list machine readable so you can programmatically call assets based on filters and to have a list for users who are not yet registered but want to look at the dataset list within earth engine. Register for a free Google Earth Engine account . The table is in the form a csv file with the following setup id provider title start_date end_date startyear endyear type tags AHN/AHN2_05M_INT AHN AHN Netherlands 0 12/31/11 12/31/11 2011 2011 Image ahn, lidar, elevation, netherlands, dem, geophysical The list allows for datasets to be built into a parser and we can pull these as though a dictionary for making selections before applying any other tool.","title":"Google Earth Engine Dataset lists"},{"location":"projects/ee_data/#earth-engine-datasets-list","text":"The idea is to make this list machine readable so you can programmatically call assets based on filters and to have a list for users who are not yet registered but want to look at the dataset list within earth engine. Register for a free Google Earth Engine account . The table is in the form a csv file with the following setup id provider title start_date end_date startyear endyear type tags AHN/AHN2_05M_INT AHN AHN Netherlands 0 12/31/11 12/31/11 2011 2011 Image ahn, lidar, elevation, netherlands, dem, geophysical The list allows for datasets to be built into a parser and we can pull these as though a dictionary for making selections before applying any other tool.","title":"Earth-Engine-Datasets-List"},{"location":"projects/gee2drive/","text":"gee2drive: Download Earth Engine Public and Private assets to Google Drive \u00b6 Google Earth Engine currently allows you to export images and assets as either GeoTiff files or TFrecords. The system splits the files if the estimated size is greater than 2GB which is the upper limit and needs the geometry to be parsed in the form of either a fusion table, a user drawn geometry or a table imported into the user's assets. While the javascript frontend is great owing to the queryable catalog whereby you can search and and export your personal and private assets, the limitation lies in batch exports. To resolve this the python API access allows you to call batch export functions but now it is limited to checking for itersects first and running without having a queryable catalog. With the same idea I created this tool which allows you to run a terminal environment where your personal and general catalog images are part of a autosuggest feature. This tool allows you to look for images based on names for example \" you can search for Sentinel and it will show you full path of images which have the word sentinel in the title\". It also creates a report for your image collections and images so apart from the public datasets this can also find your own datasets as well. You can then generate bandlist to make sure all bands you are exporting are of the same type and then export all images that intersect you aoi. The assumption here is * Every image in the give image have the same band structure, choose the bandlist that you know to common to all images * If the geomery is too complex use the operator feature to use a bounding box instead. * For now all it filters is geometry and date, and it is does not filter based on metadata (however in the examples folder I have shown how to import and use additional filter before exporting an image collection) In the future I will try to integrate some other functionalities to this environment and you can indeed run the tool without the use of the autosuggest terminal as a simple CLI. Hence the terminal feature is optional. Table of contents \u00b6 Installation Getting started Google Earth Engine to Drive Manager GEE to Google Drive CLI gee2drive Terminal gee2drive refresh gee2drive idsearch gee2drive bandtype gee2drive export Installation \u00b6 This assumes that you have native python & pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying. This assumes that you are also well aware of Google Earth Engine Python setup and have it installed and authetenticated on your system. If not you can read about it here python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have tested this only on python 2.7.15 but can be easily modified for python 3. To install Python CLI for Digital Ocean you can install using two methods pip install gee2drive or you can also try git clone https://github.com/samapriya/gee2drive.git cd gee2drive python setup.py install Use might have to use sudo privileges Installation is an optional step; the application can be also run directly by executing gee2drive.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. If you don't want to install, browse into the gee2drive folder and try python gee2drive.py to get to the same result. Getting started \u00b6 As usual, to print help: usage: gee2drive [-h] {terminal,refresh,idsearch,bandtype,export} ... Google Earth Engine to Drive Exporter positional arguments: {terminal,refresh,idsearch,bandtype,export} terminal Starts the interactive terminal with autosuggest refresh Refreshes your personal asset list and GEE Asset list idsearch Does possible matches using asset name to give you asseth id/full path bandtype Prints bandtype and generates list to be used for export export Export Collections based on filter optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: gee2drive idsearch -h . If you didn't install gee2drive, then you can run it just by going to gee2drive directory and running python gee2drive.py [arguments go here] GEE to Google Drive CLI \u00b6 This tool is designed to augment to the existing facilty of image export using a CLI, whereby you can pass it arguments to filter based on an area of interest geojson file, a start and end date for collection gee2drive Terminal \u00b6 This is an autosuggestive terminal which uses the gee2add package to perform all of the functions but has autosuggest for Earth Engine catalog and your own personal catalog. This way you can get access to image id without needing the catalog id in the javascript codeeditor. usage: gee2drive terminal [-h] optional arguments: -h, --help show this help message and exit Once you type gee2drive terminal you get a shell inside your current terminal where you get autosuggest for image and have full functionality of the terminal. gee2drive refresh \u00b6 For the past couple of months I have maintained a catalog of the most current Google Earth Engine assets , within their raster data catalog. I update this list every week. This tool downloads the most current version of this list, and also looks into your personal assets to generate your very own asset report which then serve as a master dataset to feed into autosuggestions. gee2drive refresh -h usage: gee2drive refresh [-h] optional arguments: -h, --help show this help message and exit gee2drive idsearch \u00b6 There is a possibility that you don't really remember the full path to your asset or the public asset. Fortunately when I parse and collect the image list and path for you they have names that are searchable so use a keyword. for example search using \"MODIS\" or \"sentinel\". Also it is not case sensitive, so you should be able to type \"SENTINEl\" or \"Sentinel\" or \"sentinel\" and it should still work gee2drive idsearch -h usage: gee2drive idsearch [-h] [--name NAME] optional arguments: -h, --help show this help message and exit --name NAME Name or part of name to search for gee2drive bandtype \u00b6 Export requires all the bandtypes to be of the same kind. To do this, I simply generate the band types for you and you can select the band list you want , remember to paste it as a list. usage: gee2drive bandtype [-h] [--id ID] optional arguments: -h, --help show this help message and exit --id ID full path for collection or image gee2drive export \u00b6 Finally the export tool, that lets you export an image or a collection clipped to your AOI. This makes use of the bandlist you exported. Incase you are exporting an image and not a collection you don't need a start and end date. The tool uses the bounds() function to use a bounding box incase the geometry has a complex geometry or too many vertices simply use the operator bb . If the geojson/json/kml keeps giving parsing error go to geojson.io usage: gee2drive export [-h] [--id ID] [--type TYPE] [--folder FOLDER] [--aoi AOI] [--start START] [--end END] [--bandlist BANDLIST] [--operator OPERATOR] optional arguments: -h, --help show this help message and exit --id ID Full path for collection or image --type TYPE Type whether image or collection --folder FOLDER Drive folder path --aoi AOI Full path to geojson/json/kml to be used for bounds Optional named arguments for image collection only: --start START Start date to filter image --end END End date to filter image --bandlist BANDLIST Bandlist we generated from bandtype export must be same bandtype --operator OPERATOR Use bb for Bounding box incase the geometry is complex or has too many vertices A typical setup would be ```gee2drive export --id \"COPERNICUS/S2\" --folder \"sentinel-export\" --aoi \"C:\\Users\\sam\\boulder.geojson\" --start \"2018-02-01\" --end \"2018-03-01\" --bandlist ['B2','B3','B4'] --operator \"bb\" --type \"collection\" Also as promised earlier , there is a way to add additional filters and then pass it through the export function here is how and I have included this in the Examples folder. This for example uses the Landsat collection but applies the Cloud cover filter before passing it for export ```python import ee import os import sys import gee2drive [head,tail]=os.path.split(gee2drive.__file__) os.chdir(head) sys.path.append(head) from export import exp ee.Initialize() exp(collection=ee.ImageCollection('LANDSAT/LC08/C01/T1_SR').filterMetadata('CLOUD_COVER','less_than',20), folderpath=\"l8-out\",start=\"2018-02-01\",end=\"2018-06-01\", geojson=r\"C:\\Users\\sam\\boulder.geojson\",bandnames=\"['B1','B2']\", operator=\"bb\",typ=\"ImageCollection\") Changelog \u00b6 v0.0.4 \u00b6 Can now parse gejson, json,kml Minor fixes and general improvements v0.0.3 \u00b6 Minor Fixes","title":"Google Earth Engine to Drive Export Manager"},{"location":"projects/gee2drive/#gee2drive-download-earth-engine-public-and-private-assets-to-google-drive","text":"Google Earth Engine currently allows you to export images and assets as either GeoTiff files or TFrecords. The system splits the files if the estimated size is greater than 2GB which is the upper limit and needs the geometry to be parsed in the form of either a fusion table, a user drawn geometry or a table imported into the user's assets. While the javascript frontend is great owing to the queryable catalog whereby you can search and and export your personal and private assets, the limitation lies in batch exports. To resolve this the python API access allows you to call batch export functions but now it is limited to checking for itersects first and running without having a queryable catalog. With the same idea I created this tool which allows you to run a terminal environment where your personal and general catalog images are part of a autosuggest feature. This tool allows you to look for images based on names for example \" you can search for Sentinel and it will show you full path of images which have the word sentinel in the title\". It also creates a report for your image collections and images so apart from the public datasets this can also find your own datasets as well. You can then generate bandlist to make sure all bands you are exporting are of the same type and then export all images that intersect you aoi. The assumption here is * Every image in the give image have the same band structure, choose the bandlist that you know to common to all images * If the geomery is too complex use the operator feature to use a bounding box instead. * For now all it filters is geometry and date, and it is does not filter based on metadata (however in the examples folder I have shown how to import and use additional filter before exporting an image collection) In the future I will try to integrate some other functionalities to this environment and you can indeed run the tool without the use of the autosuggest terminal as a simple CLI. Hence the terminal feature is optional.","title":"gee2drive: Download Earth Engine Public and Private assets to Google Drive"},{"location":"projects/gee2drive/#table-of-contents","text":"Installation Getting started Google Earth Engine to Drive Manager GEE to Google Drive CLI gee2drive Terminal gee2drive refresh gee2drive idsearch gee2drive bandtype gee2drive export","title":"Table of contents"},{"location":"projects/gee2drive/#installation","text":"This assumes that you have native python & pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying. This assumes that you are also well aware of Google Earth Engine Python setup and have it installed and authetenticated on your system. If not you can read about it here python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have tested this only on python 2.7.15 but can be easily modified for python 3. To install Python CLI for Digital Ocean you can install using two methods pip install gee2drive or you can also try git clone https://github.com/samapriya/gee2drive.git cd gee2drive python setup.py install Use might have to use sudo privileges Installation is an optional step; the application can be also run directly by executing gee2drive.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. If you don't want to install, browse into the gee2drive folder and try python gee2drive.py to get to the same result.","title":"Installation"},{"location":"projects/gee2drive/#getting-started","text":"As usual, to print help: usage: gee2drive [-h] {terminal,refresh,idsearch,bandtype,export} ... Google Earth Engine to Drive Exporter positional arguments: {terminal,refresh,idsearch,bandtype,export} terminal Starts the interactive terminal with autosuggest refresh Refreshes your personal asset list and GEE Asset list idsearch Does possible matches using asset name to give you asseth id/full path bandtype Prints bandtype and generates list to be used for export export Export Collections based on filter optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: gee2drive idsearch -h . If you didn't install gee2drive, then you can run it just by going to gee2drive directory and running python gee2drive.py [arguments go here]","title":"Getting started"},{"location":"projects/gee2drive/#gee-to-google-drive-cli","text":"This tool is designed to augment to the existing facilty of image export using a CLI, whereby you can pass it arguments to filter based on an area of interest geojson file, a start and end date for collection","title":"GEE to Google Drive CLI"},{"location":"projects/gee2drive/#gee2drive-terminal","text":"This is an autosuggestive terminal which uses the gee2add package to perform all of the functions but has autosuggest for Earth Engine catalog and your own personal catalog. This way you can get access to image id without needing the catalog id in the javascript codeeditor. usage: gee2drive terminal [-h] optional arguments: -h, --help show this help message and exit Once you type gee2drive terminal you get a shell inside your current terminal where you get autosuggest for image and have full functionality of the terminal.","title":"gee2drive Terminal"},{"location":"projects/gee2drive/#gee2drive-refresh","text":"For the past couple of months I have maintained a catalog of the most current Google Earth Engine assets , within their raster data catalog. I update this list every week. This tool downloads the most current version of this list, and also looks into your personal assets to generate your very own asset report which then serve as a master dataset to feed into autosuggestions. gee2drive refresh -h usage: gee2drive refresh [-h] optional arguments: -h, --help show this help message and exit","title":"gee2drive refresh"},{"location":"projects/gee2drive/#gee2drive-idsearch","text":"There is a possibility that you don't really remember the full path to your asset or the public asset. Fortunately when I parse and collect the image list and path for you they have names that are searchable so use a keyword. for example search using \"MODIS\" or \"sentinel\". Also it is not case sensitive, so you should be able to type \"SENTINEl\" or \"Sentinel\" or \"sentinel\" and it should still work gee2drive idsearch -h usage: gee2drive idsearch [-h] [--name NAME] optional arguments: -h, --help show this help message and exit --name NAME Name or part of name to search for","title":"gee2drive idsearch"},{"location":"projects/gee2drive/#gee2drive-bandtype","text":"Export requires all the bandtypes to be of the same kind. To do this, I simply generate the band types for you and you can select the band list you want , remember to paste it as a list. usage: gee2drive bandtype [-h] [--id ID] optional arguments: -h, --help show this help message and exit --id ID full path for collection or image","title":"gee2drive bandtype"},{"location":"projects/gee2drive/#gee2drive-export","text":"Finally the export tool, that lets you export an image or a collection clipped to your AOI. This makes use of the bandlist you exported. Incase you are exporting an image and not a collection you don't need a start and end date. The tool uses the bounds() function to use a bounding box incase the geometry has a complex geometry or too many vertices simply use the operator bb . If the geojson/json/kml keeps giving parsing error go to geojson.io usage: gee2drive export [-h] [--id ID] [--type TYPE] [--folder FOLDER] [--aoi AOI] [--start START] [--end END] [--bandlist BANDLIST] [--operator OPERATOR] optional arguments: -h, --help show this help message and exit --id ID Full path for collection or image --type TYPE Type whether image or collection --folder FOLDER Drive folder path --aoi AOI Full path to geojson/json/kml to be used for bounds Optional named arguments for image collection only: --start START Start date to filter image --end END End date to filter image --bandlist BANDLIST Bandlist we generated from bandtype export must be same bandtype --operator OPERATOR Use bb for Bounding box incase the geometry is complex or has too many vertices A typical setup would be ```gee2drive export --id \"COPERNICUS/S2\" --folder \"sentinel-export\" --aoi \"C:\\Users\\sam\\boulder.geojson\" --start \"2018-02-01\" --end \"2018-03-01\" --bandlist ['B2','B3','B4'] --operator \"bb\" --type \"collection\" Also as promised earlier , there is a way to add additional filters and then pass it through the export function here is how and I have included this in the Examples folder. This for example uses the Landsat collection but applies the Cloud cover filter before passing it for export ```python import ee import os import sys import gee2drive [head,tail]=os.path.split(gee2drive.__file__) os.chdir(head) sys.path.append(head) from export import exp ee.Initialize() exp(collection=ee.ImageCollection('LANDSAT/LC08/C01/T1_SR').filterMetadata('CLOUD_COVER','less_than',20), folderpath=\"l8-out\",start=\"2018-02-01\",end=\"2018-06-01\", geojson=r\"C:\\Users\\sam\\boulder.geojson\",bandnames=\"['B1','B2']\", operator=\"bb\",typ=\"ImageCollection\")","title":"gee2drive export"},{"location":"projects/gee2drive/#changelog","text":"","title":"Changelog"},{"location":"projects/gee2drive/#v004","text":"Can now parse gejson, json,kml Minor fixes and general improvements","title":"v0.0.4"},{"location":"projects/gee2drive/#v003","text":"Minor Fixes","title":"v0.0.3"},{"location":"projects/gee_asset_manager_addon/","text":"Google Earth Engine Batch Asset Manager with Addons \u00b6 Google Earth Engine Batch Asset Manager with Addons is an extension of the one developed by Lukasz here and additional tools were added to include functionality for moving assets, conversion of objects to fusion table, cleaning folders, querying tasks. The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. If you use this tool to download data for your research, and find this tool useful, star and cite it as below Samapriya Roy. (2019, October 6). samapriya/gee_asset_manager_addon: GEE Asset Manager with Addons (Version 0.3.3). Zenodo. http://doi.org/10.5281/zenodo.3474295 Table of contents \u00b6 Installation Getting started Uploading Usage examples EE User Asset List Asset Size Earth Engine Asset Report Task Query App to Script Task Report Delete a collection with content: Assets Move Assets Copy Assets Access Set Collection Property Cancel all tasks Installation \u00b6 We assume Earth Engine Python API is installed and EE authorised as desribed here . Quick installation pip install geeadd To install using github: git clone https://github.com/samapriya/gee_asset_manager_addon cd gee_asset_manager_addon && pip install -r requirements.txt python setup.py install Installation is an optional step; the application can be also run directly by executing geeadd.py script. The advantage of having it installed is being able to execute geeadd as any command line tool. I recommend installation within virtual environment. To install run python setup.py develop or python setup.py install In a linux distribution sudo python setup.py develop or sudo python setup.py install Getting started \u00b6 As usual, to print help: To obtain help for a specific functionality, simply call it with help switch, e.g.: geeadd upload -h . If you didn't install geeadd, then you can run it just by going to geeadd directory and running python geeadd.py [arguments go here] Uploading \u00b6 You can upload tables and rasters using geeup . This uses selenium to handle uploading and hence cannot be used in a headless environment. Usage examples \u00b6 EE User \u00b6 This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back Create \u00b6 This tool allows you to create a collection or folder in your earth engine root directory. The tool uses the system cli to achieve this and this has been included so as to reduce the need to switch between multiple tools and CLI. usage: geeadd.py create [-h] --typ TYP --path PATH optional arguments: -h, --help show this help message and exit --typ TYP Specify type: collection or folder --path PATH This is the path for the earth engine asset to be created full path is needsed eg: users/johndoe/collection Asset List \u00b6 This tool is designed to either print or output asset lists within folders or collections using earthengine ls tool functions. usage: geeadd.py lst [-h] --location LOCATION --typ TYP [--items ITEMS] [--output OUTPUT] optional arguments: -h, --help show this help message and exit Required named arguments.: --location LOCATION This it the location of your folder/collection --typ TYP Whether you want the list to be printed or output as text[print/report] Optional named arguments: --items ITEMS Number of items to list --output OUTPUT Folder location for report to be exported Asset Size \u00b6 This tool allows you to query the size of any Earth Engine asset[Images, Image Collections, Tables and Folders] and prints out the number of assets and total asset size in non-byte encoding meaning KB, MB, GB, TB depending on size. usage: geeadd assetsize [-h] --asset ASSET optional arguments: -h, --help show this help message and exit --asset ASSET Earth Engine Asset for which to get size properties App to Script \u00b6 This tool writes out or prints the underlying earthengine code for any public earthengine app. The tool has an option to export the code into a javascript file that you can then paste into Google Earth Engine code editor. geeadd app2script -h usage: geeadd app2script [-h] --url URL [--outfile OUTFILE] optional arguments: -h, --help show this help message and exit --url URL Earthengine app url Optional named arguments: --outfile OUTFILE Write the script out to a .js file: Open in any text editor Simple setup can be geeadd app2script --url \"https://gena.users.earthengine.app/view/urban-lights\" or write to a javascript file which you can then open with any text editor and paste in earthengine code editor geeadd app2script --url \"https://gena.users.earthengine.app/view/urban-lights\" --outfile \"Full path to javascript.js\" Earth Engine Asset Report \u00b6 This tool recursively goes through all your assets(Includes Images, ImageCollection,Table,) and generates a report containing the following fields [Type,Asset Type, Path,Number of Assets,size(MB),unit,owner,readers,writers]. usage: geeadd.py ee_report [-h] --outfile OUTFILE optional arguments: -h, --help show this help message and exit --outfile OUTFILE This it the location of your report csv file A simple setup is the following geeadd --outfile \"C:\\johndoe\\report.csv\" Task Query \u00b6 This script counts all currently running and ready tasks along with failed tasks. usage: geeadd.py tasks [-h] optional arguments: -h, --help show this help message and exit geeadd.py tasks Task Report \u00b6 Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type usage: geeadd taskreport [-h] [--r R] optional arguments: -h, --help show this help message and exit --r R Folder Path where the reports will be saved Delete a collection with content: \u00b6 The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution! geeadd delete users/johndoe/test Console output: 2016-07-17 16:14:09,212 :: oauth2client.client :: INFO :: Attempting refresh to obtain initial access_token 2016-07-17 16:14:09,213 :: oauth2client.client :: INFO :: Refreshing access_token 2016-07-17 16:14:10,842 :: root :: INFO :: Attempting to delete collection test 2016-07-17 16:14:16,898 :: root :: INFO :: Collection users/johndoe/test removed Delete all directories / collections based on a Unix-like pattern \u00b6 geeadd delete users/johndoe/*weird[0-9]?name* Assets Move \u00b6 This script allows us to recursively move assets from one collection to the other. usage: geeadd.py mover [-h] [--assetpath ASSETPATH] [--finalpath FINALPATH] optional arguments: -h, --help show this help message and exit --assetpath ASSETPATH Existing path of assets --finalpath FINALPATH New path for assets geeadd.py mover --assetpath \"users/johndoe/myfolder/myponycollection\" --destination \"users/johndoe/myfolder/myotherponycollection\" Assets Copy \u00b6 This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections. usage: geeadd.py copy [-h] [--initial INITIAL] [--final FINAL] optional arguments: -h, --help show this help message and exit --initial INITIAL Existing path of assets --final FINAL New path for assets geeadd.py mover --initial \"users/johndoe/myfolder/myponycollection\" --final \"users/johndoe/myfolder/myotherponycollection\" Assets Access \u00b6 This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time. usage: geeadd.py access [-h] --asset ASSET --user USER --role ROLE optional arguments: -h, --help show this help message and exit --asset ASSET This is the path to the earth engine asset whose permission you are changing folder/collection/image --user USER Full email address of the user, try using \"AllUsers\" to make it public --role ROLE Choose between reader, writer or delete Set Collection Property \u00b6 This script is derived from the ee tool to set collection properties and will set overall properties for collection. usage: geeadd.py collprop [-h] [--coll COLL] [--p P] optional arguments: -h, --help show this help message and exit --coll COLL Path of Image Collection --p P \"system:description=Description\"/\"system:provider_url=url\"/\"sys tem:tags=tags\"/\"system:title=title Cancel all tasks \u00b6 This is a simpler tool, can be called directly from the earthengine cli as well earthengine cli command earthengine task cancel all usage: geeadd.py cancel [-h] optional arguments: -h, --help show this help message and exit Changelog \u00b6 v0.3.3 \u00b6 General improvements Added tool to get underlying code from earthengine app v0.3.1 \u00b6 Updated list and asset size functions Updated function to generate earthengine asset report General optimization and improvements to distribution Better error handling v0.3.0 \u00b6 Removed upload function Upload handles by geeup General optimization and improvements to distribution Better error handling v0.2.8 \u00b6 Uses poster for streaming upload more stable with memory issues and large files Poster dependency limits use to Py 2.7 will fix in the new version v0.2.6 \u00b6 Major improvement to move, batch copy, and task reporting Major improvements to access tool to allow users read/write permission to entire Folder/collection. v0.2.5 \u00b6 Handles bandnames during upload thanks to Lukasz for original upload code Removed manifest option, that can be handled by seperate tool (ppipe) v0.2.3 \u00b6 Removing the initialization loop error v0.2.2 \u00b6 Added improvement to earthengine authorization v0.2.1 \u00b6 Added capability to handle PlanetScope 4Band Surface Reflectance Metadata Type General Improvements v0.2.0 \u00b6 Tool improvements and enhancements v0.1.9 \u00b6 New tool EE_Report was added v0.1.8 \u00b6 Fixed issues with install Dependencies now part of setup.py Updated Parser and general improvements","title":"Google Earth Engine Asset Manager Addon"},{"location":"projects/gee_asset_manager_addon/#google-earth-engine-batch-asset-manager-with-addons","text":"Google Earth Engine Batch Asset Manager with Addons is an extension of the one developed by Lukasz here and additional tools were added to include functionality for moving assets, conversion of objects to fusion table, cleaning folders, querying tasks. The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. If you use this tool to download data for your research, and find this tool useful, star and cite it as below Samapriya Roy. (2019, October 6). samapriya/gee_asset_manager_addon: GEE Asset Manager with Addons (Version 0.3.3). Zenodo. http://doi.org/10.5281/zenodo.3474295","title":"Google Earth Engine Batch Asset Manager with Addons"},{"location":"projects/gee_asset_manager_addon/#table-of-contents","text":"Installation Getting started Uploading Usage examples EE User Asset List Asset Size Earth Engine Asset Report Task Query App to Script Task Report Delete a collection with content: Assets Move Assets Copy Assets Access Set Collection Property Cancel all tasks","title":"Table of contents"},{"location":"projects/gee_asset_manager_addon/#installation","text":"We assume Earth Engine Python API is installed and EE authorised as desribed here . Quick installation pip install geeadd To install using github: git clone https://github.com/samapriya/gee_asset_manager_addon cd gee_asset_manager_addon && pip install -r requirements.txt python setup.py install Installation is an optional step; the application can be also run directly by executing geeadd.py script. The advantage of having it installed is being able to execute geeadd as any command line tool. I recommend installation within virtual environment. To install run python setup.py develop or python setup.py install In a linux distribution sudo python setup.py develop or sudo python setup.py install","title":"Installation"},{"location":"projects/gee_asset_manager_addon/#getting-started","text":"As usual, to print help: To obtain help for a specific functionality, simply call it with help switch, e.g.: geeadd upload -h . If you didn't install geeadd, then you can run it just by going to geeadd directory and running python geeadd.py [arguments go here]","title":"Getting started"},{"location":"projects/gee_asset_manager_addon/#uploading","text":"You can upload tables and rasters using geeup . This uses selenium to handle uploading and hence cannot be used in a headless environment.","title":"Uploading"},{"location":"projects/gee_asset_manager_addon/#usage-examples","text":"","title":"Usage examples"},{"location":"projects/gee_asset_manager_addon/#ee-user","text":"This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back","title":"EE User"},{"location":"projects/gee_asset_manager_addon/#create","text":"This tool allows you to create a collection or folder in your earth engine root directory. The tool uses the system cli to achieve this and this has been included so as to reduce the need to switch between multiple tools and CLI. usage: geeadd.py create [-h] --typ TYP --path PATH optional arguments: -h, --help show this help message and exit --typ TYP Specify type: collection or folder --path PATH This is the path for the earth engine asset to be created full path is needsed eg: users/johndoe/collection","title":"Create"},{"location":"projects/gee_asset_manager_addon/#asset-list","text":"This tool is designed to either print or output asset lists within folders or collections using earthengine ls tool functions. usage: geeadd.py lst [-h] --location LOCATION --typ TYP [--items ITEMS] [--output OUTPUT] optional arguments: -h, --help show this help message and exit Required named arguments.: --location LOCATION This it the location of your folder/collection --typ TYP Whether you want the list to be printed or output as text[print/report] Optional named arguments: --items ITEMS Number of items to list --output OUTPUT Folder location for report to be exported","title":"Asset List"},{"location":"projects/gee_asset_manager_addon/#asset-size","text":"This tool allows you to query the size of any Earth Engine asset[Images, Image Collections, Tables and Folders] and prints out the number of assets and total asset size in non-byte encoding meaning KB, MB, GB, TB depending on size. usage: geeadd assetsize [-h] --asset ASSET optional arguments: -h, --help show this help message and exit --asset ASSET Earth Engine Asset for which to get size properties","title":"Asset Size"},{"location":"projects/gee_asset_manager_addon/#app-to-script","text":"This tool writes out or prints the underlying earthengine code for any public earthengine app. The tool has an option to export the code into a javascript file that you can then paste into Google Earth Engine code editor. geeadd app2script -h usage: geeadd app2script [-h] --url URL [--outfile OUTFILE] optional arguments: -h, --help show this help message and exit --url URL Earthengine app url Optional named arguments: --outfile OUTFILE Write the script out to a .js file: Open in any text editor Simple setup can be geeadd app2script --url \"https://gena.users.earthengine.app/view/urban-lights\" or write to a javascript file which you can then open with any text editor and paste in earthengine code editor geeadd app2script --url \"https://gena.users.earthengine.app/view/urban-lights\" --outfile \"Full path to javascript.js\"","title":"App to Script"},{"location":"projects/gee_asset_manager_addon/#earth-engine-asset-report","text":"This tool recursively goes through all your assets(Includes Images, ImageCollection,Table,) and generates a report containing the following fields [Type,Asset Type, Path,Number of Assets,size(MB),unit,owner,readers,writers]. usage: geeadd.py ee_report [-h] --outfile OUTFILE optional arguments: -h, --help show this help message and exit --outfile OUTFILE This it the location of your report csv file A simple setup is the following geeadd --outfile \"C:\\johndoe\\report.csv\"","title":"Earth Engine Asset Report"},{"location":"projects/gee_asset_manager_addon/#task-query","text":"This script counts all currently running and ready tasks along with failed tasks. usage: geeadd.py tasks [-h] optional arguments: -h, --help show this help message and exit geeadd.py tasks","title":"Task Query"},{"location":"projects/gee_asset_manager_addon/#task-report","text":"Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type usage: geeadd taskreport [-h] [--r R] optional arguments: -h, --help show this help message and exit --r R Folder Path where the reports will be saved","title":"Task Report"},{"location":"projects/gee_asset_manager_addon/#delete-a-collection-with-content","text":"The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution! geeadd delete users/johndoe/test Console output: 2016-07-17 16:14:09,212 :: oauth2client.client :: INFO :: Attempting refresh to obtain initial access_token 2016-07-17 16:14:09,213 :: oauth2client.client :: INFO :: Refreshing access_token 2016-07-17 16:14:10,842 :: root :: INFO :: Attempting to delete collection test 2016-07-17 16:14:16,898 :: root :: INFO :: Collection users/johndoe/test removed","title":"Delete a collection with content:"},{"location":"projects/gee_asset_manager_addon/#delete-all-directories-collections-based-on-a-unix-like-pattern","text":"geeadd delete users/johndoe/*weird[0-9]?name*","title":"Delete all directories / collections based on a Unix-like pattern"},{"location":"projects/gee_asset_manager_addon/#assets-move","text":"This script allows us to recursively move assets from one collection to the other. usage: geeadd.py mover [-h] [--assetpath ASSETPATH] [--finalpath FINALPATH] optional arguments: -h, --help show this help message and exit --assetpath ASSETPATH Existing path of assets --finalpath FINALPATH New path for assets geeadd.py mover --assetpath \"users/johndoe/myfolder/myponycollection\" --destination \"users/johndoe/myfolder/myotherponycollection\"","title":"Assets Move"},{"location":"projects/gee_asset_manager_addon/#assets-copy","text":"This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections. usage: geeadd.py copy [-h] [--initial INITIAL] [--final FINAL] optional arguments: -h, --help show this help message and exit --initial INITIAL Existing path of assets --final FINAL New path for assets geeadd.py mover --initial \"users/johndoe/myfolder/myponycollection\" --final \"users/johndoe/myfolder/myotherponycollection\"","title":"Assets Copy"},{"location":"projects/gee_asset_manager_addon/#assets-access","text":"This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time. usage: geeadd.py access [-h] --asset ASSET --user USER --role ROLE optional arguments: -h, --help show this help message and exit --asset ASSET This is the path to the earth engine asset whose permission you are changing folder/collection/image --user USER Full email address of the user, try using \"AllUsers\" to make it public --role ROLE Choose between reader, writer or delete","title":"Assets Access"},{"location":"projects/gee_asset_manager_addon/#set-collection-property","text":"This script is derived from the ee tool to set collection properties and will set overall properties for collection. usage: geeadd.py collprop [-h] [--coll COLL] [--p P] optional arguments: -h, --help show this help message and exit --coll COLL Path of Image Collection --p P \"system:description=Description\"/\"system:provider_url=url\"/\"sys tem:tags=tags\"/\"system:title=title","title":"Set Collection Property"},{"location":"projects/gee_asset_manager_addon/#cancel-all-tasks","text":"This is a simpler tool, can be called directly from the earthengine cli as well earthengine cli command earthengine task cancel all usage: geeadd.py cancel [-h] optional arguments: -h, --help show this help message and exit","title":"Cancel all tasks"},{"location":"projects/gee_asset_manager_addon/#changelog","text":"","title":"Changelog"},{"location":"projects/gee_asset_manager_addon/#v033","text":"General improvements Added tool to get underlying code from earthengine app","title":"v0.3.3"},{"location":"projects/gee_asset_manager_addon/#v031","text":"Updated list and asset size functions Updated function to generate earthengine asset report General optimization and improvements to distribution Better error handling","title":"v0.3.1"},{"location":"projects/gee_asset_manager_addon/#v030","text":"Removed upload function Upload handles by geeup General optimization and improvements to distribution Better error handling","title":"v0.3.0"},{"location":"projects/gee_asset_manager_addon/#v028","text":"Uses poster for streaming upload more stable with memory issues and large files Poster dependency limits use to Py 2.7 will fix in the new version","title":"v0.2.8"},{"location":"projects/gee_asset_manager_addon/#v026","text":"Major improvement to move, batch copy, and task reporting Major improvements to access tool to allow users read/write permission to entire Folder/collection.","title":"v0.2.6"},{"location":"projects/gee_asset_manager_addon/#v025","text":"Handles bandnames during upload thanks to Lukasz for original upload code Removed manifest option, that can be handled by seperate tool (ppipe)","title":"v0.2.5"},{"location":"projects/gee_asset_manager_addon/#v023","text":"Removing the initialization loop error","title":"v0.2.3"},{"location":"projects/gee_asset_manager_addon/#v022","text":"Added improvement to earthengine authorization","title":"v0.2.2"},{"location":"projects/gee_asset_manager_addon/#v021","text":"Added capability to handle PlanetScope 4Band Surface Reflectance Metadata Type General Improvements","title":"v0.2.1"},{"location":"projects/gee_asset_manager_addon/#v020","text":"Tool improvements and enhancements","title":"v0.2.0"},{"location":"projects/gee_asset_manager_addon/#v019","text":"New tool EE_Report was added","title":"v0.1.9"},{"location":"projects/gee_asset_manager_addon/#v018","text":"Fixed issues with install Dependencies now part of setup.py Updated Parser and general improvements","title":"v0.1.8"},{"location":"projects/gee_takeout/","text":"Google Takeout and Transfer: Tools and Guide for Code and Asset Transfer \u00b6 Command Line Interface Allows you to copy all codes and assets from one Google account to another Note: This is something that I have tested and have designed only for a windows machine with python 2.7.14 but can be easily ported into an different operating system. Use these tool and steps at your own risk and backup your scripts always just in case. If you still want to proceed which I assume you do in case you are still reading, I am including descriptions links to the tool I made and the steps I used to achieve the same. The tool is a single command line interface with three sections. Before you do this make sure of a few things Both your google accounts have an external password, since it requires that to download and perform a lot of the operations. Also enable __Let Less Secure App use your account_ on both these accounts._ Your system has native python available in terminal or command prompt depending on what kind of system you are using. You can check this by typing python --version _Git is installed on your system. For windows you can find __installation here_ _Earth Engine Command Line(earthengine cli) interface is installed, instructions are in the __developer page_ _You have authenticated earthengine cli using _ earthengine authenticate Make sure you visit the __git source for your account_ within earth engine and allow access._ Check git is accessible via your system path type git help and check if the system can reach installed git command line tools. Now we setup and install the tool by running the following set of steps. The Github repository containing this tool and codes can be found here For windows download the zip here and after extraction go to the folder containing \"setup.py\" and open command prompt at that location and type python setup.py install pip install -r requirements.txt Now call the tool for the first time, by typing in geetakeout -h . This will only work if you have python in the system path which you can test for opening up terminal or command prompt and typing **python**. If you want, you don't have to install the tool to use it you can browse to the folder geetakeout inside the main zipped folder. You can then call the tool by simple python geetakeout.py -h The housekeeping and credential setup is optional since most of you have probably installed the earthengine cli and authenticated it using **earthengine authenticate**. > Anatomy of the Process: How to transfer step by step Getting first things out of the way is to understand the three sections of this tool. To make life and this process simpler I designed the tool to have a flow so you can run these tools one after the other. The EE Setup and Housekeeping sections are optional , since I will generally updated the selenium driver for mozilla and it assumes you have authenticated your earthengine CLI . The tool might show an error if you have not authenticated using earthengine authenticate If you have installed the tool run geetakeout -h If you have migrated into the folder python geetakeout.py -h The GEE Takeout Tool CLI printout Setting up the case study For this blog I decided to make the transfer simple I have a university account but since my university if shifting umail services to a google app service it means my domain would change from @umail .iu.edu to @iu .edu which are separate accounts. I created the iu.edu GEE account recently. Code Editor comparison Left(my @umail account and right my @iu account) This also means that the root path for my home folder and repository are different. The idea is simple to be able to replicate the codes and assets from one account to the other. This includes every type if assets including collections but also making sure that the structure of the folders are same. That being said you will still have to change the home path in the codes but if the structure is same then only a single root-path changes. Comparing the root path and assets folder for both accounts So now that we have a setup, I am going to approach it step by step and have a walk through to explain the process better. Step 1: Getting your Repository Lists(gee_repo) This assumes that you have visited the Git Source for your codes in Google Earth Engine and authorized it. If not allow it here and then you are set to download your repo contents and perform git operations. The tool is setup for accessing all repositories that are shared with you. This downloads the list into an html file which can then be parsed for your repositories. Create GEE Repo List Step 2: Setting up your Git with Earth Engine Credentials(git_auth) You can do this using two methods The first simple includes you visiting your gitsource account page that we accessed earlier and click **Generate Password ** and follow instructions. I am going to talk more about the second method because this eliminates the need to get the password again and again since it is save as passkey. This will authenticate your git client with your git password using a browser less login and also store your gitkey GIT Auth (saves git key) We will use this again to setup our second account post authorization. This will print our gitkey location and make sure you copy that so you can swap in out as needed. Note the name of the key is in the format **git-\"username\"** ** **in this case it is **git-roysam** Step 3: Authorize your Git Client with Git Key(git_swap) The next step is to use the saved gitkey to authorize the git client. We are setting everything up so that we can clone the repositories to which we have access. Authorize using the git key stored earlier Step 4: Clone your repositories(git_clone) This tool makes use of your earlier created GIT list, now that your git client has been authorized in step 2, you should be able to download your repos. This tool uses the account already linked to your terminal account. If you are not sure try **earthengine ls** to see your username. The export path is noted for the collection of repositories. Cloning your Git Repositories Step 5: Working with Assets: Generating Asset Report(ee_report) This includes all your assets , including tables, images, image collections and folders. We need to make sure we have this list to work on copying over your assets to the secondary account. Running this is simple and just requires a location for the csv file (the full path). Running Earth Engine Reports The output is a csv file consiting of the type of asset and the asset path to be replicated in the new account. And now that we have the list time to get permissions to copy these assets. Step 6: Setting Permissions to Assets(ee_permissions) We now use the report file generated to grant read access to all assets in your account. Once this is completed you will be able to copy your assets apart from being able to copy your codes. Getting permissions to assets Let us Begin to Copy : We change gears and switch over to the destination account Step 7: Setting up the Destination Account(ee_user and git_swap) Now we have to do two steps one after the other, do a quick earthengine authenticate and authenticate to your new account and perform Step 2 and Step 3 this time using your new account. The tool **ee_user** will also allow you to change your accounts. I already created Step 2 for my secondary account and now I will use that to authorize my git client with the new account. Change your earthengine authentication and also validate your git key Now we authorized the git client with the second key Step 8: Replicate Repositories (git_create) To setup our new account we need to build the outline of the earlier account, the repolists and folders inside these repos and then similary the folders and empty collections in the secondary account. Note: Git cannot push an empty repository so if you have an empty repository delete it before downloading and pushing to new account Git Create your folder based on your earlier account The repo lists now look similar Repo created on secondary account Step 9: Push to New Account(git_replicate) Now we push all codes from our earlier account to our new account, this way our repositories will now be populated with the most recent codes. \"Do not push to any repository that already has code because this will overwrite it\" git_replicate to new account Step 10: Replicate Asset Structure(asset_create) and Assets (asset_replicate) This is similar to git_create here we replicate the collection and folder structure so we can push our assets to them. You pass it the original report you created from your primary account and it sets up as needed. Creates asset structure (folders and collections) This has replicated the collection and Image folder structures. Asset Collections and Folders have been created(Left: asset home before asset_create Right: asset home after asset_create) However this is still empty and the last step makes sure that your assets are actually copied over to your new asset home. I have included a counter to measure transfers left incase it is a large collection. Asset Replication: Copying assets to your home folder The final results is your assets and codes all copied, you will still have to edit codes to change your path as needed but for now we have replicated an Earth Engine account into a new location. Replicated Assets on Both Accounts Copied from Left to Right There you go, over the last 10 steps we have managed to replicate and move an earth engine account from one place to another. Though I found this useful to move accounts within a university setting, I see some value in moving accounts and replicating when a project member leaves a project or for simply migrating at large. For now if an owner of an account deletes his/her account or looses access to his/her account and even if you are a writer to the repository and the collection, you will loose access to these codes and assets. So this can aid in maintaining continuity by moving codes to more persistent account. Though I have not tested this tools in a linux setting, these setup tools can be adapted and used easily in that framework, since I have tested the individual components in such setups.","title":"Google Earth Engine Takeout"},{"location":"projects/gee_takeout/#google-takeout-and-transfer-tools-and-guide-for-code-and-asset-transfer","text":"Command Line Interface Allows you to copy all codes and assets from one Google account to another Note: This is something that I have tested and have designed only for a windows machine with python 2.7.14 but can be easily ported into an different operating system. Use these tool and steps at your own risk and backup your scripts always just in case. If you still want to proceed which I assume you do in case you are still reading, I am including descriptions links to the tool I made and the steps I used to achieve the same. The tool is a single command line interface with three sections. Before you do this make sure of a few things Both your google accounts have an external password, since it requires that to download and perform a lot of the operations. Also enable __Let Less Secure App use your account_ on both these accounts._ Your system has native python available in terminal or command prompt depending on what kind of system you are using. You can check this by typing python --version _Git is installed on your system. For windows you can find __installation here_ _Earth Engine Command Line(earthengine cli) interface is installed, instructions are in the __developer page_ _You have authenticated earthengine cli using _ earthengine authenticate Make sure you visit the __git source for your account_ within earth engine and allow access._ Check git is accessible via your system path type git help and check if the system can reach installed git command line tools. Now we setup and install the tool by running the following set of steps. The Github repository containing this tool and codes can be found here For windows download the zip here and after extraction go to the folder containing \"setup.py\" and open command prompt at that location and type python setup.py install pip install -r requirements.txt Now call the tool for the first time, by typing in geetakeout -h . This will only work if you have python in the system path which you can test for opening up terminal or command prompt and typing **python**. If you want, you don't have to install the tool to use it you can browse to the folder geetakeout inside the main zipped folder. You can then call the tool by simple python geetakeout.py -h The housekeeping and credential setup is optional since most of you have probably installed the earthengine cli and authenticated it using **earthengine authenticate**. > Anatomy of the Process: How to transfer step by step Getting first things out of the way is to understand the three sections of this tool. To make life and this process simpler I designed the tool to have a flow so you can run these tools one after the other. The EE Setup and Housekeeping sections are optional , since I will generally updated the selenium driver for mozilla and it assumes you have authenticated your earthengine CLI . The tool might show an error if you have not authenticated using earthengine authenticate If you have installed the tool run geetakeout -h If you have migrated into the folder python geetakeout.py -h The GEE Takeout Tool CLI printout Setting up the case study For this blog I decided to make the transfer simple I have a university account but since my university if shifting umail services to a google app service it means my domain would change from @umail .iu.edu to @iu .edu which are separate accounts. I created the iu.edu GEE account recently. Code Editor comparison Left(my @umail account and right my @iu account) This also means that the root path for my home folder and repository are different. The idea is simple to be able to replicate the codes and assets from one account to the other. This includes every type if assets including collections but also making sure that the structure of the folders are same. That being said you will still have to change the home path in the codes but if the structure is same then only a single root-path changes. Comparing the root path and assets folder for both accounts So now that we have a setup, I am going to approach it step by step and have a walk through to explain the process better. Step 1: Getting your Repository Lists(gee_repo) This assumes that you have visited the Git Source for your codes in Google Earth Engine and authorized it. If not allow it here and then you are set to download your repo contents and perform git operations. The tool is setup for accessing all repositories that are shared with you. This downloads the list into an html file which can then be parsed for your repositories. Create GEE Repo List Step 2: Setting up your Git with Earth Engine Credentials(git_auth) You can do this using two methods The first simple includes you visiting your gitsource account page that we accessed earlier and click **Generate Password ** and follow instructions. I am going to talk more about the second method because this eliminates the need to get the password again and again since it is save as passkey. This will authenticate your git client with your git password using a browser less login and also store your gitkey GIT Auth (saves git key) We will use this again to setup our second account post authorization. This will print our gitkey location and make sure you copy that so you can swap in out as needed. Note the name of the key is in the format **git-\"username\"** ** **in this case it is **git-roysam** Step 3: Authorize your Git Client with Git Key(git_swap) The next step is to use the saved gitkey to authorize the git client. We are setting everything up so that we can clone the repositories to which we have access. Authorize using the git key stored earlier Step 4: Clone your repositories(git_clone) This tool makes use of your earlier created GIT list, now that your git client has been authorized in step 2, you should be able to download your repos. This tool uses the account already linked to your terminal account. If you are not sure try **earthengine ls** to see your username. The export path is noted for the collection of repositories. Cloning your Git Repositories Step 5: Working with Assets: Generating Asset Report(ee_report) This includes all your assets , including tables, images, image collections and folders. We need to make sure we have this list to work on copying over your assets to the secondary account. Running this is simple and just requires a location for the csv file (the full path). Running Earth Engine Reports The output is a csv file consiting of the type of asset and the asset path to be replicated in the new account. And now that we have the list time to get permissions to copy these assets. Step 6: Setting Permissions to Assets(ee_permissions) We now use the report file generated to grant read access to all assets in your account. Once this is completed you will be able to copy your assets apart from being able to copy your codes. Getting permissions to assets Let us Begin to Copy : We change gears and switch over to the destination account Step 7: Setting up the Destination Account(ee_user and git_swap) Now we have to do two steps one after the other, do a quick earthengine authenticate and authenticate to your new account and perform Step 2 and Step 3 this time using your new account. The tool **ee_user** will also allow you to change your accounts. I already created Step 2 for my secondary account and now I will use that to authorize my git client with the new account. Change your earthengine authentication and also validate your git key Now we authorized the git client with the second key Step 8: Replicate Repositories (git_create) To setup our new account we need to build the outline of the earlier account, the repolists and folders inside these repos and then similary the folders and empty collections in the secondary account. Note: Git cannot push an empty repository so if you have an empty repository delete it before downloading and pushing to new account Git Create your folder based on your earlier account The repo lists now look similar Repo created on secondary account Step 9: Push to New Account(git_replicate) Now we push all codes from our earlier account to our new account, this way our repositories will now be populated with the most recent codes. \"Do not push to any repository that already has code because this will overwrite it\" git_replicate to new account Step 10: Replicate Asset Structure(asset_create) and Assets (asset_replicate) This is similar to git_create here we replicate the collection and folder structure so we can push our assets to them. You pass it the original report you created from your primary account and it sets up as needed. Creates asset structure (folders and collections) This has replicated the collection and Image folder structures. Asset Collections and Folders have been created(Left: asset home before asset_create Right: asset home after asset_create) However this is still empty and the last step makes sure that your assets are actually copied over to your new asset home. I have included a counter to measure transfers left incase it is a large collection. Asset Replication: Copying assets to your home folder The final results is your assets and codes all copied, you will still have to edit codes to change your path as needed but for now we have replicated an Earth Engine account into a new location. Replicated Assets on Both Accounts Copied from Left to Right There you go, over the last 10 steps we have managed to replicate and move an earth engine account from one place to another. Though I found this useful to move accounts within a university setting, I see some value in moving accounts and replicating when a project member leaves a project or for simply migrating at large. For now if an owner of an account deletes his/her account or looses access to his/her account and even if you are a writer to the repository and the collection, you will loose access to these codes and assets. So this can aid in maintaining continuity by moving codes to more persistent account. Though I have not tested this tools in a linux setting, these setup tools can be adapted and used easily in that framework, since I have tested the individual components in such setups.","title":"Google Takeout and Transfer: Tools and Guide for Code and Asset Transfer"},{"location":"projects/geeup/","text":"geeup: Simple CLI for Earth Engine Uploads with Selenium Support \u00b6 This tool came of the simple need to handle batch uploads of both image assets to collections but also thanks to the new table feature the possibility of batch uploading shapefiles into a folder. Though a lot of these tools including batch image uploader is part of my other project geeadd which also includes additional features to add to the python CLI, this tool was designed to be minimal so as to allow the user to simply query their quota, upload images or tables and also to query ongoing tasks and delete assets. I am hoping this tool with a simple objective proves useful to a few users of Google Earth Engine. -If you find this tool useful, star and cite it as below Samapriya Roy. (2019, August 16). samapriya/geeup: geeup: Simple CLI for Earth Engine Uploads (Version 0.2.5). Zenodo. http://doi.org/10.5281/zenodo.3369484 Table of contents \u00b6 Installation Windows Setup Getting started geeup Simple CLI for Earth Engine Uploads geeup init gee Quota gee getmeta gee Zipshape gee upload gee seltabup gee selsetup gee tasks gee delete Installation \u00b6 This assumes that you have native python & pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have tested this only on python 2.7.15, but it should run on Python 3. This command line tool is dependent on functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa && sudo apt-get update sudo apt-get install gdal-bin sudo apt-get install python-gdal Windows Setup \u00b6 Shapely and a few other libraries are notoriously difficult to install on windows machines so follow the steps mentioned here before installing porder . You can download and install shapely and other libraries from the Unofficial Wheel files from here download depending on the python version you have. Do this only once you have install GDAL . I would recommend the steps mentioned above to get the GDAL properly installed. However I am including instructions to using a precompiled version of GDAL similar to the other libraries on windows. You can test to see if you have gdal by simply running gdalinfo in your command prompt. If you get a read out and not an error message you are good to go. If you don't have gdal try Option 1,2 or 3 in that order and that will install gdal along with the other libraries Option 1: \u00b6 Starting from porder v0.4.5 onwards: Simply run geeup -h after installation. This should go fetch the extra libraries you need and install them. Once installation is complete, the porder help page will show up. This should save you from the few steps below. Option 2: \u00b6 If this does not work or you get an unexpected error try the following commands. You can also use these commands if you simply want to update these libraries. pipwin refresh pipwin install gdal Option 3 \u00b6 For Windows I also found this guide from UCLA Also for Ubuntu Linux I saw that this is necessary before the install sudo apt install libcurl4-openssl-dev libssl-dev This also needs earthengine cli to be installed and authenticated on your system and earthengine to be callable in your command line or terminal To install geeup: Simple CLI for Earth Engine Uploads you can install using two methods. pip install geeup or you can also try git clone https://github.com/samapriya/geeup.git cd geeup python setup.py install For Linux use sudo or try pip install geeup --user . Installation is an optional step; the application can also be run directly by executing geeup.py script. The advantage of having it installed is that geeup can be executed as any command line tool. I recommend installation within a virtual environment. If you don't want to install, browse into the geeup folder and try python geeup.py to get to the same result. Getting started \u00b6 As usual, to print help: usage: geeup.py [-h] {update,quota,zipshape,upload,selupload,seltabup,tasks,delete} ... Simple Client for Earth Engine Uploads with Selenium Support positional arguments: {update,quota,zipshape,upload,selupload,seltabup,tasks,delete} update Updates Selenium drivers for firefox quota Print Earth Engine total quota and used quota zipshape Zips all shapefiles and subsidary files into individual zip files getmeta Generates generalized metadata for all rasters in folder upload Batch Asset Uploader using Selenium seltabup Batch Table Uploader using Selenium. selsetup Non headless setup for new google account, use if upload throws errors tasks Queries current task status [completed,running,ready,failed,cancelled] delete Deletes collection and all items inside. Supports Unix-like wildcards. optional arguments: -h, --help show this help message and exit To obtain help for specific functionality, simply call it with help switch, e.g.: geeup zipshape -h . If you didn't install geeup, then you can run it just by going to geeup directory and running python geeup.py [arguments go here] geeup Simple CLI for Earth Engine Uploads \u00b6 The tool is designed to handle batch uploading of images and tables(shapefiles). While there are image collection where you can batch upload imagery, for vector or shapefiles you have to batch upload them to a folder. geeup init \u00b6 This is a key step since all upload function depends on this step, so make sure you run this . This downloads selenium driver and places to your local directory for windows and Linux subsystems. This is the first step to use selenium supported upload. geeup init gee Quota \u00b6 Just a simple tool to print your earth engine quota quickly. usage: geeup quota [-h] optional arguments: -h, --help show this help message and exit gee Zipshape \u00b6 So here's how table upload in Google Earth Engine works, you can either upload the component files shp, shx, prj and dbf or you can zip these files together and upload it as a single file. The pros for this is that it reduces the overall size of the shapefile after zipping them along, this tool looks for the shp file and finds the subsidiary files and zips them ready for upload. It also helps when you have limited upload bandwidth. Cons you have to create a replicate structure of the file system, but it saves on bandwidth and auto-arranges your files so you don't have to look for each additional file. usage: geeup zipshape [-h] --input INPUT --output OUTPUT optional arguments: -h, --help show this help message and exit Required named arguments.: --input INPUT Path to the input directory with all shape files --output OUTPUT Destination folder Full path where shp, shx, prj and dbf files if present in input will be zipped and stored gee getmeta \u00b6 This script generates a generalized metadata using information parsed from gdalinfo and metadata properties. For now it generates metadata with image name, x and y dimension of images, the pixel resolution and the number of bands. usage: geeup getmeta [-h] --input INPUT --metadata METADATA optional arguments: -h, --help show this help message and exit Required named arguments.: --input INPUT Path to the input directory with all raster files --metadata METADATA Full path to export metadata.csv file gee upload \u00b6 The script creates an Image Collection from GeoTIFFs in your local directory. By default, the image name in the collection is the same as the local directory name; with the optional parameter you can provide a different name. usage: geeup upload [-h] --source SOURCE --dest DEST -m METADATA [--nodata NODATA] [-u USER] optional arguments: -h, --help show this help message and exit Required named arguments.: --source SOURCE Path to the directory with images for upload. --dest DEST Destination. Full path for upload to Google Earth Engine, e.g. users/pinkiepie/myponycollection -m METADATA, --metadata METADATA Path to CSV with metadata. -u USER, --user USER Google account name (gmail address). Optional named arguments: --nodata NODATA The value to burn into the raster as NoData (missing data) gee seltabup \u00b6 This tool allows you to batch download tables/shapefiles to a folder. It uses a modified version of the image upload and a wrapper around the earthengine upload cli to achieve this while creating folders if they don't exist and reporting on assets and checking on uploads. This only requires a source, destination and your ee authenticated email address. This tool also uses selenium to upload the tables. usage: geeup seltabup [-h] --source SOURCE --dest DEST [-u USER] optional arguments: -h, --help show this help message and exit Required named arguments.: --source SOURCE Path to the directory with zipped folder for upload. --dest DEST Destination. Full path for upload to Google Earth Engine, e.g. users/pinkiepie/myponycollection -u USER, --user USER Google account name (gmail address). gee selsetup \u00b6 Once in a while the geckodriver requires manual input before signing into the google earth engine, this tool will allow you to interact with the initialization of Google Earth Engine code editor window. It allows the user to specify the account they want to use, and should only be needed once. geeup selsetup gee tasks \u00b6 This script counts all currently running, ready, completed, failed and canceled tasks along with failed tasks. This tool is linked to your google earth engine account with which you initialized the earth engine client. This takes no argument. usage: geeup tasks [-h] optional arguments: -h, --help show this help message and exit gee delete \u00b6 The delete is recursive, meaning it will also delete all children assets: images, collections, and folders. Use with caution! usage: geeup delete [-h] id positional arguments: id Full path to asset for deletion. Recursively removes all folders, collections and images. optional arguments: -h, --help show this help message and exit Changelog \u00b6 v0.3.2 \u00b6 Fixed issue with selsetup. v0.3.1 \u00b6 Fixed issue with raw_input and input for selsetup. Fixed selenium path for windows and other platforms. General improvements to ReadMe v0.3.0 \u00b6 Fixed (issue 13)[ https://github.com/samapriya/geeup/issues/13 ] non relative import. Fixed issues with package import. v0.2.9 \u00b6 Fixed issues caused by --no-use_cloud_api in earthengine-api package v0.2.7 \u00b6 Fix to handle case senstive platform type for all os Fix to Issue 11 v0.2.6 \u00b6 Fixed geckodriver path to handle macos Fix to Issue 10 v0.2.5 \u00b6 Now allows for downloading geckodriver for macos Fix to Issue 10 Now includes a metadata tool to generate a generalized metadata for any raster to allow upload. Fix to Issue 7 Changed from geeup update to init to signify initialization Added selsetup this tool allows for setting up the gecko driver with your account incase there are issues uploading Better error handling for selenium driver download v0.2.4 \u00b6 Made general improvements Better error handling for selenium driver download v0.2.2 \u00b6 Can now handle generalized metadata (metadata is now required field) Fixed issues with table upload Overall code optimization and handle streaming upload v0.1.9 \u00b6 Changes to handle PyDL installation for Py2 and Py3 Removed Planet uploader to make tool more generalized v0.1.8 \u00b6 Multipart encoder using requests toolbelt for streaming upload Changed manifest upload methodology to match changes in earthengine-api v0.1.6 \u00b6 Fixed issue with module locations v0.1.5 \u00b6 Fixed issue with gecko driver paths Fixed issue with null uploads using task, switched to ee CLI upload v0.1.4 \u00b6 OS based geckdriver path fix General improvements v0.1.3 \u00b6 fixed issues with extra arguments Upload issue resolved General dependency v0.1.1 \u00b6 fixed dependency issues Upload post issues resolved Removed dependency on poster for now v0.0.9 \u00b6 fixed attribution and dependecy issues Included poster to improve streaming uploads All uploads now use selenium v0.0.8 \u00b6 fixed issues with unused imports v0.0.7 \u00b6 fixed issues with manifest lib v0.0.6 \u00b6 Detailed quota readout Uses selenium based uploader to upload images Avoids issues with python auth for upload v0.0.5 \u00b6 Removed unnecessary library imports Minor improvements and updated readme v0.0.4 \u00b6 Improved valid table name check before upload Improvements to earth engine quota tool for more accurate quota and human readable","title":"Simple CLI for Google Earth Engine Uploads"},{"location":"projects/geeup/#geeup-simple-cli-for-earth-engine-uploads-with-selenium-support","text":"This tool came of the simple need to handle batch uploads of both image assets to collections but also thanks to the new table feature the possibility of batch uploading shapefiles into a folder. Though a lot of these tools including batch image uploader is part of my other project geeadd which also includes additional features to add to the python CLI, this tool was designed to be minimal so as to allow the user to simply query their quota, upload images or tables and also to query ongoing tasks and delete assets. I am hoping this tool with a simple objective proves useful to a few users of Google Earth Engine. -If you find this tool useful, star and cite it as below Samapriya Roy. (2019, August 16). samapriya/geeup: geeup: Simple CLI for Earth Engine Uploads (Version 0.2.5). Zenodo. http://doi.org/10.5281/zenodo.3369484","title":"geeup: Simple CLI for Earth Engine Uploads with Selenium Support &nbsp;"},{"location":"projects/geeup/#table-of-contents","text":"Installation Windows Setup Getting started geeup Simple CLI for Earth Engine Uploads geeup init gee Quota gee getmeta gee Zipshape gee upload gee seltabup gee selsetup gee tasks gee delete","title":"Table of contents"},{"location":"projects/geeup/#installation","text":"This assumes that you have native python & pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have tested this only on python 2.7.15, but it should run on Python 3. This command line tool is dependent on functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa && sudo apt-get update sudo apt-get install gdal-bin sudo apt-get install python-gdal","title":"Installation"},{"location":"projects/geeup/#windows-setup","text":"Shapely and a few other libraries are notoriously difficult to install on windows machines so follow the steps mentioned here before installing porder . You can download and install shapely and other libraries from the Unofficial Wheel files from here download depending on the python version you have. Do this only once you have install GDAL . I would recommend the steps mentioned above to get the GDAL properly installed. However I am including instructions to using a precompiled version of GDAL similar to the other libraries on windows. You can test to see if you have gdal by simply running gdalinfo in your command prompt. If you get a read out and not an error message you are good to go. If you don't have gdal try Option 1,2 or 3 in that order and that will install gdal along with the other libraries","title":"Windows Setup"},{"location":"projects/geeup/#option-1","text":"Starting from porder v0.4.5 onwards: Simply run geeup -h after installation. This should go fetch the extra libraries you need and install them. Once installation is complete, the porder help page will show up. This should save you from the few steps below.","title":"Option 1:"},{"location":"projects/geeup/#option-2","text":"If this does not work or you get an unexpected error try the following commands. You can also use these commands if you simply want to update these libraries. pipwin refresh pipwin install gdal","title":"Option 2:"},{"location":"projects/geeup/#option-3","text":"For Windows I also found this guide from UCLA Also for Ubuntu Linux I saw that this is necessary before the install sudo apt install libcurl4-openssl-dev libssl-dev This also needs earthengine cli to be installed and authenticated on your system and earthengine to be callable in your command line or terminal To install geeup: Simple CLI for Earth Engine Uploads you can install using two methods. pip install geeup or you can also try git clone https://github.com/samapriya/geeup.git cd geeup python setup.py install For Linux use sudo or try pip install geeup --user . Installation is an optional step; the application can also be run directly by executing geeup.py script. The advantage of having it installed is that geeup can be executed as any command line tool. I recommend installation within a virtual environment. If you don't want to install, browse into the geeup folder and try python geeup.py to get to the same result.","title":"Option 3"},{"location":"projects/geeup/#getting-started","text":"As usual, to print help: usage: geeup.py [-h] {update,quota,zipshape,upload,selupload,seltabup,tasks,delete} ... Simple Client for Earth Engine Uploads with Selenium Support positional arguments: {update,quota,zipshape,upload,selupload,seltabup,tasks,delete} update Updates Selenium drivers for firefox quota Print Earth Engine total quota and used quota zipshape Zips all shapefiles and subsidary files into individual zip files getmeta Generates generalized metadata for all rasters in folder upload Batch Asset Uploader using Selenium seltabup Batch Table Uploader using Selenium. selsetup Non headless setup for new google account, use if upload throws errors tasks Queries current task status [completed,running,ready,failed,cancelled] delete Deletes collection and all items inside. Supports Unix-like wildcards. optional arguments: -h, --help show this help message and exit To obtain help for specific functionality, simply call it with help switch, e.g.: geeup zipshape -h . If you didn't install geeup, then you can run it just by going to geeup directory and running python geeup.py [arguments go here]","title":"Getting started"},{"location":"projects/geeup/#geeup-simple-cli-for-earth-engine-uploads","text":"The tool is designed to handle batch uploading of images and tables(shapefiles). While there are image collection where you can batch upload imagery, for vector or shapefiles you have to batch upload them to a folder.","title":"geeup Simple CLI for Earth Engine Uploads"},{"location":"projects/geeup/#geeup-init","text":"This is a key step since all upload function depends on this step, so make sure you run this . This downloads selenium driver and places to your local directory for windows and Linux subsystems. This is the first step to use selenium supported upload. geeup init","title":"geeup init"},{"location":"projects/geeup/#gee-quota","text":"Just a simple tool to print your earth engine quota quickly. usage: geeup quota [-h] optional arguments: -h, --help show this help message and exit","title":"gee Quota"},{"location":"projects/geeup/#gee-zipshape","text":"So here's how table upload in Google Earth Engine works, you can either upload the component files shp, shx, prj and dbf or you can zip these files together and upload it as a single file. The pros for this is that it reduces the overall size of the shapefile after zipping them along, this tool looks for the shp file and finds the subsidiary files and zips them ready for upload. It also helps when you have limited upload bandwidth. Cons you have to create a replicate structure of the file system, but it saves on bandwidth and auto-arranges your files so you don't have to look for each additional file. usage: geeup zipshape [-h] --input INPUT --output OUTPUT optional arguments: -h, --help show this help message and exit Required named arguments.: --input INPUT Path to the input directory with all shape files --output OUTPUT Destination folder Full path where shp, shx, prj and dbf files if present in input will be zipped and stored","title":"gee Zipshape"},{"location":"projects/geeup/#gee-getmeta","text":"This script generates a generalized metadata using information parsed from gdalinfo and metadata properties. For now it generates metadata with image name, x and y dimension of images, the pixel resolution and the number of bands. usage: geeup getmeta [-h] --input INPUT --metadata METADATA optional arguments: -h, --help show this help message and exit Required named arguments.: --input INPUT Path to the input directory with all raster files --metadata METADATA Full path to export metadata.csv file","title":"gee getmeta"},{"location":"projects/geeup/#gee-upload","text":"The script creates an Image Collection from GeoTIFFs in your local directory. By default, the image name in the collection is the same as the local directory name; with the optional parameter you can provide a different name. usage: geeup upload [-h] --source SOURCE --dest DEST -m METADATA [--nodata NODATA] [-u USER] optional arguments: -h, --help show this help message and exit Required named arguments.: --source SOURCE Path to the directory with images for upload. --dest DEST Destination. Full path for upload to Google Earth Engine, e.g. users/pinkiepie/myponycollection -m METADATA, --metadata METADATA Path to CSV with metadata. -u USER, --user USER Google account name (gmail address). Optional named arguments: --nodata NODATA The value to burn into the raster as NoData (missing data)","title":"gee upload"},{"location":"projects/geeup/#gee-seltabup","text":"This tool allows you to batch download tables/shapefiles to a folder. It uses a modified version of the image upload and a wrapper around the earthengine upload cli to achieve this while creating folders if they don't exist and reporting on assets and checking on uploads. This only requires a source, destination and your ee authenticated email address. This tool also uses selenium to upload the tables. usage: geeup seltabup [-h] --source SOURCE --dest DEST [-u USER] optional arguments: -h, --help show this help message and exit Required named arguments.: --source SOURCE Path to the directory with zipped folder for upload. --dest DEST Destination. Full path for upload to Google Earth Engine, e.g. users/pinkiepie/myponycollection -u USER, --user USER Google account name (gmail address).","title":"gee seltabup"},{"location":"projects/geeup/#gee-selsetup","text":"Once in a while the geckodriver requires manual input before signing into the google earth engine, this tool will allow you to interact with the initialization of Google Earth Engine code editor window. It allows the user to specify the account they want to use, and should only be needed once. geeup selsetup","title":"gee selsetup"},{"location":"projects/geeup/#gee-tasks","text":"This script counts all currently running, ready, completed, failed and canceled tasks along with failed tasks. This tool is linked to your google earth engine account with which you initialized the earth engine client. This takes no argument. usage: geeup tasks [-h] optional arguments: -h, --help show this help message and exit","title":"gee tasks"},{"location":"projects/geeup/#gee-delete","text":"The delete is recursive, meaning it will also delete all children assets: images, collections, and folders. Use with caution! usage: geeup delete [-h] id positional arguments: id Full path to asset for deletion. Recursively removes all folders, collections and images. optional arguments: -h, --help show this help message and exit","title":"gee delete"},{"location":"projects/geeup/#changelog","text":"","title":"Changelog"},{"location":"projects/geeup/#v032","text":"Fixed issue with selsetup.","title":"v0.3.2"},{"location":"projects/geeup/#v031","text":"Fixed issue with raw_input and input for selsetup. Fixed selenium path for windows and other platforms. General improvements to ReadMe","title":"v0.3.1"},{"location":"projects/geeup/#v030","text":"Fixed (issue 13)[ https://github.com/samapriya/geeup/issues/13 ] non relative import. Fixed issues with package import.","title":"v0.3.0"},{"location":"projects/geeup/#v029","text":"Fixed issues caused by --no-use_cloud_api in earthengine-api package","title":"v0.2.9"},{"location":"projects/geeup/#v027","text":"Fix to handle case senstive platform type for all os Fix to Issue 11","title":"v0.2.7"},{"location":"projects/geeup/#v026","text":"Fixed geckodriver path to handle macos Fix to Issue 10","title":"v0.2.6"},{"location":"projects/geeup/#v025","text":"Now allows for downloading geckodriver for macos Fix to Issue 10 Now includes a metadata tool to generate a generalized metadata for any raster to allow upload. Fix to Issue 7 Changed from geeup update to init to signify initialization Added selsetup this tool allows for setting up the gecko driver with your account incase there are issues uploading Better error handling for selenium driver download","title":"v0.2.5"},{"location":"projects/geeup/#v024","text":"Made general improvements Better error handling for selenium driver download","title":"v0.2.4"},{"location":"projects/geeup/#v022","text":"Can now handle generalized metadata (metadata is now required field) Fixed issues with table upload Overall code optimization and handle streaming upload","title":"v0.2.2"},{"location":"projects/geeup/#v019","text":"Changes to handle PyDL installation for Py2 and Py3 Removed Planet uploader to make tool more generalized","title":"v0.1.9"},{"location":"projects/geeup/#v018","text":"Multipart encoder using requests toolbelt for streaming upload Changed manifest upload methodology to match changes in earthengine-api","title":"v0.1.8"},{"location":"projects/geeup/#v016","text":"Fixed issue with module locations","title":"v0.1.6"},{"location":"projects/geeup/#v015","text":"Fixed issue with gecko driver paths Fixed issue with null uploads using task, switched to ee CLI upload","title":"v0.1.5"},{"location":"projects/geeup/#v014","text":"OS based geckdriver path fix General improvements","title":"v0.1.4"},{"location":"projects/geeup/#v013","text":"fixed issues with extra arguments Upload issue resolved General dependency","title":"v0.1.3"},{"location":"projects/geeup/#v011","text":"fixed dependency issues Upload post issues resolved Removed dependency on poster for now","title":"v0.1.1"},{"location":"projects/geeup/#v009","text":"fixed attribution and dependecy issues Included poster to improve streaming uploads All uploads now use selenium","title":"v0.0.9"},{"location":"projects/geeup/#v008","text":"fixed issues with unused imports","title":"v0.0.8"},{"location":"projects/geeup/#v007","text":"fixed issues with manifest lib","title":"v0.0.7"},{"location":"projects/geeup/#v006","text":"Detailed quota readout Uses selenium based uploader to upload images Avoids issues with python auth for upload","title":"v0.0.6"},{"location":"projects/geeup/#v005","text":"Removed unnecessary library imports Minor improvements and updated readme","title":"v0.0.5"},{"location":"projects/geeup/#v004","text":"Improved valid table name check before upload Improvements to earth engine quota tool for more accurate quota and human readable","title":"v0.0.4"},{"location":"projects/ghome/","text":"ghome: Simple CLI for Google Home & Mini \u00b6 This is an application of the google home API Blueprint reported here . While a lot of the end points maybe of interest to quite a few people, I used the ones I use most and I thought would be most useful and created a command line tool for you to search for a Google home mini device and then use this tool to interact with it and perform actions as needed. Table of contents \u00b6 Installation Getting started ghome Simple CLI for Earth Engine Uploads list reboot alarm do not disturb bluetooth status bluetooth scan bluetooth paired bluetooth discovery wifi scan Installation \u00b6 This assumes that you have native python & pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have tested this only on python 2.7.15, but it should run on Python 3. To install ghome: Simple CLI for Google Home & Mini you can install using two methods. pip install ghome or you can also try git clone https://github.com/samapriya/ghome.git cd ghome python setup.py install For Linux use sudo or try pip install ghome --user . Installation is an optional step; the application can also be run directly by executing ghome.py script. The advantage of having it installed is that ghome can be executed as any command line tool. I recommend installation within a virtual environment. If you don't want to install, browse into the ghome folder and try python ghome.py to get to the same result. Getting started \u00b6 As usual, to print help: usage: ghome [-h] {list,reboot,alarm,dnd,bstat,bscan,bpair,bdisc,wscan} ... Simple Google Home Mini Client positional arguments: {list,reboot,alarm,dnd,bstat,bscan,bpair,bdisc,wscan} list Lists all google home mini devices & IP address reboot Reboot a google home mini using IP address alarm Print out the current alarms setup on your google home mini dnd Enable or disable <Do not Disturb mode> for a google home mini using IP address bstat Print current bluetooth status for a google home mini using IP address bscan Scan for Bluetooth devices near a google home mini using IP address bpair Print current paired bluetooth devices for a google home mini using IP address bdisc Enable or disable bluetooth discovery for a google home mini using IP address wscan Scan for Wifi networks near a google home mini using IP address optional arguments: -h, --help show this help message and exit To obtain help for specific functionality, simply call it with help switch, e.g.: ghome wscan -h . If you didn't install ghome, then you can run it just by going to ghome directory and running python ghome.py [arguments go here] ghome Simple CLI for Earth Engine Uploads \u00b6 The tool is based on curret unofficial API blueprint published for the device and is subject to change in the future. list \u00b6 **This is a key step since it lists all google home mini devices in your wifi, it used nmap to identify said devices. You can avoid this tool by using a third party tool like [Fing] to identify your google home devices.Usage is simply ghome list reboot \u00b6 Just a simple tool to reboot your google home device quickly. usage: ghome reboot [-h] [--ip IP] optional arguments: -h, --help show this help message and exit Required named arguments.: --ip IP Use \"ip\" for Google Home Mini device alarm \u00b6 This tool will simply list all alarms currently on your device including date , time and time zone. usage: ghome alarm [-h] [--ip IP] optional arguments: -h, --help show this help message and exit Required named arguments.: --ip IP Use \"ip\" for Google Home Mini device do not disturb \u00b6 Enable or disable do not disturb mode on the Google home mini. usage: ghome dnd [-h] [--ip IP] [--action ACTION] optional arguments: -h, --help show this help message and exit Required named arguments.: --ip IP Use \"ip\" for Google Home Mini device --action ACTION enable|disable do not disturb mode bluetooth status \u00b6 The bluetooth status prints whether the device discovery is enabled,whether scanning is enabled and whether it is connected to a device. usage: ghome bstat [-h] [--ip IP] optional arguments: -h, --help show this help message and exit Required named arguments.: --ip IP Use \"ip\" for Google Home Mini device bluetooth scan \u00b6 This prints bluetooth status, including is possible bluetooth device name and mac name usage: ghome bstat [-h] [--ip IP] optional arguments: -h, --help show this help message and exit Required named arguments.: --ip IP Use \"ip\" for Google Home Mini device bluetooth paired \u00b6 Check if device is paried via bluetooth to any current device as well as history of all deviced connected, last connected and whether or not they are currently connected. usage: ghome bpair [-h] [--ip IP] optional arguments: -h, --help show this help message and exit Required named arguments.: --ip IP Use \"ip\" for Google Home Mini device bluetooth discovery \u00b6 This is to enable or disable bluetooth discovery to allow for pairing as needed. The action can be enable or disable coupled with the ip. usage: ghome bdisc [-h] [--ip IP] [--action ACTION] optional arguments: -h, --help show this help message and exit Required named arguments.: --ip IP Use \"ip\" for Google Home Mini device --action ACTION enable|disable bluetooth discovery wifi scan \u00b6 Prints the wifi scan results of all available wifi connections and their ssid based on the proximity of the device to other connections. You might want to run this twice, since it may not build the device cache directly. usage: ghome wscan [-h] [--ip IP] optional arguments: -h, --help show this help message and exit Required named arguments.: --ip IP Use \"ip\" for Google Home Mini device","title":"Google home mini CLI"},{"location":"projects/ghome/#ghome-simple-cli-for-google-home-mini","text":"This is an application of the google home API Blueprint reported here . While a lot of the end points maybe of interest to quite a few people, I used the ones I use most and I thought would be most useful and created a command line tool for you to search for a Google home mini device and then use this tool to interact with it and perform actions as needed.","title":"ghome: Simple CLI for Google Home &amp; Mini"},{"location":"projects/ghome/#table-of-contents","text":"Installation Getting started ghome Simple CLI for Earth Engine Uploads list reboot alarm do not disturb bluetooth status bluetooth scan bluetooth paired bluetooth discovery wifi scan","title":"Table of contents"},{"location":"projects/ghome/#installation","text":"This assumes that you have native python & pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have tested this only on python 2.7.15, but it should run on Python 3. To install ghome: Simple CLI for Google Home & Mini you can install using two methods. pip install ghome or you can also try git clone https://github.com/samapriya/ghome.git cd ghome python setup.py install For Linux use sudo or try pip install ghome --user . Installation is an optional step; the application can also be run directly by executing ghome.py script. The advantage of having it installed is that ghome can be executed as any command line tool. I recommend installation within a virtual environment. If you don't want to install, browse into the ghome folder and try python ghome.py to get to the same result.","title":"Installation"},{"location":"projects/ghome/#getting-started","text":"As usual, to print help: usage: ghome [-h] {list,reboot,alarm,dnd,bstat,bscan,bpair,bdisc,wscan} ... Simple Google Home Mini Client positional arguments: {list,reboot,alarm,dnd,bstat,bscan,bpair,bdisc,wscan} list Lists all google home mini devices & IP address reboot Reboot a google home mini using IP address alarm Print out the current alarms setup on your google home mini dnd Enable or disable <Do not Disturb mode> for a google home mini using IP address bstat Print current bluetooth status for a google home mini using IP address bscan Scan for Bluetooth devices near a google home mini using IP address bpair Print current paired bluetooth devices for a google home mini using IP address bdisc Enable or disable bluetooth discovery for a google home mini using IP address wscan Scan for Wifi networks near a google home mini using IP address optional arguments: -h, --help show this help message and exit To obtain help for specific functionality, simply call it with help switch, e.g.: ghome wscan -h . If you didn't install ghome, then you can run it just by going to ghome directory and running python ghome.py [arguments go here]","title":"Getting started"},{"location":"projects/ghome/#ghome-simple-cli-for-earth-engine-uploads","text":"The tool is based on curret unofficial API blueprint published for the device and is subject to change in the future.","title":"ghome Simple CLI for Earth Engine Uploads"},{"location":"projects/ghome/#list","text":"**This is a key step since it lists all google home mini devices in your wifi, it used nmap to identify said devices. You can avoid this tool by using a third party tool like [Fing] to identify your google home devices.Usage is simply ghome list","title":"list"},{"location":"projects/ghome/#reboot","text":"Just a simple tool to reboot your google home device quickly. usage: ghome reboot [-h] [--ip IP] optional arguments: -h, --help show this help message and exit Required named arguments.: --ip IP Use \"ip\" for Google Home Mini device","title":"reboot"},{"location":"projects/ghome/#alarm","text":"This tool will simply list all alarms currently on your device including date , time and time zone. usage: ghome alarm [-h] [--ip IP] optional arguments: -h, --help show this help message and exit Required named arguments.: --ip IP Use \"ip\" for Google Home Mini device","title":"alarm"},{"location":"projects/ghome/#do-not-disturb","text":"Enable or disable do not disturb mode on the Google home mini. usage: ghome dnd [-h] [--ip IP] [--action ACTION] optional arguments: -h, --help show this help message and exit Required named arguments.: --ip IP Use \"ip\" for Google Home Mini device --action ACTION enable|disable do not disturb mode","title":"do not disturb"},{"location":"projects/ghome/#bluetooth-status","text":"The bluetooth status prints whether the device discovery is enabled,whether scanning is enabled and whether it is connected to a device. usage: ghome bstat [-h] [--ip IP] optional arguments: -h, --help show this help message and exit Required named arguments.: --ip IP Use \"ip\" for Google Home Mini device","title":"bluetooth status"},{"location":"projects/ghome/#bluetooth-scan","text":"This prints bluetooth status, including is possible bluetooth device name and mac name usage: ghome bstat [-h] [--ip IP] optional arguments: -h, --help show this help message and exit Required named arguments.: --ip IP Use \"ip\" for Google Home Mini device","title":"bluetooth scan"},{"location":"projects/ghome/#bluetooth-paired","text":"Check if device is paried via bluetooth to any current device as well as history of all deviced connected, last connected and whether or not they are currently connected. usage: ghome bpair [-h] [--ip IP] optional arguments: -h, --help show this help message and exit Required named arguments.: --ip IP Use \"ip\" for Google Home Mini device","title":"bluetooth paired"},{"location":"projects/ghome/#bluetooth-discovery","text":"This is to enable or disable bluetooth discovery to allow for pairing as needed. The action can be enable or disable coupled with the ip. usage: ghome bdisc [-h] [--ip IP] [--action ACTION] optional arguments: -h, --help show this help message and exit Required named arguments.: --ip IP Use \"ip\" for Google Home Mini device --action ACTION enable|disable bluetooth discovery","title":"bluetooth discovery"},{"location":"projects/ghome/#wifi-scan","text":"Prints the wifi scan results of all available wifi connections and their ssid based on the proximity of the device to other connections. You might want to run this twice, since it may not build the device cache directly. usage: ghome wscan [-h] [--ip IP] optional arguments: -h, --help show this help message and exit Required named arguments.: --ip IP Use \"ip\" for Google Home Mini device","title":"wifi scan"},{"location":"projects/jetstream_unofficial_cli/","text":"Jetstream Unofficial Addon for Atmosphere VM(s) \u00b6 Jetstream provides researchers and students with a unique approach to cloud computing and data analysis on demand. Moving away from a job submission model which is more pervasice on High Performance Cluster computing this allows users to create user-friendly installs of \u201cvirtual machines\u201d which can then be custom installed with softwares and packages and even imaged further to make science replicable. The system in theory also allows a BYoOS (Bring your own Operating System) designed to further allow more flexibility while sharing the backbone of high performance compute and cloud storage infrastructure. The idea of imaging a system allows the user to not only share their data or analysis but their entire setup that allows them to run the analysis making it a plug and play model in terms of research replication and transference. As the NSF project proceeds further it allows users to apply for grants or allocation time on the machines and indeed renew these free of cost for researchers and students. For my work looking at developing a Satellite Imagery Input Output (IO) pipeline I was allocated a Jetstream grant and while using a couple of these systems I always thought it would be easier to query number of instances and volumes running on my account, my burn rate and left over allocation over sometime. And something useful was the possibility to perform system actions such as shutdown(stop), restart, start, reboot among others programatically. It is with this idea that I approach the API backend at JetStream and built just a few tools to allows users to perform these actions without the need to log into their web browsers and allows user to call upon instance actions through a rough command line interface (CLI). I would love to thank Steve Gregory who is getting the Official API ready for release and Jeremy Fischer who have helped me in more ways and allowed me to keep asking questions and learn from them to build the unofficial api for Jetstream. The Jetstream's mission and funding is supported by NSF and any and all citations are useful so please make sure to cite and citation information can be found at the end of this readme or use the link Note: I have used this on the Jetstream-IU system but it should work on the TACC end as well and the information I decided to show per instance is based on what I used, there are much more information generated but the tool prints a subset Table of contents \u00b6 Getting started Save API Password as Credential Query Current Instances Query Current Volumes Perform Instance Actions Getting started \u00b6 To get started you need an XSEDE account of a Jetstream Rapid Access Account and you can create one using instructions here . Once into the system you will still need an allocation and the wiki page setup by the project explains these along with everything you can do step by step here . Once you have a project allocation and create some instances and volumes you can query and perform instance actions. Just browse to the folder and perform a python jetstream.py -h : usage: jetstream.py [-h] { ,jskey,instance,volume,action} ... JetStream API Unofficial positional arguments: { ,jskey,instance,volume,action} ------------------------------------------- -----Choose from JetStream Tools Below----- ------------------------------------------- jskey Allows you to save your JetStream API Password instance Allows users to print out all instance information volume Allows users to print out all volume information action Allows user to start, suspend,resume,reboot instance optional arguments: -h, --help show this help message and exit Save API Password as Credential \u00b6 This tool allows the user to save the credential or password file into users/.config/jetstream making sure that they are user specific and are not shared on a system resource or location. This password is not your XSEDE or Jetstream Rapid Access account password but a API password which has to be requested atleast for now. It uses a getpass implementation and write the password as a csv and the other tools first tries to read this and if not present asks for your password. usage: jetstream.py jskey [-h] optional arguments: -h, --help show this help message and exit Query Current Instances \u00b6 As the name stated this allows the user to query instances related to your account. This queries all instances for now and prints certain key parameters(these are a subset that I selected for my use) but can be easily modified in the code to print other information. usage: jetstream.py instance [-h] [--username USERNAME] [--password PASSWORD] optional arguments: -h, --help show this help message and exit --username USERNAME Jetstream API username --password PASSWORD Jetstream API password: \"Optional if you already saved jetstream key\" Incase you have already saved your password a setup would be simply python jetstream.py instance --username \"johndoe\" if not python jetstream.py instance --username \"johndoe\" --password \"pass\" Query Current Volumes \u00b6 The current volume options queries current volumes on your project. Note that the current info does not tell you which instance a volume is attached to which might be a nice endpoint readout to have. usage: jetstream.py volume [-h] [--username USERNAME] [--password PASSWORD] optional arguments: -h, --help show this help message and exit --username USERNAME Jetstream API username --password PASSWORD Jetstream API password: \"Optional if you already saved jetstream key\" Incase you have already saved your password a setup would be simply python jetstream.py volume --username \"johndoe\" if not python jetstream.py volume --username \"johndoe\" --password \"pass\" Perform Instance Actions \u00b6 This is perhaps one of the most important aspect of building this tool, the idea was to get a read out or a email message when a system has been idle for sometime and then auto shutdown or initiate a shutdown remotely. This command allows you to choose the instance you want to perform an instance action using the instance ID mentioned on your instance profile page. The next thing you need to know is what kind of action you want to perform. There are certain things that will create a conflict, for example: If you are trying to shutdown an already shutoff system or start an already active system. In that case it will print out a conflict message. usage: jetstream.py action [-h] [--username USERNAME] [--password PASSWORD] [--id ID] [--action ACTION] optional arguments: -h, --help show this help message and exit --username USERNAME Jetstream API username --password PASSWORD Jetstream API password: \"Optional if you already saved jetstream key\" --id ID Jetstream Instance ID on your Instance Detail Page --action ACTION Jetstream Instance Action, \"start|stop|suspend|resume|reboot\" Incase you have already saved your password a setup would be simply python jetstream.py action --username \"johndoe\" --id \"00000\" --action \"start\" if not python jetstream.py action --username \"johndoe\" --password \"pass\" --id \"00000\" --action \"start\" I would like to thank my grant from JetStream TG-GEO160014. You can cite this tool as Samapriya Roy. (2017, August 16). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon. Zenodo. http://doi.org/10.5281/zenodo.844018 And I would like to include Jetstream citations for others to use Stewart, C.A., Cockerill, T.M., Foster, I., Hancock, D., Merchant, N., Skidmore, E., Stanzione, D., Taylor, J., Tuecke, S., Turner, G., Vaughn, M., and Gaffney, N.I., Jetstream: a self-provisioned, scalable science and engineering cloud environment. 2015, In Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure. St. Louis, Missouri. ACM: 2792774. p. 1-8. http://dx.doi.org/10.1145/2792745.2792774 and John Towns, Timothy Cockerill, Maytal Dahan, Ian Foster, Kelly Gaither, Andrew Grimshaw, Victor Hazlewood, Scott Lathrop, Dave Lifka, Gregory D. Peterson, Ralph Roskies, J. Ray Scott, Nancy Wilkins-Diehr, \"XSEDE: Accelerating Scientific Discovery\", Computing in Science & Engineering, vol.16, no. 5, pp. 62-74, Sept.-Oct. 2014, doi:10.1109/MCSE.2014.80","title":"Jetstream Unofficial Addon"},{"location":"projects/jetstream_unofficial_cli/#jetstream-unofficial-addon-for-atmosphere-vms","text":"Jetstream provides researchers and students with a unique approach to cloud computing and data analysis on demand. Moving away from a job submission model which is more pervasice on High Performance Cluster computing this allows users to create user-friendly installs of \u201cvirtual machines\u201d which can then be custom installed with softwares and packages and even imaged further to make science replicable. The system in theory also allows a BYoOS (Bring your own Operating System) designed to further allow more flexibility while sharing the backbone of high performance compute and cloud storage infrastructure. The idea of imaging a system allows the user to not only share their data or analysis but their entire setup that allows them to run the analysis making it a plug and play model in terms of research replication and transference. As the NSF project proceeds further it allows users to apply for grants or allocation time on the machines and indeed renew these free of cost for researchers and students. For my work looking at developing a Satellite Imagery Input Output (IO) pipeline I was allocated a Jetstream grant and while using a couple of these systems I always thought it would be easier to query number of instances and volumes running on my account, my burn rate and left over allocation over sometime. And something useful was the possibility to perform system actions such as shutdown(stop), restart, start, reboot among others programatically. It is with this idea that I approach the API backend at JetStream and built just a few tools to allows users to perform these actions without the need to log into their web browsers and allows user to call upon instance actions through a rough command line interface (CLI). I would love to thank Steve Gregory who is getting the Official API ready for release and Jeremy Fischer who have helped me in more ways and allowed me to keep asking questions and learn from them to build the unofficial api for Jetstream. The Jetstream's mission and funding is supported by NSF and any and all citations are useful so please make sure to cite and citation information can be found at the end of this readme or use the link Note: I have used this on the Jetstream-IU system but it should work on the TACC end as well and the information I decided to show per instance is based on what I used, there are much more information generated but the tool prints a subset","title":"Jetstream Unofficial Addon for Atmosphere VM(s)"},{"location":"projects/jetstream_unofficial_cli/#table-of-contents","text":"Getting started Save API Password as Credential Query Current Instances Query Current Volumes Perform Instance Actions","title":"Table of contents"},{"location":"projects/jetstream_unofficial_cli/#getting-started","text":"To get started you need an XSEDE account of a Jetstream Rapid Access Account and you can create one using instructions here . Once into the system you will still need an allocation and the wiki page setup by the project explains these along with everything you can do step by step here . Once you have a project allocation and create some instances and volumes you can query and perform instance actions. Just browse to the folder and perform a python jetstream.py -h : usage: jetstream.py [-h] { ,jskey,instance,volume,action} ... JetStream API Unofficial positional arguments: { ,jskey,instance,volume,action} ------------------------------------------- -----Choose from JetStream Tools Below----- ------------------------------------------- jskey Allows you to save your JetStream API Password instance Allows users to print out all instance information volume Allows users to print out all volume information action Allows user to start, suspend,resume,reboot instance optional arguments: -h, --help show this help message and exit","title":"Getting started"},{"location":"projects/jetstream_unofficial_cli/#save-api-password-as-credential","text":"This tool allows the user to save the credential or password file into users/.config/jetstream making sure that they are user specific and are not shared on a system resource or location. This password is not your XSEDE or Jetstream Rapid Access account password but a API password which has to be requested atleast for now. It uses a getpass implementation and write the password as a csv and the other tools first tries to read this and if not present asks for your password. usage: jetstream.py jskey [-h] optional arguments: -h, --help show this help message and exit","title":"Save API Password as Credential"},{"location":"projects/jetstream_unofficial_cli/#query-current-instances","text":"As the name stated this allows the user to query instances related to your account. This queries all instances for now and prints certain key parameters(these are a subset that I selected for my use) but can be easily modified in the code to print other information. usage: jetstream.py instance [-h] [--username USERNAME] [--password PASSWORD] optional arguments: -h, --help show this help message and exit --username USERNAME Jetstream API username --password PASSWORD Jetstream API password: \"Optional if you already saved jetstream key\" Incase you have already saved your password a setup would be simply python jetstream.py instance --username \"johndoe\" if not python jetstream.py instance --username \"johndoe\" --password \"pass\"","title":"Query Current Instances"},{"location":"projects/jetstream_unofficial_cli/#query-current-volumes","text":"The current volume options queries current volumes on your project. Note that the current info does not tell you which instance a volume is attached to which might be a nice endpoint readout to have. usage: jetstream.py volume [-h] [--username USERNAME] [--password PASSWORD] optional arguments: -h, --help show this help message and exit --username USERNAME Jetstream API username --password PASSWORD Jetstream API password: \"Optional if you already saved jetstream key\" Incase you have already saved your password a setup would be simply python jetstream.py volume --username \"johndoe\" if not python jetstream.py volume --username \"johndoe\" --password \"pass\"","title":"Query Current Volumes"},{"location":"projects/jetstream_unofficial_cli/#perform-instance-actions","text":"This is perhaps one of the most important aspect of building this tool, the idea was to get a read out or a email message when a system has been idle for sometime and then auto shutdown or initiate a shutdown remotely. This command allows you to choose the instance you want to perform an instance action using the instance ID mentioned on your instance profile page. The next thing you need to know is what kind of action you want to perform. There are certain things that will create a conflict, for example: If you are trying to shutdown an already shutoff system or start an already active system. In that case it will print out a conflict message. usage: jetstream.py action [-h] [--username USERNAME] [--password PASSWORD] [--id ID] [--action ACTION] optional arguments: -h, --help show this help message and exit --username USERNAME Jetstream API username --password PASSWORD Jetstream API password: \"Optional if you already saved jetstream key\" --id ID Jetstream Instance ID on your Instance Detail Page --action ACTION Jetstream Instance Action, \"start|stop|suspend|resume|reboot\" Incase you have already saved your password a setup would be simply python jetstream.py action --username \"johndoe\" --id \"00000\" --action \"start\" if not python jetstream.py action --username \"johndoe\" --password \"pass\" --id \"00000\" --action \"start\" I would like to thank my grant from JetStream TG-GEO160014. You can cite this tool as Samapriya Roy. (2017, August 16). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon. Zenodo. http://doi.org/10.5281/zenodo.844018 And I would like to include Jetstream citations for others to use Stewart, C.A., Cockerill, T.M., Foster, I., Hancock, D., Merchant, N., Skidmore, E., Stanzione, D., Taylor, J., Tuecke, S., Turner, G., Vaughn, M., and Gaffney, N.I., Jetstream: a self-provisioned, scalable science and engineering cloud environment. 2015, In Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure. St. Louis, Missouri. ACM: 2792774. p. 1-8. http://dx.doi.org/10.1145/2792745.2792774 and John Towns, Timothy Cockerill, Maytal Dahan, Ian Foster, Kelly Gaither, Andrew Grimshaw, Victor Hazlewood, Scott Lathrop, Dave Lifka, Gregory D. Peterson, Ralph Roskies, J. Ray Scott, Nancy Wilkins-Diehr, \"XSEDE: Accelerating Scientific Discovery\", Computing in Science & Engineering, vol.16, no. 5, pp. 62-74, Sept.-Oct. 2014, doi:10.1109/MCSE.2014.80","title":"Perform Instance Actions"},{"location":"projects/nsfsearch/","text":"nsfsearch: NSF Awards API Simple CLI \u00b6 This is an application based on the NSF Awards API . While the NSF award search tool is great, this is useful to get some quick reports and statistics on NSF awards. It also does act as a quick search and ideally as NSF updates the API this tool will be updated to reflect changes. Table of contents \u00b6 Installation Getting started nsfsearch: NSF Awards API Simple CLI keysearch export idsearch result Installation \u00b6 This assumes that you have native python & pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying python and then pip list To install nsfsearch: Simple CLI for Google Home & Mini you can install using two methods. pip install nsfsearch or you can also try git clone https://github.com/samapriya/nsfsearch.git cd nsfsearch python setup.py install For Linux use sudo or try pip install nsfsearch --user . Installation is an optional step; the application can also be run directly by executing nsfsearch.py script. The advantage of having it installed is that nsfsearch can be executed as any command line tool. I recommend installation within a virtual environment. If you don't want to install, browse into the nsfsearch folder and try python nsfsearch.py to get to the same result. Getting started \u00b6 As usual, to print help: NSF Awards API Simple CLI positional arguments: {keysearch,export,idsearch,result} keysearch Searches for all projects with keywords export Searches for all projects with keywords & export idsearch Search using project ID result Project outcome based on ID optional arguments: -h, --help show this help message and exit To obtain help for specific functionality, simply call it with help switch, e.g.: nsfsearch result -h . If you didn't install nsfsearch, then you can run it just by going to nsfsearch directory and running python nsfsearch.py [arguments go here] nsfsearch NSF Awards API Simple CLI \u00b6 The tool is based on curret NSF award search API and is subject to change in the future. keysearch \u00b6 This allows you to get all awards based on a keyword search, you can pass keywords as comman seperated value and it iterates through the list and prints the result to console. Usage is simply usage: nsfsearch keysearch [-h] [--wordlist WORDLIST [WORDLIST ...]] optional arguments: -h, --help show this help message and exit --wordlist WORDLIST [WORDLIST ...] Send comma seperated keywords export \u00b6 This tool exports the keysearch results to a csv file, instead of printing it out on the console. usage: nsfsearch export [-h] [--wordlist WORDLIST [WORDLIST ...]] [--file FILE] optional arguments: -h, --help show this help message and exit --wordlist WORDLIST [WORDLIST ...] Send comma seperated keywords --file FILE Outfile CSV file with results idsearch \u00b6 Looks at specific NSF project using project id. usage: nsfsearch idsearch [-h] [--ids IDS] optional arguments: -h, --help show this help message and exit --ids IDS Project ID result \u00b6 Get any project outcome results as reported by NSF using project id. usage: nsfsearch result [-h] [--ids IDS] optional arguments: -h, --help show this help message and exit --ids IDS Project ID","title":"NSF Awards API Simple CLI"},{"location":"projects/nsfsearch/#nsfsearch-nsf-awards-api-simple-cli","text":"This is an application based on the NSF Awards API . While the NSF award search tool is great, this is useful to get some quick reports and statistics on NSF awards. It also does act as a quick search and ideally as NSF updates the API this tool will be updated to reflect changes.","title":"nsfsearch: NSF Awards API Simple CLI"},{"location":"projects/nsfsearch/#table-of-contents","text":"Installation Getting started nsfsearch: NSF Awards API Simple CLI keysearch export idsearch result","title":"Table of contents"},{"location":"projects/nsfsearch/#installation","text":"This assumes that you have native python & pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying python and then pip list To install nsfsearch: Simple CLI for Google Home & Mini you can install using two methods. pip install nsfsearch or you can also try git clone https://github.com/samapriya/nsfsearch.git cd nsfsearch python setup.py install For Linux use sudo or try pip install nsfsearch --user . Installation is an optional step; the application can also be run directly by executing nsfsearch.py script. The advantage of having it installed is that nsfsearch can be executed as any command line tool. I recommend installation within a virtual environment. If you don't want to install, browse into the nsfsearch folder and try python nsfsearch.py to get to the same result.","title":"Installation"},{"location":"projects/nsfsearch/#getting-started","text":"As usual, to print help: NSF Awards API Simple CLI positional arguments: {keysearch,export,idsearch,result} keysearch Searches for all projects with keywords export Searches for all projects with keywords & export idsearch Search using project ID result Project outcome based on ID optional arguments: -h, --help show this help message and exit To obtain help for specific functionality, simply call it with help switch, e.g.: nsfsearch result -h . If you didn't install nsfsearch, then you can run it just by going to nsfsearch directory and running python nsfsearch.py [arguments go here]","title":"Getting started"},{"location":"projects/nsfsearch/#nsfsearch-nsf-awards-api-simple-cli_1","text":"The tool is based on curret NSF award search API and is subject to change in the future.","title":"nsfsearch NSF Awards API Simple CLI"},{"location":"projects/nsfsearch/#keysearch","text":"This allows you to get all awards based on a keyword search, you can pass keywords as comman seperated value and it iterates through the list and prints the result to console. Usage is simply usage: nsfsearch keysearch [-h] [--wordlist WORDLIST [WORDLIST ...]] optional arguments: -h, --help show this help message and exit --wordlist WORDLIST [WORDLIST ...] Send comma seperated keywords","title":"keysearch"},{"location":"projects/nsfsearch/#export","text":"This tool exports the keysearch results to a csv file, instead of printing it out on the console. usage: nsfsearch export [-h] [--wordlist WORDLIST [WORDLIST ...]] [--file FILE] optional arguments: -h, --help show this help message and exit --wordlist WORDLIST [WORDLIST ...] Send comma seperated keywords --file FILE Outfile CSV file with results","title":"export"},{"location":"projects/nsfsearch/#idsearch","text":"Looks at specific NSF project using project id. usage: nsfsearch idsearch [-h] [--ids IDS] optional arguments: -h, --help show this help message and exit --ids IDS Project ID","title":"idsearch"},{"location":"projects/nsfsearch/#result","text":"Get any project outcome results as reported by NSF using project id. usage: nsfsearch result [-h] [--ids IDS] optional arguments: -h, --help show this help message and exit --ids IDS Project ID","title":"result"},{"location":"projects/open-impact/","text":"Open Impact Training and Hackathons \u00b6 Open Impact projects include hackathons, training to support developer interaction and engagement of Education and Research users. You can click on the headings to go their page Planet Explore 2019: First User Conference \u00b6 This was the first user conference for Planet, with over 500 participants , hours of ask me anything time and I got to teach two productive workshops. Google Geo for Good 2019: Linking Stories to Scales: Through Data and Platforms \u00b6 Linking stories to scales talks to how users have used high resolution imagery and Google Earth Engine platform to work on problems and to tell stories. Scipy 2019: Hands-on Satellite Imagery Analysis \u00b6 Satellite data is more widely available than ever before, and it is now possible for the public to access sub-weekly and even daily imagery of the Earth's entire landmass. In this tutorial, gain hands-on experience exploring Planet\u2019s publicly-available satellite imagery and using Python tools for geospatial and time-series analysis of medium- and high-resolution imagery data. Using free & open source libraries, learn how to perform foundational imagery analysis techniques and apply these techniques to real satellite data. At the end of the workshop, we\u2019ll demonstrate how the skills you've learned can be applied to investigate real-world environmental and humanitarian challenges. American Geophysical Union Fall Meeting 2018 \u00b6 AGU Scientific Workshop: Sensor Fusion and Analysis Ready Data with the Planet Platform (WS32) BigEarth Water Hackathon 2018: Stanford \u00b6 Presentation was Planet API, satadd tool to access Planet, Satellogic and Digital Globe Assets. Along with data download and processing in Google Earth Engine. SatSummit 2018: Hands-on Satellite Imagery Processing and Analysis \u00b6 Presentations and more of the joined CSDMS - SEN 2018 annual meeting Geoprocesses, geohazards - CSDMS 2018. Held May 22-24 th 2018 in Boulder Colorado, USA. The presentation was Introduction to Google Earth Engine Terra 2018 Pointcloud And Remote Sensing Workshop \u00b6 Pointcloud And Remote Sensing Workshop Aimed at undergraduates, postgraduates and researchers in Earth / Ocean sciences, Computing, private companies (environmental consultancy, geomatics, topographic, geodesic services), NGOs (conservation, environment) From august 14 to 16 2018, at CICESE on Ensenada, B.C. The presentation was Planet API, data download and processing in Google Earth Engine NEON Data Institute 2018: Remote Sensing with Reproducible Workflows using Python \u00b6 Presentations and more of the joined CSDMS - SEN 2018 annual meeting Geoprocesses, geohazards - CSDMS 2018. Held May 22-24 th 2018 in Boulder Colorado, USA. The presentation was Introduction to Google Earth Engine CSDMS 2018-Introduction to Google Earth Engine \u00b6 Presentations and more of the joined CSDMS - SEN 2018 annual meeting Geoprocesses, geohazards - CSDMS 2018. Held May 22-24 th 2018 in Boulder Colorado, USA. The presentation was Introduction to Google Earth Engine Stanford Big Earth Hackathon \u00b6 Over 100 students gathered with industry and faculty mentors on April 14-15, 2018 to hack for planet earth. Congratulations to the participants for an exciting weekend of planetary solutions.","title":"Open Impact"},{"location":"projects/open-impact/#open-impact-training-and-hackathons","text":"Open Impact projects include hackathons, training to support developer interaction and engagement of Education and Research users. You can click on the headings to go their page","title":"Open Impact Training and Hackathons"},{"location":"projects/open-impact/#planet-explore-2019-first-user-conference","text":"This was the first user conference for Planet, with over 500 participants , hours of ask me anything time and I got to teach two productive workshops.","title":"Planet Explore 2019: First User Conference"},{"location":"projects/open-impact/#google-geo-for-good-2019-linking-stories-to-scales-through-data-and-platforms","text":"Linking stories to scales talks to how users have used high resolution imagery and Google Earth Engine platform to work on problems and to tell stories.","title":"Google Geo for Good 2019: Linking Stories to Scales: Through Data and Platforms"},{"location":"projects/open-impact/#scipy-2019-hands-on-satellite-imagery-analysis","text":"Satellite data is more widely available than ever before, and it is now possible for the public to access sub-weekly and even daily imagery of the Earth's entire landmass. In this tutorial, gain hands-on experience exploring Planet\u2019s publicly-available satellite imagery and using Python tools for geospatial and time-series analysis of medium- and high-resolution imagery data. Using free & open source libraries, learn how to perform foundational imagery analysis techniques and apply these techniques to real satellite data. At the end of the workshop, we\u2019ll demonstrate how the skills you've learned can be applied to investigate real-world environmental and humanitarian challenges.","title":"Scipy 2019: Hands-on Satellite Imagery Analysis"},{"location":"projects/open-impact/#american-geophysical-union-fall-meeting-2018","text":"AGU Scientific Workshop: Sensor Fusion and Analysis Ready Data with the Planet Platform (WS32)","title":"American Geophysical Union Fall Meeting 2018"},{"location":"projects/open-impact/#bigearth-water-hackathon-2018-stanford","text":"Presentation was Planet API, satadd tool to access Planet, Satellogic and Digital Globe Assets. Along with data download and processing in Google Earth Engine.","title":"BigEarth Water Hackathon 2018: Stanford"},{"location":"projects/open-impact/#satsummit-2018-hands-on-satellite-imagery-processing-and-analysis","text":"Presentations and more of the joined CSDMS - SEN 2018 annual meeting Geoprocesses, geohazards - CSDMS 2018. Held May 22-24 th 2018 in Boulder Colorado, USA. The presentation was Introduction to Google Earth Engine","title":"SatSummit 2018: Hands-on Satellite Imagery Processing and Analysis"},{"location":"projects/open-impact/#terra-2018-pointcloud-and-remote-sensing-workshop","text":"Pointcloud And Remote Sensing Workshop Aimed at undergraduates, postgraduates and researchers in Earth / Ocean sciences, Computing, private companies (environmental consultancy, geomatics, topographic, geodesic services), NGOs (conservation, environment) From august 14 to 16 2018, at CICESE on Ensenada, B.C. The presentation was Planet API, data download and processing in Google Earth Engine","title":"Terra 2018 Pointcloud And Remote Sensing Workshop"},{"location":"projects/open-impact/#neon-data-institute-2018-remote-sensing-with-reproducible-workflows-using-python","text":"Presentations and more of the joined CSDMS - SEN 2018 annual meeting Geoprocesses, geohazards - CSDMS 2018. Held May 22-24 th 2018 in Boulder Colorado, USA. The presentation was Introduction to Google Earth Engine","title":"NEON Data Institute 2018: Remote Sensing with Reproducible Workflows using Python"},{"location":"projects/open-impact/#csdms-2018-introduction-to-google-earth-engine","text":"Presentations and more of the joined CSDMS - SEN 2018 annual meeting Geoprocesses, geohazards - CSDMS 2018. Held May 22-24 th 2018 in Boulder Colorado, USA. The presentation was Introduction to Google Earth Engine","title":"CSDMS 2018-Introduction to Google Earth Engine"},{"location":"projects/open-impact/#stanford-big-earth-hackathon","text":"Over 100 students gathered with industry and faculty mentors on April 14-15, 2018 to hack for planet earth. Congratulations to the participants for an exciting weekend of planetary solutions.","title":"Stanford Big Earth Hackathon"},{"location":"projects/pbasemap/","text":"Planet-Mosaic-Quads-Download-CLI \u00b6 Planet creates global monthly mosaics apart from creating mosaics at different frequencies, monthly mosaics are of interest to a lot of people who would like to do a consistent time series analysis using these mosaics and would like to apply them to an existing analytical pipeline. I created this tool to allow you pass single or multiple geometries in a folder for the tool to find the mosaic quads and then process and download it. For now the geometry is passed as a geojson file, but I have included a tool for you to convert any shapefile into geojson files so you can use this tool. In the future I will add support for kml and json files as well. You can cite the tool using the following or from this link Samapriya Roy. (2019, June 25). samapriya/Planet-Mosaic-Quads-Download-CLI: Planet Mosaic Quads Download CLI (Version 0.1.0). Zenodo. http://doi.org/10.5281/zenodo.3255274 Table of contents \u00b6 Installation Getting started pbasemap Planet Mosaic Quads Download CLI bounding box mosaic list download mosaic multipart download mosaic shape to geojson Installation \u00b6 ** Install Fiona and GDAL for windows using the whl files here ** if there are any issues during installation. This assumes that you have native python & pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have tested this only on python 2.7.15 but it should run on python 3. To install pbasemap: Planet Mosaic Quads Download CLI you can install using two methods pip install pbasemap or you can also try git clone https://github.com/samapriya/Planet-Mosaic-Quads-Download-CLI.git cd pbasemap python setup.py install For linux use sudo and for windows right click the command prompt and run as admin . Installation is an optional step; the application can be also run directly by executing pbasemap.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. If you don't want to install, browse into the pbasemap folder and try python pbasemap.py to get to the same result. Getting started \u00b6 As usual, to print help: usage: pbasemap.py [-h] {rbox,mosaic_list,download,mpdownload,shp2geojson} ... Planet Mosaic Quads Download CLI positional arguments: {rbox,mosaic_list,download,mpdownload,shp2geojson} rbox Prints bounding box for geometry mosaic_list Tool to get Mosaic & Bounding Box list download Download quad GeoTiffs choose from name or idlist mpdownload Download quad GeoTiffs using multipart downloader shp2geojson Convert all shapefiles in folder to GeoJSON optional arguments: -h, --help show this help message and exit ``` To obtain help for a specific functionality, simply call it with _help_ switch, e.g.: `pbasemap shp2geojson -h`. If you didn't install pbasemap, then you can run it just by going to *pbasemap* directory and running `python pbasemap.py [arguments go here]` ## pbasemap Simple CLI for Basemaps API The tool allows you to list and download basemap quads that instersect with area of interest, and have controls such as date range and check for final coverage before download. The CLI also allows you to export the mosaics list as needed and can handle GeoJSON and KML files, and includes a tool to convert shapefiles to GeoJSON files for use with this tool. ### bounding box This tool simply prints the bounding box for any geometry feature that is passed. This is useful if you are using the planet CLI to downlaod quads which requires a bounding box.It prints out the bounding box for use. usage: pbasemap.py rbox [-h] [--geometry GEOMETRY] optional arguments: -h, --help show this help message and exit --geometry GEOMETRY Choose a geometry file supports GeoJSON, KML ### mosaic list This tool exports the mosaics name, id that intersect with your bounding box for your geometry. This can then be used to download the quads. usage: pbasemap.py mosaic_list [-h] [--geometry GEOMETRY] [--start START] [--end END] [--output OUTPUT] optional arguments: -h, --help show this help message and exit --geometry GEOMETRY Choose a geometry file supports GeoJSON, KML --start START Choose Start date in format YYYY-MM-DD --end END Choose End date in format YYYY-MM-DD --output OUTPUT Full path where you want your mosaic list exported ### download mosaic As the name suggests this downloads your mosaic to the local folder you specify, you can specify how much coverage you want over your geometry and over the quad. So you may decide to only download those mosaic quads that have coverage more than 90% by simply specifying ```--coverage 90``` in the arguments. Once you create the list of mosaics that intersect with your geometry, you should be able to use the idlist option to export them all. Since L15 qauds can have the same name, the name of the mosaic is prepended to the filename. usage: pbasemap.py download [-h] [--geometry GEOMETRY] [--local LOCAL] [--coverage COVERAGE] [--name NAME] [--idlist IDLIST] optional arguments: -h, --help show this help message and exit --geometry GEOMETRY Choose a geometry file supports GeoJSON, KML --local LOCAL Local folder to download images Optional named arguments: --coverage COVERAGE Choose minimum percentage coverage --name NAME Mosaic name from earlier search or csvfile --idlist IDLIST Mosaic list csvfile ### multipart download mosaic This uses a multipart downloader to download your mosaic to the local folder you specify, you can specify how much coverage you want over your geometry and over the quad. So you may decide to only download those mosaic quads that have coverage more than 90% by simply specifying ```--coverage 90``` in the arguments.Once you create the list of mosaics that intersect with your geometry, you should be able to use the idlist option to export them all. Since L15 qauds can have the same name, the name of the mosaic is prepended to the filename. usage: pbasemap.py mpdownload [-h] [--geometry GEOMETRY] [--local LOCAL] [--coverage COVERAGE] [--name NAME] [--idlist IDLIST] optional arguments: -h, --help show this help message and exit --geometry GEOMETRY Choose a geometry file supports GeoJSON, KML --local LOCAL Local folder to download images Optional named arguments: --coverage COVERAGE Choose minimum percentage coverage --name NAME Mosaic name from earlier search or csvfile --idlist IDLIST Mosaic list csvfile ### shape to geojson This tool allows you to convert from a folder with multiple shapefiles to a folder with geojson that can then be used with the tool. It makes use of geopandas and reprojects your shapefile to make it compatible while passing onto the API for search and download. usage: pbasemap shp2geojson [-h] [--source SOURCE] [--destination DESTINATION] optional arguments: -h, --help show this help message and exit --source SOURCE Choose Source Folder --destination DESTINATION Choose Destination Folder ``` Changelog \u00b6 v0.1.0 \u00b6 Improvements to installation Now creates folders to download mosaic quads Fixed issue with multipart downloader v0.0.8 \u00b6 Minor improvements Checks for download permission v0.0.7 \u00b6 Updated feedback, major changes to the codebase and underlying methodology Optimized code for search and download Overall improvements to code and major revisions v0.0.5 \u00b6 Complete change to the codebase and underlying methodology Optimized code for search and download Overall improvements to code and major revisions v0.0.4 \u00b6 Fixed projection issue for shapefiles Optimized code for shapefile to geojson export Overall improvements to code and minor revisions","title":"Planet-Mosaic-Quads-Download-CLI"},{"location":"projects/pbasemap/#planet-mosaic-quads-download-cli","text":"Planet creates global monthly mosaics apart from creating mosaics at different frequencies, monthly mosaics are of interest to a lot of people who would like to do a consistent time series analysis using these mosaics and would like to apply them to an existing analytical pipeline. I created this tool to allow you pass single or multiple geometries in a folder for the tool to find the mosaic quads and then process and download it. For now the geometry is passed as a geojson file, but I have included a tool for you to convert any shapefile into geojson files so you can use this tool. In the future I will add support for kml and json files as well. You can cite the tool using the following or from this link Samapriya Roy. (2019, June 25). samapriya/Planet-Mosaic-Quads-Download-CLI: Planet Mosaic Quads Download CLI (Version 0.1.0). Zenodo. http://doi.org/10.5281/zenodo.3255274","title":"Planet-Mosaic-Quads-Download-CLI"},{"location":"projects/pbasemap/#table-of-contents","text":"Installation Getting started pbasemap Planet Mosaic Quads Download CLI bounding box mosaic list download mosaic multipart download mosaic shape to geojson","title":"Table of contents"},{"location":"projects/pbasemap/#installation","text":"** Install Fiona and GDAL for windows using the whl files here ** if there are any issues during installation. This assumes that you have native python & pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have tested this only on python 2.7.15 but it should run on python 3. To install pbasemap: Planet Mosaic Quads Download CLI you can install using two methods pip install pbasemap or you can also try git clone https://github.com/samapriya/Planet-Mosaic-Quads-Download-CLI.git cd pbasemap python setup.py install For linux use sudo and for windows right click the command prompt and run as admin . Installation is an optional step; the application can be also run directly by executing pbasemap.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. If you don't want to install, browse into the pbasemap folder and try python pbasemap.py to get to the same result.","title":"Installation"},{"location":"projects/pbasemap/#getting-started","text":"As usual, to print help: usage: pbasemap.py [-h] {rbox,mosaic_list,download,mpdownload,shp2geojson} ... Planet Mosaic Quads Download CLI positional arguments: {rbox,mosaic_list,download,mpdownload,shp2geojson} rbox Prints bounding box for geometry mosaic_list Tool to get Mosaic & Bounding Box list download Download quad GeoTiffs choose from name or idlist mpdownload Download quad GeoTiffs using multipart downloader shp2geojson Convert all shapefiles in folder to GeoJSON optional arguments: -h, --help show this help message and exit ``` To obtain help for a specific functionality, simply call it with _help_ switch, e.g.: `pbasemap shp2geojson -h`. If you didn't install pbasemap, then you can run it just by going to *pbasemap* directory and running `python pbasemap.py [arguments go here]` ## pbasemap Simple CLI for Basemaps API The tool allows you to list and download basemap quads that instersect with area of interest, and have controls such as date range and check for final coverage before download. The CLI also allows you to export the mosaics list as needed and can handle GeoJSON and KML files, and includes a tool to convert shapefiles to GeoJSON files for use with this tool. ### bounding box This tool simply prints the bounding box for any geometry feature that is passed. This is useful if you are using the planet CLI to downlaod quads which requires a bounding box.It prints out the bounding box for use. usage: pbasemap.py rbox [-h] [--geometry GEOMETRY] optional arguments: -h, --help show this help message and exit --geometry GEOMETRY Choose a geometry file supports GeoJSON, KML ### mosaic list This tool exports the mosaics name, id that intersect with your bounding box for your geometry. This can then be used to download the quads. usage: pbasemap.py mosaic_list [-h] [--geometry GEOMETRY] [--start START] [--end END] [--output OUTPUT] optional arguments: -h, --help show this help message and exit --geometry GEOMETRY Choose a geometry file supports GeoJSON, KML --start START Choose Start date in format YYYY-MM-DD --end END Choose End date in format YYYY-MM-DD --output OUTPUT Full path where you want your mosaic list exported ### download mosaic As the name suggests this downloads your mosaic to the local folder you specify, you can specify how much coverage you want over your geometry and over the quad. So you may decide to only download those mosaic quads that have coverage more than 90% by simply specifying ```--coverage 90``` in the arguments. Once you create the list of mosaics that intersect with your geometry, you should be able to use the idlist option to export them all. Since L15 qauds can have the same name, the name of the mosaic is prepended to the filename. usage: pbasemap.py download [-h] [--geometry GEOMETRY] [--local LOCAL] [--coverage COVERAGE] [--name NAME] [--idlist IDLIST] optional arguments: -h, --help show this help message and exit --geometry GEOMETRY Choose a geometry file supports GeoJSON, KML --local LOCAL Local folder to download images Optional named arguments: --coverage COVERAGE Choose minimum percentage coverage --name NAME Mosaic name from earlier search or csvfile --idlist IDLIST Mosaic list csvfile ### multipart download mosaic This uses a multipart downloader to download your mosaic to the local folder you specify, you can specify how much coverage you want over your geometry and over the quad. So you may decide to only download those mosaic quads that have coverage more than 90% by simply specifying ```--coverage 90``` in the arguments.Once you create the list of mosaics that intersect with your geometry, you should be able to use the idlist option to export them all. Since L15 qauds can have the same name, the name of the mosaic is prepended to the filename. usage: pbasemap.py mpdownload [-h] [--geometry GEOMETRY] [--local LOCAL] [--coverage COVERAGE] [--name NAME] [--idlist IDLIST] optional arguments: -h, --help show this help message and exit --geometry GEOMETRY Choose a geometry file supports GeoJSON, KML --local LOCAL Local folder to download images Optional named arguments: --coverage COVERAGE Choose minimum percentage coverage --name NAME Mosaic name from earlier search or csvfile --idlist IDLIST Mosaic list csvfile ### shape to geojson This tool allows you to convert from a folder with multiple shapefiles to a folder with geojson that can then be used with the tool. It makes use of geopandas and reprojects your shapefile to make it compatible while passing onto the API for search and download. usage: pbasemap shp2geojson [-h] [--source SOURCE] [--destination DESTINATION] optional arguments: -h, --help show this help message and exit --source SOURCE Choose Source Folder --destination DESTINATION Choose Destination Folder ```","title":"Getting started"},{"location":"projects/pbasemap/#changelog","text":"","title":"Changelog"},{"location":"projects/pbasemap/#v010","text":"Improvements to installation Now creates folders to download mosaic quads Fixed issue with multipart downloader","title":"v0.1.0"},{"location":"projects/pbasemap/#v008","text":"Minor improvements Checks for download permission","title":"v0.0.8"},{"location":"projects/pbasemap/#v007","text":"Updated feedback, major changes to the codebase and underlying methodology Optimized code for search and download Overall improvements to code and major revisions","title":"v0.0.7"},{"location":"projects/pbasemap/#v005","text":"Complete change to the codebase and underlying methodology Optimized code for search and download Overall improvements to code and major revisions","title":"v0.0.5"},{"location":"projects/pbasemap/#v004","text":"Fixed projection issue for shapefiles Optimized code for shapefile to geojson export Overall improvements to code and minor revisions","title":"v0.0.4"},{"location":"projects/planet_batch_slack/","text":"Planet Batch & Slack Pipeline CLI \u00b6 \u00a9 Planet Labs(Full line up of Satellites) and Planet & Slack Technologies Logo For writing a readme file this time I have adapted a shared piece written for the medium article. The first part which is setting up the slack account, creating an application and a slack bot has been discussed in the article here . In the past I have written tools which act as pipelines for you to process single areas of interest at the time that could be chained, the need to write something that does a bit more heavy lifting arose. This command line interface(CLI) application was created to handle groups and teams that have multiple areas of interest and multiple input and output buckets and locations to function smoothly. I have integrated this to slack so you can be on the move while this task can be on a scheduler and update you when finished. Table of contents \u00b6 Installation Getting started Batch Approach to Structured JSON Batch Activation Batch Download and Balance Additional Tools Citation and Credits Changelog Installation \u00b6 The next step we will setup the Planet-Batch-Slack-Pipeline-CLI and integrate our previously built slack app for notifications. To setup the prerequisites you need to install the Planet Python API Client and Slack Python API Clients. * To install the tool you can go to the GitHub page at Planet-Batch-Slack-Pipeline-CLI . As always two of my favorite operating systems are Windows and Linux, and to install on Linux git clone https://github.com/samapriya/Planet-Batch-Slack-Pipeline-CLI.git cd Planet-Batch-Slack-Pipeline-CLI && sudo python setup.py install pip install -r requirements.txt for windows download the zip and after extraction go to the folder containing \"setup.py\" and open command prompt at that location and type python setup.py install pip install -r requirements.txt Now call the tool for the first time, by typing in pbatch -h . This will only work if you have python in the system path which you can test for opening up terminal or command prompt and typing python. Getting started \u00b6 Once the requirements have been satisfied the first thing we would setup would be the OAuth Keys we created. The tools consists of a bunch of slack tools as well including capability to just use this tool to send slack messages, attachments and clean up channel as needed. Planet Batch Tools and Slack Addons Interface The two critical setup tools to make Slack ready and integrated are the smain and sbot tools where you will enter the OAuth for the application and OAuth for the bot that you generated earlier. These are then stored into your session for future use, you can call them using pbatch smain Use the \" OAuth Access Token \" generated earlier . You can find the tutorial here If you want more control over being able to delete slack messages you would want to generate a token here and use that as the **pbatch smain** token. pbatch sbot Use \" _Bot User OAuth Access Token\" _ generated earlier . You can find the tutorial here Once this is done your bot is now setup to message you when a task is completed. In our case these are tied into individual tools within the batch toolkit we just installed. In case you ever forget your API keys or need it again you can access all installed applications/bots and oauth tokens here . To be clear these tools were designed based on what I thought was an effective way of looking at data, downloading them and chaining the processes together. They are still a set of individual tools to make sure that one operation is independent of the other and does not break in case of a problem. So a non-monolithic design in some sense to make sure the pieces work. We will go through each of them in the order of use pbatch planetkey is the obvious one which is your planet API key and will allow you to store this locally to a session. The _ aoijson tool is the same tool used in the Planet-EE CLI within the pbatch bundle allows you to bring any existing KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles to a structured geojson file, this is so that the Planet API can read the structured geojson which has additional filters such as cloud cover and range of dates. The tool can then allow you to convert the geojson file to include filters to be used with the Planet Data API._ Let us say we want to convert this map.geojson to a structured aoi.json from June 1 2017 to June 30 th 2017 with 15% cloud cover as our maximum . We would pass the following command pbatch.py aoijson --start \"2017-06-01\" --end \"2017-06-30\" --cloud \"0.15\" --inputfile \"GJSON\" --geo \"local path to map.geojson file\" --loc \"path where aoi.json output file will be created\" Batch Approach to Structured JSON \u00b6 This tool was then rewritten and included in the application to overcome two issues 1) Automatically detect the type of input file I am passing (For now it can automatically handle geojson, kml, shapefile and wkt files). The files are then saved in an output directory with the **filename_aoi.json . ** The tool can also read from a csv file and parse different start dates, end dates and cloud cover for different files and create structured jsons to multiple locations making an multi path export easy. The csv headers should be The csv file needs to have following headers and setup pathways start end cloud outdir C:\\demo\\dallas.geojson 2017-01-01 2017-01-02 0.15 C:\\demo C:\\demo\\denver.geojson 2017-01-01 2017-03-02 0.15 C:\\demo C:\\demo\\sfo.geojson 2017-01-01 2017-05-02 0.15 C:\\demo C:\\demo\\indy.geojson 2017-01-01 2017-09-02 0.15 C:\\demo Below is a folder based batch execution to convert multiple geojson files to structured json files Batch Activation \u00b6 This tool was rewritten to provide users with two options to activate their assets. They can either point the tool at a folder and select the item and asset combination or they can specify a CSV file which contains each asset and item type and path to the structured JSON file. A setup would be as simple as pbatch activate --indir \"path to folder with structured json files\" --asset \"item asset type example: PSOrthoTile analytic\" The csv file need to have headers pathways asset C:\\demo\\dallas_aoi.json PSOrthoTile analytic C:\\demo\\denver_aoi.json REOrthoTile analytic C:\\demo\\sfo_aoi.json PSScene4Band analytic C:\\demo\\indy_aoi.json PSScene4Band analytic_sr The tool generates a slack readout which includes the filename , the asset and item type and the number of item and asset combinations that have been requested for activation Batch Download and Balance \u00b6 We run the downloader tool to batch download these assets and again you can choose to have either a folder or a csv file containing path to the json files, the item asset combination and the output location. A simple setup is thus pbatch downloader --indir \"Pathway to your json file\" --asset \"PSOrthoTile analytic\" --outdir \"C:\\output-folder\" The csv file needs to have following headers and setup pathways directory asset C:\\demo\\dallas_aoi.json C:\\demo\\t1 PSOrthoTile analytic C:\\demo\\denver_aoi.json C:\\demo\\t1 REOrthoTile analytic C:\\demo\\sfo_aoi.json C:\\demo\\t1 PSOrthoTile analytic_xml C:\\demo\\indy_aoi.json C:\\demo\\t1 REOrthoTile analytic_xml Batch downloading using folder This tool is unique for a few reasons This can using the CSV sort to identify different pathways to strctured jsons in different locations, but it can also download different assets for each input file and write to different location each time. Meaning this can be of production value to teams who have different source folders and output buckets where they would want their data to be written. The tool also prints information of Number of assets already active, number of assets that could not be activated and the total number of assets. Incase number of assets active do not match those that can be activated it will wait and show you a progressbar before trying again. This is the load balancing for each input file while making sure you don't have to estimate wait times for large requests. The tool generates a slack message posted on your channel letting you keep track of downloads. Additional Tools \u00b6 Two things that keep changing are space (The amount of space needed to store your data) and the time since you may want to look at different time windows . With this in mind an easy way to update you about the total space for the assets you activated I created a tool called pbatch space . A simple setup would be pbatch space --indir \"Input directory with structured json file\" --asset \"PSOrthoTile analytic\" And it can also consume a csv file where the csv file need to have headers CSV Setup to estimate size of assets in GB pathways asset C:\\demo\\dallas_aoi.json PSOrthoTile analytic C:\\demo\\denver_aoi.json REOrthoTile analytic C:\\demo\\sfo_aoi.json PSScene4Band analytic C:\\demo\\indy_aoi.json PSScene4Band analytic_sr Slack will record the last time you ran this tool because new assets may have activated since you last ran this or new assets may have become available for you to activate. This tool is useful only after you have activated your assets. To quickly change start times on structured JSON I created another tool pbatch aoiupdate . Most often all your data needs can be considered as x number of days from whatever you sent . Meaning I may want to look at 30 days of data from the end date and we don't want to recreate the structured json files. Turns out we can easily change that using a time delta function and simply rewrite the start date for our json files from which to start looking for data. Note: Your end date should be current date or later the way the date is now written is date greater than equal to 30 days from today and end date remains constant A good rule of them to be safe with this tool is to save the end date into the future so for example. The setup maybe pbatch aoiupdate --indir \"directory\" --days 30 start date \"**2017-01-01**\" end date \"**2017-12-31**\" new start date \"**2017-10-26**\" new end date \"**2017-12-31**\" You can also create a CSV setup for this as well with different days for each JSON file CSV Setup to update structured JSON. Note: the number of days is calculated from current date pathways days C:\\demo\\dallas_aoi.json 3 C:\\demo\\denver_aoi.json 5 C:\\demo\\sfo_aoi.json 14 C:\\demo\\indy_aoi.json 23 Another tool that was solely written witht he purpose of integration with Google Earth Engine is the pbatch metadata which allows you to tabulate metadata and this is for use in conjunction with Google Earth Engine atleast for my workflow. Citation and Credits \u00b6 You can cite the tool as Samapriya Roy. (2017, December 4). samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.1079887 Thanks to the Planet Ambassador Program Changelog \u00b6 v0.1.4 \u00b6 Now handles CSV formatting for all tools Added CSV Example setups folder for use v0.1.3 (Pre-release) \u00b6 Handles issues with subprocess module and long wait v0.1.2 (Pre-release) \u00b6 Updates to parsing CSV, optimized handling of filetypes v0.1.1 (Pre-release) \u00b6 Updates to Batch download handler","title":"Planet Batch Slack Pipeline"},{"location":"projects/planet_batch_slack/#planet-batch-slack-pipeline-cli","text":"\u00a9 Planet Labs(Full line up of Satellites) and Planet & Slack Technologies Logo For writing a readme file this time I have adapted a shared piece written for the medium article. The first part which is setting up the slack account, creating an application and a slack bot has been discussed in the article here . In the past I have written tools which act as pipelines for you to process single areas of interest at the time that could be chained, the need to write something that does a bit more heavy lifting arose. This command line interface(CLI) application was created to handle groups and teams that have multiple areas of interest and multiple input and output buckets and locations to function smoothly. I have integrated this to slack so you can be on the move while this task can be on a scheduler and update you when finished.","title":"Planet Batch &amp; Slack Pipeline CLI"},{"location":"projects/planet_batch_slack/#table-of-contents","text":"Installation Getting started Batch Approach to Structured JSON Batch Activation Batch Download and Balance Additional Tools Citation and Credits Changelog","title":"Table of contents"},{"location":"projects/planet_batch_slack/#installation","text":"The next step we will setup the Planet-Batch-Slack-Pipeline-CLI and integrate our previously built slack app for notifications. To setup the prerequisites you need to install the Planet Python API Client and Slack Python API Clients. * To install the tool you can go to the GitHub page at Planet-Batch-Slack-Pipeline-CLI . As always two of my favorite operating systems are Windows and Linux, and to install on Linux git clone https://github.com/samapriya/Planet-Batch-Slack-Pipeline-CLI.git cd Planet-Batch-Slack-Pipeline-CLI && sudo python setup.py install pip install -r requirements.txt for windows download the zip and after extraction go to the folder containing \"setup.py\" and open command prompt at that location and type python setup.py install pip install -r requirements.txt Now call the tool for the first time, by typing in pbatch -h . This will only work if you have python in the system path which you can test for opening up terminal or command prompt and typing python.","title":"Installation"},{"location":"projects/planet_batch_slack/#getting-started","text":"Once the requirements have been satisfied the first thing we would setup would be the OAuth Keys we created. The tools consists of a bunch of slack tools as well including capability to just use this tool to send slack messages, attachments and clean up channel as needed. Planet Batch Tools and Slack Addons Interface The two critical setup tools to make Slack ready and integrated are the smain and sbot tools where you will enter the OAuth for the application and OAuth for the bot that you generated earlier. These are then stored into your session for future use, you can call them using pbatch smain Use the \" OAuth Access Token \" generated earlier . You can find the tutorial here If you want more control over being able to delete slack messages you would want to generate a token here and use that as the **pbatch smain** token. pbatch sbot Use \" _Bot User OAuth Access Token\" _ generated earlier . You can find the tutorial here Once this is done your bot is now setup to message you when a task is completed. In our case these are tied into individual tools within the batch toolkit we just installed. In case you ever forget your API keys or need it again you can access all installed applications/bots and oauth tokens here . To be clear these tools were designed based on what I thought was an effective way of looking at data, downloading them and chaining the processes together. They are still a set of individual tools to make sure that one operation is independent of the other and does not break in case of a problem. So a non-monolithic design in some sense to make sure the pieces work. We will go through each of them in the order of use pbatch planetkey is the obvious one which is your planet API key and will allow you to store this locally to a session. The _ aoijson tool is the same tool used in the Planet-EE CLI within the pbatch bundle allows you to bring any existing KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles to a structured geojson file, this is so that the Planet API can read the structured geojson which has additional filters such as cloud cover and range of dates. The tool can then allow you to convert the geojson file to include filters to be used with the Planet Data API._ Let us say we want to convert this map.geojson to a structured aoi.json from June 1 2017 to June 30 th 2017 with 15% cloud cover as our maximum . We would pass the following command pbatch.py aoijson --start \"2017-06-01\" --end \"2017-06-30\" --cloud \"0.15\" --inputfile \"GJSON\" --geo \"local path to map.geojson file\" --loc \"path where aoi.json output file will be created\"","title":"Getting started"},{"location":"projects/planet_batch_slack/#batch-approach-to-structured-json","text":"This tool was then rewritten and included in the application to overcome two issues 1) Automatically detect the type of input file I am passing (For now it can automatically handle geojson, kml, shapefile and wkt files). The files are then saved in an output directory with the **filename_aoi.json . ** The tool can also read from a csv file and parse different start dates, end dates and cloud cover for different files and create structured jsons to multiple locations making an multi path export easy. The csv headers should be The csv file needs to have following headers and setup pathways start end cloud outdir C:\\demo\\dallas.geojson 2017-01-01 2017-01-02 0.15 C:\\demo C:\\demo\\denver.geojson 2017-01-01 2017-03-02 0.15 C:\\demo C:\\demo\\sfo.geojson 2017-01-01 2017-05-02 0.15 C:\\demo C:\\demo\\indy.geojson 2017-01-01 2017-09-02 0.15 C:\\demo Below is a folder based batch execution to convert multiple geojson files to structured json files","title":"Batch Approach to Structured JSON"},{"location":"projects/planet_batch_slack/#batch-activation","text":"This tool was rewritten to provide users with two options to activate their assets. They can either point the tool at a folder and select the item and asset combination or they can specify a CSV file which contains each asset and item type and path to the structured JSON file. A setup would be as simple as pbatch activate --indir \"path to folder with structured json files\" --asset \"item asset type example: PSOrthoTile analytic\" The csv file need to have headers pathways asset C:\\demo\\dallas_aoi.json PSOrthoTile analytic C:\\demo\\denver_aoi.json REOrthoTile analytic C:\\demo\\sfo_aoi.json PSScene4Band analytic C:\\demo\\indy_aoi.json PSScene4Band analytic_sr The tool generates a slack readout which includes the filename , the asset and item type and the number of item and asset combinations that have been requested for activation","title":"Batch Activation"},{"location":"projects/planet_batch_slack/#batch-download-and-balance","text":"We run the downloader tool to batch download these assets and again you can choose to have either a folder or a csv file containing path to the json files, the item asset combination and the output location. A simple setup is thus pbatch downloader --indir \"Pathway to your json file\" --asset \"PSOrthoTile analytic\" --outdir \"C:\\output-folder\" The csv file needs to have following headers and setup pathways directory asset C:\\demo\\dallas_aoi.json C:\\demo\\t1 PSOrthoTile analytic C:\\demo\\denver_aoi.json C:\\demo\\t1 REOrthoTile analytic C:\\demo\\sfo_aoi.json C:\\demo\\t1 PSOrthoTile analytic_xml C:\\demo\\indy_aoi.json C:\\demo\\t1 REOrthoTile analytic_xml Batch downloading using folder This tool is unique for a few reasons This can using the CSV sort to identify different pathways to strctured jsons in different locations, but it can also download different assets for each input file and write to different location each time. Meaning this can be of production value to teams who have different source folders and output buckets where they would want their data to be written. The tool also prints information of Number of assets already active, number of assets that could not be activated and the total number of assets. Incase number of assets active do not match those that can be activated it will wait and show you a progressbar before trying again. This is the load balancing for each input file while making sure you don't have to estimate wait times for large requests. The tool generates a slack message posted on your channel letting you keep track of downloads.","title":"Batch Download and Balance"},{"location":"projects/planet_batch_slack/#additional-tools","text":"Two things that keep changing are space (The amount of space needed to store your data) and the time since you may want to look at different time windows . With this in mind an easy way to update you about the total space for the assets you activated I created a tool called pbatch space . A simple setup would be pbatch space --indir \"Input directory with structured json file\" --asset \"PSOrthoTile analytic\" And it can also consume a csv file where the csv file need to have headers CSV Setup to estimate size of assets in GB pathways asset C:\\demo\\dallas_aoi.json PSOrthoTile analytic C:\\demo\\denver_aoi.json REOrthoTile analytic C:\\demo\\sfo_aoi.json PSScene4Band analytic C:\\demo\\indy_aoi.json PSScene4Band analytic_sr Slack will record the last time you ran this tool because new assets may have activated since you last ran this or new assets may have become available for you to activate. This tool is useful only after you have activated your assets. To quickly change start times on structured JSON I created another tool pbatch aoiupdate . Most often all your data needs can be considered as x number of days from whatever you sent . Meaning I may want to look at 30 days of data from the end date and we don't want to recreate the structured json files. Turns out we can easily change that using a time delta function and simply rewrite the start date for our json files from which to start looking for data. Note: Your end date should be current date or later the way the date is now written is date greater than equal to 30 days from today and end date remains constant A good rule of them to be safe with this tool is to save the end date into the future so for example. The setup maybe pbatch aoiupdate --indir \"directory\" --days 30 start date \"**2017-01-01**\" end date \"**2017-12-31**\" new start date \"**2017-10-26**\" new end date \"**2017-12-31**\" You can also create a CSV setup for this as well with different days for each JSON file CSV Setup to update structured JSON. Note: the number of days is calculated from current date pathways days C:\\demo\\dallas_aoi.json 3 C:\\demo\\denver_aoi.json 5 C:\\demo\\sfo_aoi.json 14 C:\\demo\\indy_aoi.json 23 Another tool that was solely written witht he purpose of integration with Google Earth Engine is the pbatch metadata which allows you to tabulate metadata and this is for use in conjunction with Google Earth Engine atleast for my workflow.","title":"Additional Tools"},{"location":"projects/planet_batch_slack/#citation-and-credits","text":"You can cite the tool as Samapriya Roy. (2017, December 4). samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.1079887 Thanks to the Planet Ambassador Program","title":"Citation and Credits"},{"location":"projects/planet_batch_slack/#changelog","text":"","title":"Changelog"},{"location":"projects/planet_batch_slack/#v014","text":"Now handles CSV formatting for all tools Added CSV Example setups folder for use","title":"v0.1.4"},{"location":"projects/planet_batch_slack/#v013-pre-release","text":"Handles issues with subprocess module and long wait","title":"v0.1.3 (Pre-release)"},{"location":"projects/planet_batch_slack/#v012-pre-release","text":"Updates to parsing CSV, optimized handling of filetypes","title":"v0.1.2 (Pre-release)"},{"location":"projects/planet_batch_slack/#v011-pre-release","text":"Updates to Batch download handler","title":"v0.1.1 (Pre-release)"},{"location":"projects/planet_gee_pipeline_cli/","text":"Planet GEE Pipeline CLI \u00b6 While moving between assets from Planet Inc and Google Earth Engine it was imperative to create a pipeline that allows for easy transitions between the two service end points and this tool is designed to act as a step by step process chain from Planet Assets to batch upload and modification within the Google Earth Engine environment. The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. tab. Table of contents \u00b6 Installation Getting started Batch uploader Parsing metadata Usage examples Planet Tools Planet Key AOI JSON Activate or Check Asset Check Total size of assets Download Asset Metadata Parser Earth Engine Tools EE User Create Upload a directory with images and associate properties with each image: Upload a directory with images with specific NoData value to a selected destination: Asset List Asset Size Earth Engine Asset Report Task Query Task Report Delete a collection with content: Assets Move Assets Copy Assets Access Set Collection Property Cancel all tasks Credits Installation \u00b6 We assume Earth Engine Python API is installed and EE authorised as desribed here . We also assume Planet Python API is installed you can install by simply running. pip install planet Further instructions can be found here This toolbox also uses some functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa && sudo apt-get update sudo apt-get install gdal-bin For Windows I found this guide from UCLA To install Planet-GEE-Pipeline-CLI: git clone https://github.com/samapriya/Planet-GEE-Pipeline-CLI.git cd Planet-GEE-Pipeline-CLI && pip install -r requirements.txt for linux use sudo pip install -r requirements.txt This release also contains a windows installer which bypasses the need for you to have admin permission, it does however require you to have python in the system path meaning when you open up command prompt you should be able to type python and start it within the command prompt window. Post installation using the installer you can just call ppipe using the command prompt similar to calling python. Give it a go post installation type ppipe -h Installation is an optional step; the application can be also run directly by executing ppipe.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. To install run python setup.py develop or python setup.py install In a linux distribution sudo python setup.py develop or sudo python setup.py install Getting started \u00b6 As usual, to print help: usage: ppipe.py [-h] { ,planetkey,aoijson,activatepl,space,downloadpl,metadata,ee_user,create,upload,lst,collsize,delete,tasks,taskquery,report,cancel,mover,copy,access,collprop,cleanout} ... Planet Pipeline with Google Earth Engine Batch Addons positional arguments: { ,planetkey,aoijson,activatepl,space,downloadpl,metadata,ee_user,create,upload,lst,collsize,delete,tasks,taskquery,report,cancel,mover,copy,access,collprop,cleanout} --------------------------------------- -----Choose from Planet Tools Below----- --------------------------------------- planetkey Enter your planet API Key aoijson Tool to convert KML, Shapefile,WKT,GeoJSON or Landsat WRS PathRow file to AreaOfInterest.JSON file with structured query for use with Planet API 1.0 activatepl Tool to query and/or activate Planet Assets space Tool to query total download size of activated assets & local space left for download downloadpl Tool to download Planet Assets metadata Tool to tabulate and convert all metadata files from Planet or Digital Globe Assets ------------------------------------------- ----Choose from Earth Engine Tools Below---- ------------------------------------------- ee_user Get Earth Engine API Key & Paste it back to Command line/shell to change user create Allows the user to create an asset collection or folder in Google Earth Engine upload Batch Asset Uploader. lst List assets in a folder/collection or write as text file collsize Collects collection size in Human Readable form & Number of assets delete Deletes collection and all items inside. Supports Unix-like wildcards. tasks Queries currently running, enqued,failed taskquery Queries currently running, enqued,failed ingestions and uploaded assets report Create a report of all tasks and exports to a CSV file cancel Cancel all running tasks mover Moves all assets from one collection to another copy Copies all assets from one collection to another: Including copying from other users if you have read permission to their assets access Sets Permissions for Images, Collection or all assets in EE Folder Example: python ee_permissions.py --mode \"folder\" --asset \"users/john/doe\" --user \"jimmy@doe.com:R\" collprop Sets Overall Properties for Image Collection cleanout Clear folders with datasets from earlier downloaded optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: ppipe upload -h . If you didn't install ppipe, then you can run it just by going to ppipe directory and running python ppipe.py [arguments go here] Batch uploader \u00b6 The script creates an Image Collection from GeoTIFFs in your local directory. By default, the collection name is the same as the local directory name; with optional parameter you can provide a different name. Another optional parameter is a path to a CSV file with metadata for images, which is covered in the next section: Parsing metadata . usage: ppipe.py upload [-h] --source SOURCE --dest DEST [-m METADATA] [-mf MANIFEST] [--large] [--nodata NODATA] [-u USER] [-s SERVICE_ACCOUNT] [-k PRIVATE_KEY] [-b BUCKET] optional arguments: -h, --help show this help message and exit Required named arguments.: --source SOURCE Path to the directory with images for upload. --dest DEST Destination. Full path for upload to Google Earth Engine, e.g. users/pinkiepie/myponycollection -u USER, --user USER Google account name (gmail address). Optional named arguments: -m METADATA, --metadata METADATA Path to CSV with metadata. -mf MANIFEST, --manifest MANIFEST Manifest type to be used,for planetscope use \"planetscope\" --large (Advanced) Use multipart upload. Might help if upload of large files is failing on some systems. Might cause other issues. --nodata NODATA The value to burn into the raster as NoData (missing data) -s SERVICE_ACCOUNT, --service-account SERVICE_ACCOUNT Google Earth Engine service account. -k PRIVATE_KEY, --private-key PRIVATE_KEY Google Earth Engine private key file. -b BUCKET, --bucket BUCKET Google Cloud Storage bucket name. Parsing metadata \u00b6 By metadata we understand here the properties associated with each image. Thanks to these, GEE user can easily filter collection based on specified criteria. The file with metadata should be organised as follows: filename (without extension) property1 header property2 header file1 value1 value2 file2 value3 value4 Note that header can contain only letters, digits and underscores. Example: id_no class category binomial system:time_start my_file_1 GASTROPODA EN Aaadonta constricta 1478943081000 my_file_2 GASTROPODA CR Aaadonta irregularis 1478943081000 The corresponding files are my_file_1.tif and my_file_2.tif. With each of the files five properties are associated: id_no, class, category, binomial and system:time_start. The latter is time in Unix epoch format, in milliseconds, as documented in GEE glosary. The program will match the file names from the upload directory with ones provided in the CSV and pass the metadata in JSON format: { id_no: my_file_1, class: GASTROPODA, category: EN, binomial: Aaadonta constricta, system:time_start: 1478943081000} The program will report any illegal fields, it will also complain if not all of the images passed for upload have metadata associated. User can opt to ignore it, in which case some assets will have no properties. Having metadata helps in organising your asstets, but is not mandatory - you can skip it. Usage examples \u00b6 Usage examples have been segmented into two parts focusing on both planet tools as well as earth engine tools, earth engine tools include additional developments in CLI which allows you to recursively interact with their python API Planet Tools \u00b6 The Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications. Planet Key \u00b6 This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools usage: ppipe.py planetkey [-h] optional arguments: -h, --help show this help message and exit If using on a private machine the Key is saved as a csv file for all future runs of the tool. AOI JSON \u00b6 The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you. usage: ppipe.py aoijson [-h] [--start START] [--end END] [--cloud CLOUD] [--inputfile INPUTFILE] [--geo GEO] [--loc LOC] optional arguments: -h, --help show this help message and exit --start START Start date in YYYY-MM-DD? --end END End date in YYYY-MM-DD? --cloud CLOUD Maximum Cloud Cover(0-1) representing 0-100 --inputfile INPUTFILE Choose a kml/shapefile/geojson or WKT file for AOI(KML/SHP/GJSON/WKT) or WRS (6 digit RowPath Example: 023042) --geo GEO map.geojson/aoi.kml/aoi.shp/aoi.wkt file --loc LOC Location where aoi.json file is to be stored Activate or Check Asset \u00b6 The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier usage: ppipe.py activatepl [-h] [--aoi AOI] [--action ACTION] [--asst ASST] optional arguments: -h, --help show this help message and exit --aoi AOI Choose aoi.json file created earlier --action ACTION choose between check/activate --asst ASST Choose between planet asset types (PSOrthoTile analytic/REOrthoTile analytic/PSOrthoTile analytic_xml/REOrthoTile analytic_xml Check Total size of assets \u00b6 It is important to sometimes estimate the overall size of download before you can actually download activated assets. This tool allows you to estimate local storage available at any location and overall size of download in MB or GB. This tool makes use of an existing url get request to look at content size and estimate overall download size of download for the activated assets. usage: ppipe.py space [-h] [--aoi AOI] [--local LOCAL] [--asset ASSET] optional arguments: -h, --help show this help message and exit --aoi AOI Choose aoi.json file created earlier --local LOCAL local path where you are downloading assets --asset ASSET Choose between planet asset types (PSOrthoTile analytic/PSOrthoTile analytic_dn/PSOrthoTile visual/PSScene4Band analytic/PSScene4Band analytic_dn/PSScene3Band analytic/PSScene3Band analytic_dn/PSScene3Band visual/REOrthoTile analytic/REOrthoTile visual Download Asset \u00b6 Having metadata helps in organising your asstets, but is not mandatory - you can skip it. The downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types. usage: ppipe.py downloadpl [-h] [--aoi AOI] [--action ACTION] [--asst ASST] [--pathway PATHWAY] optional arguments: -h, --help show this help message and exit --aoi AOI Choose aoi.json file created earlier --action ACTION choose download --asst ASST Choose between planet asset types (PSOrthoTile analytic/REOrthoTile analytic/PSOrthoTile analytic_xml/REOrthoTile analytic_xml --pathway PATHWAY Folder Pathways where PlanetAssets are saved exampled ./PlanetScope ./RapidEye Metadata Parser \u00b6 The metadata tab is a more powerful tool and consists of metadata parsing for All PlanetScope and RapiEye Assets along with Digital Globe MultiSpectral and DigitalGlobe PanChromatic datasets. This was developed as a standalone to process xml metadata files from multiple sources and is important step is the user plans to upload these assets to Google Earth Engine. usage: ppipe.py metadata [-h] [--asset ASSET] [--mf MF] [--mfile MFILE] [--errorlog ERRORLOG] optional arguments: -h, --help show this help message and exit --asset ASSET Choose PS OrthoTile(PSO)|PS OrthoTile DN(PSO_DN)|PS OrthoTile Visual(PSO_V)|PS4Band Analytic(PS4B)|PS4Band DN(PS4B_DN)|PS3Band Analytic(PS3B)|PS3Band DN(PS3B_DN)|PS3Band Visual(PS3B_V)|RE OrthoTile (REO)|RE OrthoTile Visual(REO_V)|DigitalGlobe MultiSpectral(DGMS)|DigitalGlobe Panchromatic(DGP)? --mf MF Metadata folder? --mfile MFILE Metadata filename to be exported along with Path.csv --errorlog ERRORLOG Errorlog to be exported along with Path.csv Earth Engine Tools \u00b6 The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. This is also a seperate package for earth engine users to use and can be downloaded here EE User \u00b6 This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back Create \u00b6 This tool allows you to create a collection or folder in your earth engine root directory. The tool uses the system cli to achieve this and this has been included so as to reduce the need to switch between multiple tools and CLI. usage: ppipe.py create [-h] --typ TYP --path PATH optional arguments: -h, --help show this help message and exit --typ TYP Specify type: collection or folder --path PATH This is the path for the earth engine asset to be created full path is needsed eg: users/johndoe/collection Upload a directory with images to your myfolder/mycollection and associate properties with each image: \u00b6 ppipe upload -u johndoe@gmail.com --source path_to_directory_with_tif -m path_to_metadata.csv -mf maifest_type(ex:planetscope) --dest users/johndoe/myfolder/myponycollection The script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.You can also skip manifest for RapidEye imagery or any other source that does not require metadata field type handling. Upload a directory with images with specific NoData value to a selected destination \u00b6 ppipe upload -u johndoe@gmail.com --source path_to_directory_with_tif --dest users/johndoe/myfolder/myponycollection --nodata 222 In this case we need to supply full path to the destination, which is helpful when we upload to a shared folder. In the provided example we also burn value 222 into all rasters for missing data (NoData). Asset List \u00b6 This tool is designed to either print or output asset lists within folders or collections using earthengine ls tool functions. usage: geeadd.py lst [-h] --location LOCATION --typ TYP [--items ITEMS] [--output OUTPUT] optional arguments: -h, --help show this help message and exit Required named arguments.: --location LOCATION This it the location of your folder/collection --typ TYP Whether you want the list to be printed or output as text[print/report] Optional named arguments: --items ITEMS Number of items to list --output OUTPUT Folder location for report to be exported Asset Size \u00b6 This tool allows you to query the size of any Earth Engine asset[Images, Image Collections, Tables and Folders] and prints out the number of assets and total asset size in non-byte encoding meaning KB, MB, GB, TB depending on size. usage: geeadd assetsize [-h] --asset ASSET optional arguments: -h, --help show this help message and exit --asset ASSET Earth Engine Asset for which to get size properties Earth Engine Asset Report \u00b6 This tool recursively goes through all your assets(Includes Images, ImageCollection,Table,) and generates a report containing the following fields [Type,Asset Type, Path,Number of Assets,size(MB),unit,owner,readers,writers]. usage: geeadd.py ee_report [-h] --outfile OUTFILE optional arguments: -h, --help show this help message and exit --outfile OUTFILE This it the location of your report csv file A simple setup is the following geeadd --outfile \"C:\\johndoe\\report.csv\" Task Query \u00b6 This script counts all currently running and ready tasks along with failed tasks. usage: geeadd.py tasks [-h] optional arguments: -h, --help show this help message and exit geeadd.py tasks Task Report \u00b6 Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type usage: geeadd taskreport [-h] [--r R] optional arguments: -h, --help show this help message and exit --r R Folder Path where the reports will be saved Delete a collection with content: \u00b6 The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution! geeadd delete users/johndoe/test Console output: 2016-07-17 16:14:09,212 :: oauth2client.client :: INFO :: Attempting refresh to obtain initial access_token 2016-07-17 16:14:09,213 :: oauth2client.client :: INFO :: Refreshing access_token 2016-07-17 16:14:10,842 :: root :: INFO :: Attempting to delete collection test 2016-07-17 16:14:16,898 :: root :: INFO :: Collection users/johndoe/test removed Delete all directories / collections based on a Unix-like pattern \u00b6 geeadd delete users/johndoe/*weird[0-9]?name* Assets Move \u00b6 This script allows us to recursively move assets from one collection to the other. usage: geeadd.py mover [-h] [--assetpath ASSETPATH] [--finalpath FINALPATH] optional arguments: -h, --help show this help message and exit --assetpath ASSETPATH Existing path of assets --finalpath FINALPATH New path for assets geeadd.py mover --assetpath \"users/johndoe/myfolder/myponycollection\" --destination \"users/johndoe/myfolder/myotherponycollection\" Assets Copy \u00b6 This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections. usage: geeadd.py copy [-h] [--initial INITIAL] [--final FINAL] optional arguments: -h, --help show this help message and exit --initial INITIAL Existing path of assets --final FINAL New path for assets geeadd.py mover --initial \"users/johndoe/myfolder/myponycollection\" --final \"users/johndoe/myfolder/myotherponycollection\" Assets Access \u00b6 This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time. usage: geeadd access [-h] --mode MODE --asset ASSET --user USER optional arguments: -h, --help show this help message and exit --mode MODE This lets you select if you want to change permission or folder/collection/image --asset ASSET This is the path to the earth engine asset whose permission you are changing folder/collection/image --user USER This is the email address to whom you want to give read or write permission Usage: \"john@doe.com:R\" or \"john@doe.com:W\" R/W refers to read or write permission geeadd.py access --mode folder --asset \"folder/collection/image\" --user \"john@doe.com:R\" Set Collection Property \u00b6 This script is derived from the ee tool to set collection properties and will set overall properties for collection. usage: geeadd.py collprop [-h] [--coll COLL] [--p P] optional arguments: -h, --help show this help message and exit --coll COLL Path of Image Collection --p P \"system:description=Description\"/\"system:provider_url=url\"/\"sys tem:tags=tags\"/\"system:title=title Cancel all tasks \u00b6 This is a simpler tool, can be called directly from the earthengine cli as well earthengine cli command earthengine task cancel all usage: geeadd.py cancel [-h] optional arguments: -h, --help show this help message and exit Credits \u00b6 JetStream A portion of the work is suported by JetStream Grant TG-GEO160014. Also supported by Planet Labs Ambassador Program Original upload function adapted from Lukasz's asset manager tool Changelog \u00b6 v0.3.0 \u00b6 Allows for quiet authentication for use in Google Colab or non interactive environments Improved planet key entry and authentication protocols v0.2.91 \u00b6 Fixed issue with Surface Reflectance metadata and manifest lib Improved ingestion support for (PSScene4Band analytic_Sr)[PS4B_SR] v0.2.9 \u00b6 Fixed issues with generating id list Improved overall security of command calls v0.2.2 \u00b6 Major improvements to ingestion using manifest ingest in Google Earth Engine Contains manifest for all commonly used Planet Data item and asset combinations Added additional tool to Earth Engine Enhancement including quota check before upload to GEE v0.2.1 \u00b6 Fixed initialization loop issue v0.2.0 \u00b6 Metadata parser and Uploader Can now handle PlanetScope 4 Band Surface Reflectance Datasets General Improvements v0.1.9 \u00b6 Changes made to reflect updated GEE Addon tools general improvements v0.1.8 \u00b6 Minor fixes to parser and general improvements Planet Key is now stored in a configuration folder which is safer \"C:\\users.config\\planet\" Earth Engine now requires you to assign a field type for metadata meaning an alphanumeric column like satID cannot also have numeric values unless specified explicitly . Manifest option has been added to handle this (just use -mf \"planetscope\") Added capability to query download size and local disk capacity before downloading planet assets. Added the list function to generate list of collections or folders including reports Added the collection size tool which allows you to estimate total size or quota used from your allocated quota. ogr2ft feature is removed since Earth Engine now allows vector and table uploading.","title":"Planet-Google Earth Engine Pipeline CLI"},{"location":"projects/planet_gee_pipeline_cli/#planet-gee-pipeline-cli","text":"While moving between assets from Planet Inc and Google Earth Engine it was imperative to create a pipeline that allows for easy transitions between the two service end points and this tool is designed to act as a step by step process chain from Planet Assets to batch upload and modification within the Google Earth Engine environment. The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. tab.","title":"Planet GEE Pipeline CLI"},{"location":"projects/planet_gee_pipeline_cli/#table-of-contents","text":"Installation Getting started Batch uploader Parsing metadata Usage examples Planet Tools Planet Key AOI JSON Activate or Check Asset Check Total size of assets Download Asset Metadata Parser Earth Engine Tools EE User Create Upload a directory with images and associate properties with each image: Upload a directory with images with specific NoData value to a selected destination: Asset List Asset Size Earth Engine Asset Report Task Query Task Report Delete a collection with content: Assets Move Assets Copy Assets Access Set Collection Property Cancel all tasks Credits","title":"Table of contents"},{"location":"projects/planet_gee_pipeline_cli/#installation","text":"We assume Earth Engine Python API is installed and EE authorised as desribed here . We also assume Planet Python API is installed you can install by simply running. pip install planet Further instructions can be found here This toolbox also uses some functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa && sudo apt-get update sudo apt-get install gdal-bin For Windows I found this guide from UCLA To install Planet-GEE-Pipeline-CLI: git clone https://github.com/samapriya/Planet-GEE-Pipeline-CLI.git cd Planet-GEE-Pipeline-CLI && pip install -r requirements.txt for linux use sudo pip install -r requirements.txt This release also contains a windows installer which bypasses the need for you to have admin permission, it does however require you to have python in the system path meaning when you open up command prompt you should be able to type python and start it within the command prompt window. Post installation using the installer you can just call ppipe using the command prompt similar to calling python. Give it a go post installation type ppipe -h Installation is an optional step; the application can be also run directly by executing ppipe.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. To install run python setup.py develop or python setup.py install In a linux distribution sudo python setup.py develop or sudo python setup.py install","title":"Installation"},{"location":"projects/planet_gee_pipeline_cli/#getting-started","text":"As usual, to print help: usage: ppipe.py [-h] { ,planetkey,aoijson,activatepl,space,downloadpl,metadata,ee_user,create,upload,lst,collsize,delete,tasks,taskquery,report,cancel,mover,copy,access,collprop,cleanout} ... Planet Pipeline with Google Earth Engine Batch Addons positional arguments: { ,planetkey,aoijson,activatepl,space,downloadpl,metadata,ee_user,create,upload,lst,collsize,delete,tasks,taskquery,report,cancel,mover,copy,access,collprop,cleanout} --------------------------------------- -----Choose from Planet Tools Below----- --------------------------------------- planetkey Enter your planet API Key aoijson Tool to convert KML, Shapefile,WKT,GeoJSON or Landsat WRS PathRow file to AreaOfInterest.JSON file with structured query for use with Planet API 1.0 activatepl Tool to query and/or activate Planet Assets space Tool to query total download size of activated assets & local space left for download downloadpl Tool to download Planet Assets metadata Tool to tabulate and convert all metadata files from Planet or Digital Globe Assets ------------------------------------------- ----Choose from Earth Engine Tools Below---- ------------------------------------------- ee_user Get Earth Engine API Key & Paste it back to Command line/shell to change user create Allows the user to create an asset collection or folder in Google Earth Engine upload Batch Asset Uploader. lst List assets in a folder/collection or write as text file collsize Collects collection size in Human Readable form & Number of assets delete Deletes collection and all items inside. Supports Unix-like wildcards. tasks Queries currently running, enqued,failed taskquery Queries currently running, enqued,failed ingestions and uploaded assets report Create a report of all tasks and exports to a CSV file cancel Cancel all running tasks mover Moves all assets from one collection to another copy Copies all assets from one collection to another: Including copying from other users if you have read permission to their assets access Sets Permissions for Images, Collection or all assets in EE Folder Example: python ee_permissions.py --mode \"folder\" --asset \"users/john/doe\" --user \"jimmy@doe.com:R\" collprop Sets Overall Properties for Image Collection cleanout Clear folders with datasets from earlier downloaded optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: ppipe upload -h . If you didn't install ppipe, then you can run it just by going to ppipe directory and running python ppipe.py [arguments go here]","title":"Getting started"},{"location":"projects/planet_gee_pipeline_cli/#batch-uploader","text":"The script creates an Image Collection from GeoTIFFs in your local directory. By default, the collection name is the same as the local directory name; with optional parameter you can provide a different name. Another optional parameter is a path to a CSV file with metadata for images, which is covered in the next section: Parsing metadata . usage: ppipe.py upload [-h] --source SOURCE --dest DEST [-m METADATA] [-mf MANIFEST] [--large] [--nodata NODATA] [-u USER] [-s SERVICE_ACCOUNT] [-k PRIVATE_KEY] [-b BUCKET] optional arguments: -h, --help show this help message and exit Required named arguments.: --source SOURCE Path to the directory with images for upload. --dest DEST Destination. Full path for upload to Google Earth Engine, e.g. users/pinkiepie/myponycollection -u USER, --user USER Google account name (gmail address). Optional named arguments: -m METADATA, --metadata METADATA Path to CSV with metadata. -mf MANIFEST, --manifest MANIFEST Manifest type to be used,for planetscope use \"planetscope\" --large (Advanced) Use multipart upload. Might help if upload of large files is failing on some systems. Might cause other issues. --nodata NODATA The value to burn into the raster as NoData (missing data) -s SERVICE_ACCOUNT, --service-account SERVICE_ACCOUNT Google Earth Engine service account. -k PRIVATE_KEY, --private-key PRIVATE_KEY Google Earth Engine private key file. -b BUCKET, --bucket BUCKET Google Cloud Storage bucket name.","title":"Batch uploader"},{"location":"projects/planet_gee_pipeline_cli/#parsing-metadata","text":"By metadata we understand here the properties associated with each image. Thanks to these, GEE user can easily filter collection based on specified criteria. The file with metadata should be organised as follows: filename (without extension) property1 header property2 header file1 value1 value2 file2 value3 value4 Note that header can contain only letters, digits and underscores. Example: id_no class category binomial system:time_start my_file_1 GASTROPODA EN Aaadonta constricta 1478943081000 my_file_2 GASTROPODA CR Aaadonta irregularis 1478943081000 The corresponding files are my_file_1.tif and my_file_2.tif. With each of the files five properties are associated: id_no, class, category, binomial and system:time_start. The latter is time in Unix epoch format, in milliseconds, as documented in GEE glosary. The program will match the file names from the upload directory with ones provided in the CSV and pass the metadata in JSON format: { id_no: my_file_1, class: GASTROPODA, category: EN, binomial: Aaadonta constricta, system:time_start: 1478943081000} The program will report any illegal fields, it will also complain if not all of the images passed for upload have metadata associated. User can opt to ignore it, in which case some assets will have no properties. Having metadata helps in organising your asstets, but is not mandatory - you can skip it.","title":"Parsing metadata"},{"location":"projects/planet_gee_pipeline_cli/#usage-examples","text":"Usage examples have been segmented into two parts focusing on both planet tools as well as earth engine tools, earth engine tools include additional developments in CLI which allows you to recursively interact with their python API","title":"Usage examples"},{"location":"projects/planet_gee_pipeline_cli/#planet-tools","text":"The Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.","title":"Planet Tools"},{"location":"projects/planet_gee_pipeline_cli/#planet-key","text":"This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools usage: ppipe.py planetkey [-h] optional arguments: -h, --help show this help message and exit If using on a private machine the Key is saved as a csv file for all future runs of the tool.","title":"Planet Key"},{"location":"projects/planet_gee_pipeline_cli/#aoi-json","text":"The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you. usage: ppipe.py aoijson [-h] [--start START] [--end END] [--cloud CLOUD] [--inputfile INPUTFILE] [--geo GEO] [--loc LOC] optional arguments: -h, --help show this help message and exit --start START Start date in YYYY-MM-DD? --end END End date in YYYY-MM-DD? --cloud CLOUD Maximum Cloud Cover(0-1) representing 0-100 --inputfile INPUTFILE Choose a kml/shapefile/geojson or WKT file for AOI(KML/SHP/GJSON/WKT) or WRS (6 digit RowPath Example: 023042) --geo GEO map.geojson/aoi.kml/aoi.shp/aoi.wkt file --loc LOC Location where aoi.json file is to be stored","title":"AOI JSON"},{"location":"projects/planet_gee_pipeline_cli/#activate-or-check-asset","text":"The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier usage: ppipe.py activatepl [-h] [--aoi AOI] [--action ACTION] [--asst ASST] optional arguments: -h, --help show this help message and exit --aoi AOI Choose aoi.json file created earlier --action ACTION choose between check/activate --asst ASST Choose between planet asset types (PSOrthoTile analytic/REOrthoTile analytic/PSOrthoTile analytic_xml/REOrthoTile analytic_xml","title":"Activate or Check Asset"},{"location":"projects/planet_gee_pipeline_cli/#check-total-size-of-assets","text":"It is important to sometimes estimate the overall size of download before you can actually download activated assets. This tool allows you to estimate local storage available at any location and overall size of download in MB or GB. This tool makes use of an existing url get request to look at content size and estimate overall download size of download for the activated assets. usage: ppipe.py space [-h] [--aoi AOI] [--local LOCAL] [--asset ASSET] optional arguments: -h, --help show this help message and exit --aoi AOI Choose aoi.json file created earlier --local LOCAL local path where you are downloading assets --asset ASSET Choose between planet asset types (PSOrthoTile analytic/PSOrthoTile analytic_dn/PSOrthoTile visual/PSScene4Band analytic/PSScene4Band analytic_dn/PSScene3Band analytic/PSScene3Band analytic_dn/PSScene3Band visual/REOrthoTile analytic/REOrthoTile visual","title":"Check Total size of assets"},{"location":"projects/planet_gee_pipeline_cli/#download-asset","text":"Having metadata helps in organising your asstets, but is not mandatory - you can skip it. The downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types. usage: ppipe.py downloadpl [-h] [--aoi AOI] [--action ACTION] [--asst ASST] [--pathway PATHWAY] optional arguments: -h, --help show this help message and exit --aoi AOI Choose aoi.json file created earlier --action ACTION choose download --asst ASST Choose between planet asset types (PSOrthoTile analytic/REOrthoTile analytic/PSOrthoTile analytic_xml/REOrthoTile analytic_xml --pathway PATHWAY Folder Pathways where PlanetAssets are saved exampled ./PlanetScope ./RapidEye","title":"Download Asset"},{"location":"projects/planet_gee_pipeline_cli/#metadata-parser","text":"The metadata tab is a more powerful tool and consists of metadata parsing for All PlanetScope and RapiEye Assets along with Digital Globe MultiSpectral and DigitalGlobe PanChromatic datasets. This was developed as a standalone to process xml metadata files from multiple sources and is important step is the user plans to upload these assets to Google Earth Engine. usage: ppipe.py metadata [-h] [--asset ASSET] [--mf MF] [--mfile MFILE] [--errorlog ERRORLOG] optional arguments: -h, --help show this help message and exit --asset ASSET Choose PS OrthoTile(PSO)|PS OrthoTile DN(PSO_DN)|PS OrthoTile Visual(PSO_V)|PS4Band Analytic(PS4B)|PS4Band DN(PS4B_DN)|PS3Band Analytic(PS3B)|PS3Band DN(PS3B_DN)|PS3Band Visual(PS3B_V)|RE OrthoTile (REO)|RE OrthoTile Visual(REO_V)|DigitalGlobe MultiSpectral(DGMS)|DigitalGlobe Panchromatic(DGP)? --mf MF Metadata folder? --mfile MFILE Metadata filename to be exported along with Path.csv --errorlog ERRORLOG Errorlog to be exported along with Path.csv","title":"Metadata Parser"},{"location":"projects/planet_gee_pipeline_cli/#earth-engine-tools","text":"The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. This is also a seperate package for earth engine users to use and can be downloaded here","title":"Earth Engine Tools"},{"location":"projects/planet_gee_pipeline_cli/#ee-user","text":"This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back","title":"EE User"},{"location":"projects/planet_gee_pipeline_cli/#create","text":"This tool allows you to create a collection or folder in your earth engine root directory. The tool uses the system cli to achieve this and this has been included so as to reduce the need to switch between multiple tools and CLI. usage: ppipe.py create [-h] --typ TYP --path PATH optional arguments: -h, --help show this help message and exit --typ TYP Specify type: collection or folder --path PATH This is the path for the earth engine asset to be created full path is needsed eg: users/johndoe/collection","title":"Create"},{"location":"projects/planet_gee_pipeline_cli/#upload-a-directory-with-images-to-your-myfoldermycollection-and-associate-properties-with-each-image","text":"ppipe upload -u johndoe@gmail.com --source path_to_directory_with_tif -m path_to_metadata.csv -mf maifest_type(ex:planetscope) --dest users/johndoe/myfolder/myponycollection The script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.You can also skip manifest for RapidEye imagery or any other source that does not require metadata field type handling.","title":"Upload a directory with images to your myfolder/mycollection and associate properties with each image:"},{"location":"projects/planet_gee_pipeline_cli/#upload-a-directory-with-images-with-specific-nodata-value-to-a-selected-destination","text":"ppipe upload -u johndoe@gmail.com --source path_to_directory_with_tif --dest users/johndoe/myfolder/myponycollection --nodata 222 In this case we need to supply full path to the destination, which is helpful when we upload to a shared folder. In the provided example we also burn value 222 into all rasters for missing data (NoData).","title":"Upload a directory with images with specific NoData value to a selected destination"},{"location":"projects/planet_gee_pipeline_cli/#asset-list","text":"This tool is designed to either print or output asset lists within folders or collections using earthengine ls tool functions. usage: geeadd.py lst [-h] --location LOCATION --typ TYP [--items ITEMS] [--output OUTPUT] optional arguments: -h, --help show this help message and exit Required named arguments.: --location LOCATION This it the location of your folder/collection --typ TYP Whether you want the list to be printed or output as text[print/report] Optional named arguments: --items ITEMS Number of items to list --output OUTPUT Folder location for report to be exported","title":"Asset List"},{"location":"projects/planet_gee_pipeline_cli/#asset-size","text":"This tool allows you to query the size of any Earth Engine asset[Images, Image Collections, Tables and Folders] and prints out the number of assets and total asset size in non-byte encoding meaning KB, MB, GB, TB depending on size. usage: geeadd assetsize [-h] --asset ASSET optional arguments: -h, --help show this help message and exit --asset ASSET Earth Engine Asset for which to get size properties","title":"Asset Size"},{"location":"projects/planet_gee_pipeline_cli/#earth-engine-asset-report","text":"This tool recursively goes through all your assets(Includes Images, ImageCollection,Table,) and generates a report containing the following fields [Type,Asset Type, Path,Number of Assets,size(MB),unit,owner,readers,writers]. usage: geeadd.py ee_report [-h] --outfile OUTFILE optional arguments: -h, --help show this help message and exit --outfile OUTFILE This it the location of your report csv file A simple setup is the following geeadd --outfile \"C:\\johndoe\\report.csv\"","title":"Earth Engine Asset Report"},{"location":"projects/planet_gee_pipeline_cli/#task-query","text":"This script counts all currently running and ready tasks along with failed tasks. usage: geeadd.py tasks [-h] optional arguments: -h, --help show this help message and exit geeadd.py tasks","title":"Task Query"},{"location":"projects/planet_gee_pipeline_cli/#task-report","text":"Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type usage: geeadd taskreport [-h] [--r R] optional arguments: -h, --help show this help message and exit --r R Folder Path where the reports will be saved","title":"Task Report"},{"location":"projects/planet_gee_pipeline_cli/#delete-a-collection-with-content","text":"The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution! geeadd delete users/johndoe/test Console output: 2016-07-17 16:14:09,212 :: oauth2client.client :: INFO :: Attempting refresh to obtain initial access_token 2016-07-17 16:14:09,213 :: oauth2client.client :: INFO :: Refreshing access_token 2016-07-17 16:14:10,842 :: root :: INFO :: Attempting to delete collection test 2016-07-17 16:14:16,898 :: root :: INFO :: Collection users/johndoe/test removed","title":"Delete a collection with content:"},{"location":"projects/planet_gee_pipeline_cli/#delete-all-directories-collections-based-on-a-unix-like-pattern","text":"geeadd delete users/johndoe/*weird[0-9]?name*","title":"Delete all directories / collections based on a Unix-like pattern"},{"location":"projects/planet_gee_pipeline_cli/#assets-move","text":"This script allows us to recursively move assets from one collection to the other. usage: geeadd.py mover [-h] [--assetpath ASSETPATH] [--finalpath FINALPATH] optional arguments: -h, --help show this help message and exit --assetpath ASSETPATH Existing path of assets --finalpath FINALPATH New path for assets geeadd.py mover --assetpath \"users/johndoe/myfolder/myponycollection\" --destination \"users/johndoe/myfolder/myotherponycollection\"","title":"Assets Move"},{"location":"projects/planet_gee_pipeline_cli/#assets-copy","text":"This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections. usage: geeadd.py copy [-h] [--initial INITIAL] [--final FINAL] optional arguments: -h, --help show this help message and exit --initial INITIAL Existing path of assets --final FINAL New path for assets geeadd.py mover --initial \"users/johndoe/myfolder/myponycollection\" --final \"users/johndoe/myfolder/myotherponycollection\"","title":"Assets Copy"},{"location":"projects/planet_gee_pipeline_cli/#assets-access","text":"This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time. usage: geeadd access [-h] --mode MODE --asset ASSET --user USER optional arguments: -h, --help show this help message and exit --mode MODE This lets you select if you want to change permission or folder/collection/image --asset ASSET This is the path to the earth engine asset whose permission you are changing folder/collection/image --user USER This is the email address to whom you want to give read or write permission Usage: \"john@doe.com:R\" or \"john@doe.com:W\" R/W refers to read or write permission geeadd.py access --mode folder --asset \"folder/collection/image\" --user \"john@doe.com:R\"","title":"Assets Access"},{"location":"projects/planet_gee_pipeline_cli/#set-collection-property","text":"This script is derived from the ee tool to set collection properties and will set overall properties for collection. usage: geeadd.py collprop [-h] [--coll COLL] [--p P] optional arguments: -h, --help show this help message and exit --coll COLL Path of Image Collection --p P \"system:description=Description\"/\"system:provider_url=url\"/\"sys tem:tags=tags\"/\"system:title=title","title":"Set Collection Property"},{"location":"projects/planet_gee_pipeline_cli/#cancel-all-tasks","text":"This is a simpler tool, can be called directly from the earthengine cli as well earthengine cli command earthengine task cancel all usage: geeadd.py cancel [-h] optional arguments: -h, --help show this help message and exit","title":"Cancel all tasks"},{"location":"projects/planet_gee_pipeline_cli/#credits","text":"JetStream A portion of the work is suported by JetStream Grant TG-GEO160014. Also supported by Planet Labs Ambassador Program Original upload function adapted from Lukasz's asset manager tool","title":"Credits"},{"location":"projects/planet_gee_pipeline_cli/#changelog","text":"","title":"Changelog"},{"location":"projects/planet_gee_pipeline_cli/#v030","text":"Allows for quiet authentication for use in Google Colab or non interactive environments Improved planet key entry and authentication protocols","title":"v0.3.0"},{"location":"projects/planet_gee_pipeline_cli/#v0291","text":"Fixed issue with Surface Reflectance metadata and manifest lib Improved ingestion support for (PSScene4Band analytic_Sr)[PS4B_SR]","title":"v0.2.91"},{"location":"projects/planet_gee_pipeline_cli/#v029","text":"Fixed issues with generating id list Improved overall security of command calls","title":"v0.2.9"},{"location":"projects/planet_gee_pipeline_cli/#v022","text":"Major improvements to ingestion using manifest ingest in Google Earth Engine Contains manifest for all commonly used Planet Data item and asset combinations Added additional tool to Earth Engine Enhancement including quota check before upload to GEE","title":"v0.2.2"},{"location":"projects/planet_gee_pipeline_cli/#v021","text":"Fixed initialization loop issue","title":"v0.2.1"},{"location":"projects/planet_gee_pipeline_cli/#v020","text":"Metadata parser and Uploader Can now handle PlanetScope 4 Band Surface Reflectance Datasets General Improvements","title":"v0.2.0"},{"location":"projects/planet_gee_pipeline_cli/#v019","text":"Changes made to reflect updated GEE Addon tools general improvements","title":"v0.1.9"},{"location":"projects/planet_gee_pipeline_cli/#v018","text":"Minor fixes to parser and general improvements Planet Key is now stored in a configuration folder which is safer \"C:\\users.config\\planet\" Earth Engine now requires you to assign a field type for metadata meaning an alphanumeric column like satID cannot also have numeric values unless specified explicitly . Manifest option has been added to handle this (just use -mf \"planetscope\") Added capability to query download size and local disk capacity before downloading planet assets. Added the list function to generate list of collections or folders including reports Added the collection size tool which allows you to estimate total size or quota used from your allocated quota. ogr2ft feature is removed since Earth Engine now allows vector and table uploading.","title":"v0.1.8"},{"location":"projects/planet_gee_pipeline_gui/","text":"Planet GEE Pipeline GUI \u00b6 The Planet Pipeline GUI came from the actual CLI (command line interface tools) to enable the use of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications. Table of contents \u00b6 Installation Usage examples Planet Tools Planet Key AOI JSON Activate or Check Asset Download Asset Metadata Parser Earth Engine Tools EE User Upload a directory with images and associate properties with each image: Upload a directory with images with specific NoData value to a selected destination: Task Query Task Query during ingestion Task Report Delete a collection with content: Assets Move Assets Copy Assets Access Set Collection Property Convert to Fusion Table Cleanup Utility Cancel all tasks Credits Installation \u00b6 We assume Earth Engine Python API is installed and EE authorised as desribed here . We also assume Planet Python API is installed you can install by simply running pip install planet Further instructions can be found here You require two important packages for this to run WxPython(which is what the GUI is built on) for windows(Tested in Windows 10) https://wxpython.org/download.php pip install wxPython for linux(Tested in Ubuntu 16) sudo add-apt-repository \"deb http://archive.ubuntu.com/ubuntu utopic main restricted universe\" sudo apt-get update apt-cache search python-wxgtk3.0 sudo apt-get install python-wxgtk3.0 This toolbox also uses some functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa && sudo apt-get update sudo apt-get install gdal-bin For Windows I found this guide from UCLA Usage examples \u00b6 Usage examples have been segmented into two parts focusing on both planet tools as well as earth engine tools, earth engine tools include additional developments in CLI which allows you to recursively interact with their python API. To run the tool open a command prompt window or terminal and type python ee_ppipe.pyc Planet Tools \u00b6 The Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications. Planet Key \u00b6 This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools If using on a private machine the Key is saved as a csv file for all future runs of the tool. AOI JSON \u00b6 The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you. Activate or Check Asset \u00b6 The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier Download Asset \u00b6 Having metadata helps in organising your asstets, but is not mandatory - you can skip it. The downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types. Metadata Parser \u00b6 The metadata tab is a more powerful tool and consists of metadata parsing for PlanetScope OrthoTile RapiEye OrthoTile along with Digital Globe MultiSpectral and DigitalGlobe PanChromatic datasets. This was developed as a standalone to process xml metadata files from multiple sources and is important step is the user plans to upload these assets to Google Earth Engine. The combine Planet-GEE Pipeline tool will be made available soon for testing. Earth Engine Tools \u00b6 The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. This is also a seperate package for earth engine users to use and can be downloaded here EE User \u00b6 This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back Upload a directory with images to your myfolder/mycollection and associate properties with each image: \u00b6 The script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option. Task Query \u00b6 This script counts all currently running and ready tasks along with failed tasks. Task Query during ingestion \u00b6 This script can be used intermittently to look at running, failed and ready(waiting) tasks during ingestion. This script is a special case using query tasks only when uploading assets to collection by providing collection pathway to see how collection size increases. Task Report \u00b6 Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type Delete a collection with content: \u00b6 The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution! Assets Move \u00b6 This script allows us to recursively move assets from one collection to the other. Assets Copy \u00b6 This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections. Assets Access \u00b6 This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time. Set Collection Property \u00b6 This script is derived from the ee tool to set collection properties and will set overall properties for collection. Convert to Fusion Table \u00b6 Once validated with gdal and google fusion table it can be used to convert any geoObject to google fusion table. Forked and contributed by Gennadii here . The scripts can be used only with a specific google account Cleanup Utility \u00b6 This script is used to clean folders once all processes have been completed. In short this is a function to clear folder on local machine. Cancel all tasks \u00b6 This is a simpler tool, can be called directly from the earthengine cli as well Credits \u00b6 JetStream A portion of the work is suported by JetStream Grant TG-GEO160014. Also supported by Planet Labs Ambassador Program","title":"Planet-Google Earth Engine Pipeline GUI"},{"location":"projects/planet_gee_pipeline_gui/#planet-gee-pipeline-gui","text":"The Planet Pipeline GUI came from the actual CLI (command line interface tools) to enable the use of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.","title":"Planet GEE Pipeline GUI"},{"location":"projects/planet_gee_pipeline_gui/#table-of-contents","text":"Installation Usage examples Planet Tools Planet Key AOI JSON Activate or Check Asset Download Asset Metadata Parser Earth Engine Tools EE User Upload a directory with images and associate properties with each image: Upload a directory with images with specific NoData value to a selected destination: Task Query Task Query during ingestion Task Report Delete a collection with content: Assets Move Assets Copy Assets Access Set Collection Property Convert to Fusion Table Cleanup Utility Cancel all tasks Credits","title":"Table of contents"},{"location":"projects/planet_gee_pipeline_gui/#installation","text":"We assume Earth Engine Python API is installed and EE authorised as desribed here . We also assume Planet Python API is installed you can install by simply running pip install planet Further instructions can be found here You require two important packages for this to run WxPython(which is what the GUI is built on) for windows(Tested in Windows 10) https://wxpython.org/download.php pip install wxPython for linux(Tested in Ubuntu 16) sudo add-apt-repository \"deb http://archive.ubuntu.com/ubuntu utopic main restricted universe\" sudo apt-get update apt-cache search python-wxgtk3.0 sudo apt-get install python-wxgtk3.0 This toolbox also uses some functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa && sudo apt-get update sudo apt-get install gdal-bin For Windows I found this guide from UCLA","title":"Installation"},{"location":"projects/planet_gee_pipeline_gui/#usage-examples","text":"Usage examples have been segmented into two parts focusing on both planet tools as well as earth engine tools, earth engine tools include additional developments in CLI which allows you to recursively interact with their python API. To run the tool open a command prompt window or terminal and type python ee_ppipe.pyc","title":"Usage examples"},{"location":"projects/planet_gee_pipeline_gui/#planet-tools","text":"The Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.","title":"Planet Tools"},{"location":"projects/planet_gee_pipeline_gui/#planet-key","text":"This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools If using on a private machine the Key is saved as a csv file for all future runs of the tool.","title":"Planet Key"},{"location":"projects/planet_gee_pipeline_gui/#aoi-json","text":"The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.","title":"AOI JSON"},{"location":"projects/planet_gee_pipeline_gui/#activate-or-check-asset","text":"The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier","title":"Activate or Check Asset"},{"location":"projects/planet_gee_pipeline_gui/#download-asset","text":"Having metadata helps in organising your asstets, but is not mandatory - you can skip it. The downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types.","title":"Download Asset"},{"location":"projects/planet_gee_pipeline_gui/#metadata-parser","text":"The metadata tab is a more powerful tool and consists of metadata parsing for PlanetScope OrthoTile RapiEye OrthoTile along with Digital Globe MultiSpectral and DigitalGlobe PanChromatic datasets. This was developed as a standalone to process xml metadata files from multiple sources and is important step is the user plans to upload these assets to Google Earth Engine. The combine Planet-GEE Pipeline tool will be made available soon for testing.","title":"Metadata Parser"},{"location":"projects/planet_gee_pipeline_gui/#earth-engine-tools","text":"The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. This is also a seperate package for earth engine users to use and can be downloaded here","title":"Earth Engine Tools"},{"location":"projects/planet_gee_pipeline_gui/#ee-user","text":"This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back","title":"EE User"},{"location":"projects/planet_gee_pipeline_gui/#upload-a-directory-with-images-to-your-myfoldermycollection-and-associate-properties-with-each-image","text":"The script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.","title":"Upload a directory with images to your myfolder/mycollection and associate properties with each image:"},{"location":"projects/planet_gee_pipeline_gui/#task-query","text":"This script counts all currently running and ready tasks along with failed tasks.","title":"Task Query"},{"location":"projects/planet_gee_pipeline_gui/#task-query-during-ingestion","text":"This script can be used intermittently to look at running, failed and ready(waiting) tasks during ingestion. This script is a special case using query tasks only when uploading assets to collection by providing collection pathway to see how collection size increases.","title":"Task Query during ingestion"},{"location":"projects/planet_gee_pipeline_gui/#task-report","text":"Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type","title":"Task Report"},{"location":"projects/planet_gee_pipeline_gui/#delete-a-collection-with-content","text":"The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution!","title":"Delete a collection with content:"},{"location":"projects/planet_gee_pipeline_gui/#assets-move","text":"This script allows us to recursively move assets from one collection to the other.","title":"Assets Move"},{"location":"projects/planet_gee_pipeline_gui/#assets-copy","text":"This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections.","title":"Assets Copy"},{"location":"projects/planet_gee_pipeline_gui/#assets-access","text":"This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time.","title":"Assets Access"},{"location":"projects/planet_gee_pipeline_gui/#set-collection-property","text":"This script is derived from the ee tool to set collection properties and will set overall properties for collection.","title":"Set Collection Property"},{"location":"projects/planet_gee_pipeline_gui/#convert-to-fusion-table","text":"Once validated with gdal and google fusion table it can be used to convert any geoObject to google fusion table. Forked and contributed by Gennadii here . The scripts can be used only with a specific google account","title":"Convert to Fusion Table"},{"location":"projects/planet_gee_pipeline_gui/#cleanup-utility","text":"This script is used to clean folders once all processes have been completed. In short this is a function to clear folder on local machine.","title":"Cleanup Utility"},{"location":"projects/planet_gee_pipeline_gui/#cancel-all-tasks","text":"This is a simpler tool, can be called directly from the earthengine cli as well","title":"Cancel all tasks"},{"location":"projects/planet_gee_pipeline_gui/#credits","text":"JetStream A portion of the work is suported by JetStream Grant TG-GEO160014. Also supported by Planet Labs Ambassador Program","title":"Credits"},{"location":"projects/planet_pipeline_gui/","text":"Planet Pipeline GUI \u00b6 The Planet Pipeline GUI came from the actual CLI (command line interface tools) to enable the use of tools required to access control and download planet labs assets as well as parse metadata in a tabular form which maybe required by other applications. Table of contents \u00b6 Installation Getting started Planet Key AOI JSON Activate or Check Asset Download Size Download Asset Metadata Parser Credits Installation \u00b6 We assume Planet Python API is installed you can install by simply running pip install planet Further instructions can be found here To install the tool: git clone https://github.com/samapriya/Planet-Pipeline-GUI.git cd Planet-Pipeline-GUI && pip install . The application can be also run directly by executing PlanetPipe_GUI.pyc script. You require two important packages for this to run WxPython(which is what the GUI is built on) for windows(Tested in Windows 10) https://wxpython.org/download.php for linux(Tested in Ubuntu 16) sudo add-apt-repository \"deb http://archive.ubuntu.com/ubuntu utopic main restricted universe\" sudo apt-get update apt-cache search python-wxgtk3.0 sudo apt-get install python-wxgtk3.0 Getting started \u00b6 This should be pretty simple on windows systems with python >=2.7.13 you can just double click the PlanetPipe_GUI.pyc for linux user python PlanetPipe_GUI.pyc Planet Key \u00b6 This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools If using on a private machine the Key is saved as a csv file for all future runs of the tool. AOI JSON \u00b6 The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you. Activate or Check Asset \u00b6 The activatepl tab allows the users to either check or activate planet assets. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier Download Size \u00b6 The space tool allows the user to check the size of the activated assets before downloading while also estimating the available space on the local drive. The overall size and space is printed in MB and GB Download Asset \u00b6 The downloadpl tab allows the users to download assets that have been activated. Credits \u00b6 JetStream A portion of the work is suported by JetStream Grant TG-GEO160014. Also supported by Planet Labs Ambassador Program","title":"Planet Bulk Data Downloader GUI"},{"location":"projects/planet_pipeline_gui/#planet-pipeline-gui","text":"The Planet Pipeline GUI came from the actual CLI (command line interface tools) to enable the use of tools required to access control and download planet labs assets as well as parse metadata in a tabular form which maybe required by other applications.","title":"Planet Pipeline GUI"},{"location":"projects/planet_pipeline_gui/#table-of-contents","text":"Installation Getting started Planet Key AOI JSON Activate or Check Asset Download Size Download Asset Metadata Parser Credits","title":"Table of contents"},{"location":"projects/planet_pipeline_gui/#installation","text":"We assume Planet Python API is installed you can install by simply running pip install planet Further instructions can be found here To install the tool: git clone https://github.com/samapriya/Planet-Pipeline-GUI.git cd Planet-Pipeline-GUI && pip install . The application can be also run directly by executing PlanetPipe_GUI.pyc script. You require two important packages for this to run WxPython(which is what the GUI is built on) for windows(Tested in Windows 10) https://wxpython.org/download.php for linux(Tested in Ubuntu 16) sudo add-apt-repository \"deb http://archive.ubuntu.com/ubuntu utopic main restricted universe\" sudo apt-get update apt-cache search python-wxgtk3.0 sudo apt-get install python-wxgtk3.0","title":"Installation"},{"location":"projects/planet_pipeline_gui/#getting-started","text":"This should be pretty simple on windows systems with python >=2.7.13 you can just double click the PlanetPipe_GUI.pyc for linux user python PlanetPipe_GUI.pyc","title":"Getting started"},{"location":"projects/planet_pipeline_gui/#planet-key","text":"This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools If using on a private machine the Key is saved as a csv file for all future runs of the tool.","title":"Planet Key"},{"location":"projects/planet_pipeline_gui/#aoi-json","text":"The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.","title":"AOI JSON"},{"location":"projects/planet_pipeline_gui/#activate-or-check-asset","text":"The activatepl tab allows the users to either check or activate planet assets. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier","title":"Activate or Check Asset"},{"location":"projects/planet_pipeline_gui/#download-size","text":"The space tool allows the user to check the size of the activated assets before downloading while also estimating the available space on the local drive. The overall size and space is printed in MB and GB","title":"Download Size"},{"location":"projects/planet_pipeline_gui/#download-asset","text":"The downloadpl tab allows the users to download assets that have been activated.","title":"Download Asset"},{"location":"projects/planet_pipeline_gui/#credits","text":"JetStream A portion of the work is suported by JetStream Grant TG-GEO160014. Also supported by Planet Labs Ambassador Program","title":"Credits"},{"location":"projects/porder/","text":"porder: Simple CLI for Planet ordersV2 API \u00b6 Ordersv2 is the next iteration of Planet's API in getting Analysis Ready Data (ARD) delivered to you. Orders v2 allows you to improved functionality in this domain, including capability to submit an number of images in a batch order, and perform operations such as top of atmospheric reflectance, compression, coregistration and also enhanced notifications such as email and webhooks. Based on your access you can use this tool to chain together a sequence of operations. This tool is a command line interface that allows you to interact with the ordersv2 API along with place orders and download orders as needed. The tool also allows you to chain multiple processes together and additional functionalities will be added as needed. For exporting to cloud storages release 0.0.8 onwards has a configuration folder with config yml structures to be used with this tool. Simply replaces the fields as needed. Please note: This tool is in no way an official tool or Planet offering, but is a personal project created and maintained by Samapriya Roy If you use this tool to download data for your research, and find this tool useful, star and cite it as below Samapriya Roy. (2019, October 24). samapriya/porder: porder: Simple CLI for Planet ordersV2 API (Version 0.5.2). Zenodo. http://doi.org/10.5281/zenodo.3518295 Table of contents \u00b6 Prerequisites Installing porder Windows Setup Getting started porder Simple CLI for Planet ordersv2 API porder version porder quota base64 shape to geojson simplify idlist difflist idsplit idcheck bundles order ordersize stats download multipart download multiprocessing download Prerequisites \u00b6 This assumes that you have native python & pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying. I recommend installation within virtual environment if you are worries about messing up your current environment. python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. This command line tool is dependent on shapely and fiona and as such uses functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa && sudo apt-get update sudo apt-get install gdal-bin sudo apt-get install python-gdal For Windows I found this guide from UCLA Also for Ubuntu Linux I saw that this is necessary before the install sudo apt install libcurl4-openssl-dev libssl-dev Installing porder \u00b6 Once you have shapely and the other libraries configured, to install porder: Simple CLI for Planet ordersv2 API you can install using two methods pip install porder For linux I found it helps to specify the pip type and use --user pip install porder --user or pip3 install porder --user or you can also try git clone https://github.com/samapriya/porder.git cd porder python setup.py install Windows Setup \u00b6 Shapely and a few other libraries are notoriously difficult to install on windows machines so follow the steps mentioned here before installing porder . You can download and install shapely and other libraries from the Unofficial Wheel files from here download depending on the python version you have. Do this only once you have install GDAL . I would recommend the steps mentioned above to get the GDAL properly installed. However I am including instructions to using a precompiled version of GDAL similar to the other libraries on windows. You can test to see if you have gdal by simply running gdalinfo in your command prompt. If you get a read out and not an error message you are good to go. If you don't have gdal try Option 1,2 or 3 in that order and that will install gdal along with the other libraries Option 1: \u00b6 Starting from porder v0.4.5 onwards: Simply run porder -h after installation. This should go fetch the extra libraries you need and install them. Once installation is complete, the porder help page will show up. This should save you from the few steps below. Option 2: \u00b6 If this does not work or you get an unexpected error try the following commands. You can also use these commands if you simply want to update these libraries. pipwin refresh pipwin install gdal pipwin install pyproj pipwin install shapely pipwin install fiona pipwing install geopandas Option 3 \u00b6 For windows first thing you need to figure out is your Python version and whether it is 32 bit or 64 bit. You can do this by going to your command prompt and typing python. For my windows machine, I have both 32-bit python 2.7.16 and 64-bit Python 3.6.6. You can get the python version at the beginning of the highlighted lines and the 32 or 64 bit within the Intel or AMD64 within the square brackets. Your default python is the one you get by just typing python in the command line. Then download the following packages based on the information we collect about our python type in the earlier step. We use unofficial binaries to install these. This step is only needed if you are on a windows machine if you are using a setup manager like anaconda you might be able to avoid this setup completely At this stage if you were unable to install gdal then download the gdal binaries first , install that before everything else gdal: https://www.lfd.uci.edu/~gohlke/pythonlibs/#gdal Then follow along the following libraries * pyproj: https://www.lfd.uci.edu/~gohlke/pythonlibs/#pyproj * shapely: https://www.lfd.uci.edu/~gohlke/pythonlibs/#shapely * fiona: https://www.lfd.uci.edu/~gohlke/pythonlibs/#fiona * geopandas: https://www.lfd.uci.edu/~gohlke/pythonlibs/#geopandas To choose the version that is correct for you use the python information you collected earlier For example for my python 3.6.6 and AMD 64 if I was installing shapely I would choose the following, here 36 means python 3.6 and amd64 refers to the 64bit we were talking about. Shapely\u20111.6.4.post2\u2011cp36\u2011cp36m\u2011win_amd64.whl You will get a wheel file or a file ending with .whl. You can now simply browse to the folder or migrate to it in your command prompt. Once there if I am installing for my python 3.6 the command was. At this point we will make use of our trusted package installer that comes with python called pip. Note the choice of pip or pip3 depends on your python version usually you can get the pip to use with your python by typing pip3 -V you get a readout like this pip 18.1 from c:\\python3\\lib\\site-packages\\pip (python 3.6) if you have pip just replace that with pip -V Then simply install the wheel files you downloaded using the following setup pip3 install full path to Shapely\u20111.6.4.post2\u2011cp36\u2011cp36m\u2011win_amd64.whl in my case that would be pip3 install \"C:\\Users\\samapriya\\Downloads\\Shapely\u20111.6.4.post2\u2011cp36\u2011cp36m\u2011win_amd64.whl\" Or you can use anaconda to install . Again, both of these options are mentioned on Shapely\u2019s Official PyPI page . Getting started \u00b6 Make sure you initialized planet client by typing planet init . As usual, to print help: To obtain help for a specific functionality, simply call it with help switch, e.g.: porder idlist -h . If you didn't install porder, then you can run it just by going to porder directory and running python porder.py [arguments go here] porder Simple CLI for Planet ordersv2 API \u00b6 The tool is designed to simplify using the ordersv2 API and allows the user to chain together tools and operations for multiple item and asset types and perform these operations and download the assets locally. porder version \u00b6 This prints the tool version and escapes. Simple use would be porder version porder quota \u00b6 Just a simple tool to print your planet subscription quota quickly. base64 \u00b6 This does exactly as it sounds, it encodes your credential files to base64 for use with gcs. shape to geojson \u00b6 This tool allows you to convert from a folder with multiple shapefiles to a folder with geojson that can then be used with the tool. It makes use of geopandas and reprojects your shapefile to make it compatible while passing onto the API for search and download. simplify \u00b6 This reduces the number of vertices for a multi vertex and complex GeoJSON. Extremely high vertex count (over 500) seem to fail and hence this tool allows you to export a new geojson with reduced vertices. It uses an implementation of the Visvalingam-Wyatt line simplification algorithm. This tool does work with and without Fiona, but Fiona installation is recommended. idlist \u00b6 Create an idlist for your geometry based on some basic filters,including geometry, start and end date and cloud cover. If no cloud cover is specified everything form 0 to 100% cloud cover is included. For now the tool can handle geojson,json and kml files. The output is a csv file with ids. The tool also allows you to make sure you get percentage overlap, when selecting image, for clip operations adjust it accordingly (usally --ovp 1 for orders not to fail during clip). The tool now also prints estimated area in Square kilometes for the download and estimated area if you clipped your area with the geometry you are searching (just estimates). I have changed the setup to now do the following two things The number option is optional, so it can look for all images in the time range, but be careful if the area is too large, use at own risk . A better option is to supply the number. It is possible to often forget about the different asset types, so you can now not pass an item and the script will return every possible type of asset for each item type depending on the bundle. A simple setup would be To run an experiment to add additional filter, you can now pass an additional string or range filter or both flag for string and range filters, a setup would be. The additional filters are optional porder idlist --input \"Path to geojson file\" --start \"YYYY-MM-DD\" --end \"YYYY-MM-DD\" --item \"PSScene4Band\" --asset \"analytic\" --outfile \"Path to idlist.csv\" --filters range:clear_percent:55:100 --number 20 porder idlist --input \"Path to geojson file\" --start \"YYYY-MM-DD\" --end \"YYYY-MM-DD\" --item \"PSScene4Band\" --asset \"analytic\" --outfile \"Path to idlist.csv\" --filters string:satellite_id:\"1003,1006,1012,1020,1038\" --number 20 porder idlist --input \"Path to geojson file\" --start \"YYYY-MM-DD\" --end \"YYYY-MM-DD\" --item \"PSScene4Band\" --asset \"analytic\" --outfile \"Path to idlist.csv\" --filters string:satellite_id:\"1003,1006,1012,1020,1038\" range:clear_percent:55:100 --number 20 The idlist tool can now use a multipolygon and iteratively look for scenes. difflist \u00b6 It is possible you already downloaded some images or metadata files, and your you want a difference idlist to create orders for only assets and item types you do not have. It takes in your local folder path, type image or metadata and some basic filters,including geometry, start and end date and cloud cover. If no cloud cover is specified everything form 0 to 100% cloud cover is included. For now the tool can handle geojson,json and kml files. The output is a csv file with ids. A simple setup would be or without the cloud filter idsplit \u00b6 This allows you to split your idlist into small csv files incase you wanted to created batches of orders. A simple setup would be idcheck \u00b6 It is possible for you to modify the idlist, add or remove ids. Once done, this tool allows you to estimate the total area of images and area that intersect with your geometry or area if clipped. A simple setup would be bundles \u00b6 Ordering using ordersv2 uses the concept of bundles. A bundle is a combination of multiple assets for an item that come together and are delivered as part of the overall fulfillment of the order. For example an analytic asset for PSScene4Band is a single tif file, however the analytic bundle for PSScene4Band includes analytic tiff file, the analytic_xml metadata and the udm data mask file as part of the bundle. You can find more information about bundles here . Thus the concept of bundles bring together single function to order and download multiple related assets. Since the list of bundles is long, this tool simply allows you to get every bundle type based on item type. The setup is simple A simple setup would be porder bundles --item \"PSScene4Band\" order \u00b6 This tool allows you to actually place the order using the idlist that you created earlier. the --op argument allows you to take operations, delivery and notifications in a sequence for example --op toar clip email performs Top of Atmospheric reflectance, followed by clipping to your geometry and send you an email notification once the order has completed, failed or had any any change of status. An important changes is the concept of passing bundles instead of using assets. The list of operations for the --op are below and ** the order of these operations is important** clip|toar|comp osite|zip|zipall|compression|projection|kernel|aws|azu re|gcs|email : ndvi|gndvi|bndvi|ndwi|tvi|osavi|evi2|msavi2|sr op description clip Clip imagery can handle single and multi polygon verify or create geojson.io toar Top of Atmosphere Reflectance imagery generated for imagery harmonize Harmonize Dove R (instrument type PS2.SD) data to classic dove (instrument type PS) composite Composite number of images in a given order zip Zip bundles together and creates downloads (each asset has a single bundle so multiple zip files) zipall Create a single zip file containing all assets compression Use image compression projection Reproject before downloaing image aws Option called to specify delivery to AWS azure Option called to specify delivery to AZURE gcs Option called to specify delivery to GCS email Email notification to your planet registered email You can now add some predefined indices for PlanetScope 4 band items with a maximum of 5 indices for a single setup . This is experimental. The list of indices include Index Source Simple ratio (SR) Jordan 1969 Normalized Difference Vegetation Index (NDVI) Rouse et al 1973 Green Normalized Difference Index (GNDVI) Gitelson et al 1996 Blue Normalized Difference Vegetation Index (BNDVI) Wang et al 2007 Transformed Vegetation Index (TVI) Broge and Leblanc 2000 Optimized Soil Adjusted Vegetation Index (OSAVI) Rondeaux et al 1996 Enhanced Vegetation Index (EVI2) Jian et al 2008 Normalized Difference Water Index (NDWI) Gao 1996 Modified Soil-adjusted Vegetation Index v2 (MSAVI2) Qi 1994 A simple setup with image clip with email notification would be The same setup with delivery of each image, its metadata as a zip file would be. Note how we only added zip to the op list A simple setup with Top of Atmospher reflectance and a few indices along with email notification would be ordersize \u00b6 The tool now allows you to estimate the total download size for a specific order. An example setup look like the following stats \u00b6 The tool allows you to check on number of running and queued orders for both organization and user level. Using this is simple porder stats output should look like this: Checking on all running orders... Total queued order for organization: 0 Total running orders for organization: 1 Total queued orders for user: 0 Total running orders for user: 0 download \u00b6 The allows you to download the files in your order, to a local folder. It uses the order url generated using the orders tool to access and download the files. multipart download \u00b6 The allows you to multipart download the files in your order, this uses a multiprocessing downloader to quickly download your files to a local folder. It uses the order url generated using the orders tool to access and download the files. multiprocessing download \u00b6 The uses the multiprocessing library to quickly download your files to a local folder. It uses the order url generated using the orders tool to access and download the files and includes an expotential rate limiting function to handle too many requests. To save on time it uses an extension filter so for example if you are using the zip operation you can use \".zip\" and if you are downloading only images, udm and xml you can use \".tif\" or \".xml\" accordingly. For python 3.4 or higher, this switches to using an true async downloader instead of using multiprocessing. A simple setup would be Changelog \u00b6 v0.5.2 \u00b6 Added harmonization tool to harmonize PS2.SD to PS2. Improvements and error handling to quota tool Merged pull request 35 to keep download progress via enumerate. v0.5.1 \u00b6 Added utf-8 encoding for shapefile to geojson conversion Merged pull request 34 to refresh url once expired. v0.4.9 \u00b6 Fixed issue with gdal import for pipwin windows. Fixed import issue with stats endpoint. v0.4.8 \u00b6 Replaced concurrency check with stats endpoint to get queued and running orders. Change pipwin cache refresh time to two weeks. v0.4.7 \u00b6 Fixed issue with queuing state for orders and downloads. v0.4.6 \u00b6 Handles refreshing pipwin cache and better error handling Fixed issue with downloading unique manifest ID for zip files. Updated ReadMe with improved documentation. v0.4.5 \u00b6 Handles installation of windows specific libraries using pipwin . General improvements v0.4.4 \u00b6 Manifest files for each asset is now written in format ItemID_manifest.json to avoid skipping manifest.json common file name. Simple and multipart downloader now show number of items remaining during the download. General improvements, bundles tool now prints Bundles:Name followed by assets included in the bundle v0.4.3 \u00b6 Fixed issues with setup.py and pyproj version. Improved ReadMe instructions. v0.4.2 \u00b6 Added geometry check functionality to multipolygon with shapely self intersection Issue 30 . For multipolygons this also performs a vertex count check and simplifies polygon to fit under 500 vertices. General improvements v0.4.1 \u00b6 Fixed issue with shapely self intersection using buffer(0). General improvements v0.4.0 \u00b6 Fixed issue with placing reprojection request. Downloader can now download partial as well as completely successful orders. Added retry method for rate limit during downloading General improvements v0.3.9 \u00b6 Removed deprecated bundles from bundles list. Improved parameter description v0.3.7 \u00b6 Added capability to pass subscription id when submitting order. v0.3.6 \u00b6 Replaced asset in order tool with bundles. Created a new bundles tool to generate bundle list for an item type Improvements to the idlist tool now prints output as it makes progress. v0.3.5 \u00b6 Better integration for quota tool Updates information while waiting for idlist Updated requirements v0.3.4 \u00b6 Added async downloader for python 3.4 Checks for existing files before spawning processes Better handling of multiprocessing output Added a quick version tool v0.3.3 \u00b6 Fixed issue with order name when no ops are used. Used file basename for splitting the idlist. v0.3.2 \u00b6 idlist tool can no use a multipolygon and iteratively look for scenes Orders clip tool can now handle multipolygon clip Added new tool zipall to handle single archive download in format ordername_date.zip v0.3.1 \u00b6 Can now support an additional string and range filter Check total area and clip area estimates from any idlist using idcheck. General improvements to the tool. v0.3.0 \u00b6 Enhances idlist to execute faster search and return using Planet CLI Included better error handling while placing order. v0.2.8 \u00b6 Added tool to convert folder with shapefiles to GeoJSONs v0.2.7 \u00b6 Improved overlap calculations for larger geometries Added a geometry simplification tool to reduce number of vertices v0.2.6 \u00b6 Skysat area are calculated using EPSG:3857 to resolve metadata EPSG issue General improvements v0.2.5 \u00b6 Fixed issue with area calculation estimates General improvements v0.2.4 \u00b6 Now functions without limit on the number of assets in the idlist Parses possible asset types if only item type is supplied for idlist v0.2.3 \u00b6 Now estimates area before and after clip when you run idlist General improvements v0.2.1 \u00b6 Now exports only csv idlist Fixed count with concurrency check v0.2.0 \u00b6 Fixed pysmartdl install issues Added concurrent orders check version and os resolve for shapely v0.1.9 \u00b6 Added msavi Fixed issues with GeoJSON read v0.1.8 \u00b6 Fixed issues with empty JSON append General improvements to the tool v0.1.7 \u00b6 Added band math indices for PlanetScope item Fixed issues with retry for downloader General improvements to the tool v0.1.6 \u00b6 Made fixes to have python 3.X compatability v0.1.5 \u00b6 General improvements and bug fixes v0.1.4 \u00b6 Fixed issue with Python 3 CSV write compatability Fixed issues with Shapely instance issue v0.1.3 \u00b6 Fixed issue with clipboard access in headless setup v0.1.2 \u00b6 Fixed issue and extension for multiprocessing downloader Overall general improvements to the tool v0.1.0 \u00b6 Fixed issue and improved idlist and sort Fixed issue with clip tool Overall general improvements to the tool v0.0.8 \u00b6 Improvements to operations in order tool Now supports export to gcs/azure/aws along with kernel, projection and compression base64 encoding tool for encoding gcs credentials Overall general improvements to the tool v0.0.7 \u00b6 Now allows for all downloads or download using extension Polling for order to complete and automatically download General improvements v0.0.6 \u00b6 Merged contribution by David Shean Fixed issues with op equals None Fixed issues with relative import Improved Py3 compatability General improvements v0.0.5 \u00b6 Added exponential backoff for pydl Fixed issues with dependency General overall improvements v0.0.4 \u00b6 Created strict geoinstersection to avoid orders to fail Improvements to overlap function General overall improvements v0.0.3 \u00b6 Added overlap function to idlist Added multiprocessing downloader with rate limit and extension filter General overall improvements v0.0.2 \u00b6 Fixed issues with import modules","title":"Planet Simple CLI for Ordersv2"},{"location":"projects/porder/#porder-simple-cli-for-planet-ordersv2-api","text":"Ordersv2 is the next iteration of Planet's API in getting Analysis Ready Data (ARD) delivered to you. Orders v2 allows you to improved functionality in this domain, including capability to submit an number of images in a batch order, and perform operations such as top of atmospheric reflectance, compression, coregistration and also enhanced notifications such as email and webhooks. Based on your access you can use this tool to chain together a sequence of operations. This tool is a command line interface that allows you to interact with the ordersv2 API along with place orders and download orders as needed. The tool also allows you to chain multiple processes together and additional functionalities will be added as needed. For exporting to cloud storages release 0.0.8 onwards has a configuration folder with config yml structures to be used with this tool. Simply replaces the fields as needed. Please note: This tool is in no way an official tool or Planet offering, but is a personal project created and maintained by Samapriya Roy If you use this tool to download data for your research, and find this tool useful, star and cite it as below Samapriya Roy. (2019, October 24). samapriya/porder: porder: Simple CLI for Planet ordersV2 API (Version 0.5.2). Zenodo. http://doi.org/10.5281/zenodo.3518295","title":"porder: Simple CLI for Planet ordersV2 API &nbsp;"},{"location":"projects/porder/#table-of-contents","text":"Prerequisites Installing porder Windows Setup Getting started porder Simple CLI for Planet ordersv2 API porder version porder quota base64 shape to geojson simplify idlist difflist idsplit idcheck bundles order ordersize stats download multipart download multiprocessing download","title":"Table of contents"},{"location":"projects/porder/#prerequisites","text":"This assumes that you have native python & pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying. I recommend installation within virtual environment if you are worries about messing up your current environment. python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. This command line tool is dependent on shapely and fiona and as such uses functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa && sudo apt-get update sudo apt-get install gdal-bin sudo apt-get install python-gdal For Windows I found this guide from UCLA Also for Ubuntu Linux I saw that this is necessary before the install sudo apt install libcurl4-openssl-dev libssl-dev","title":"Prerequisites"},{"location":"projects/porder/#installing-porder","text":"Once you have shapely and the other libraries configured, to install porder: Simple CLI for Planet ordersv2 API you can install using two methods pip install porder For linux I found it helps to specify the pip type and use --user pip install porder --user or pip3 install porder --user or you can also try git clone https://github.com/samapriya/porder.git cd porder python setup.py install","title":"Installing porder"},{"location":"projects/porder/#windows-setup","text":"Shapely and a few other libraries are notoriously difficult to install on windows machines so follow the steps mentioned here before installing porder . You can download and install shapely and other libraries from the Unofficial Wheel files from here download depending on the python version you have. Do this only once you have install GDAL . I would recommend the steps mentioned above to get the GDAL properly installed. However I am including instructions to using a precompiled version of GDAL similar to the other libraries on windows. You can test to see if you have gdal by simply running gdalinfo in your command prompt. If you get a read out and not an error message you are good to go. If you don't have gdal try Option 1,2 or 3 in that order and that will install gdal along with the other libraries","title":"Windows Setup"},{"location":"projects/porder/#option-1","text":"Starting from porder v0.4.5 onwards: Simply run porder -h after installation. This should go fetch the extra libraries you need and install them. Once installation is complete, the porder help page will show up. This should save you from the few steps below.","title":"Option 1:"},{"location":"projects/porder/#option-2","text":"If this does not work or you get an unexpected error try the following commands. You can also use these commands if you simply want to update these libraries. pipwin refresh pipwin install gdal pipwin install pyproj pipwin install shapely pipwin install fiona pipwing install geopandas","title":"Option 2:"},{"location":"projects/porder/#option-3","text":"For windows first thing you need to figure out is your Python version and whether it is 32 bit or 64 bit. You can do this by going to your command prompt and typing python. For my windows machine, I have both 32-bit python 2.7.16 and 64-bit Python 3.6.6. You can get the python version at the beginning of the highlighted lines and the 32 or 64 bit within the Intel or AMD64 within the square brackets. Your default python is the one you get by just typing python in the command line. Then download the following packages based on the information we collect about our python type in the earlier step. We use unofficial binaries to install these. This step is only needed if you are on a windows machine if you are using a setup manager like anaconda you might be able to avoid this setup completely At this stage if you were unable to install gdal then download the gdal binaries first , install that before everything else gdal: https://www.lfd.uci.edu/~gohlke/pythonlibs/#gdal Then follow along the following libraries * pyproj: https://www.lfd.uci.edu/~gohlke/pythonlibs/#pyproj * shapely: https://www.lfd.uci.edu/~gohlke/pythonlibs/#shapely * fiona: https://www.lfd.uci.edu/~gohlke/pythonlibs/#fiona * geopandas: https://www.lfd.uci.edu/~gohlke/pythonlibs/#geopandas To choose the version that is correct for you use the python information you collected earlier For example for my python 3.6.6 and AMD 64 if I was installing shapely I would choose the following, here 36 means python 3.6 and amd64 refers to the 64bit we were talking about. Shapely\u20111.6.4.post2\u2011cp36\u2011cp36m\u2011win_amd64.whl You will get a wheel file or a file ending with .whl. You can now simply browse to the folder or migrate to it in your command prompt. Once there if I am installing for my python 3.6 the command was. At this point we will make use of our trusted package installer that comes with python called pip. Note the choice of pip or pip3 depends on your python version usually you can get the pip to use with your python by typing pip3 -V you get a readout like this pip 18.1 from c:\\python3\\lib\\site-packages\\pip (python 3.6) if you have pip just replace that with pip -V Then simply install the wheel files you downloaded using the following setup pip3 install full path to Shapely\u20111.6.4.post2\u2011cp36\u2011cp36m\u2011win_amd64.whl in my case that would be pip3 install \"C:\\Users\\samapriya\\Downloads\\Shapely\u20111.6.4.post2\u2011cp36\u2011cp36m\u2011win_amd64.whl\" Or you can use anaconda to install . Again, both of these options are mentioned on Shapely\u2019s Official PyPI page .","title":"Option 3"},{"location":"projects/porder/#getting-started","text":"Make sure you initialized planet client by typing planet init . As usual, to print help: To obtain help for a specific functionality, simply call it with help switch, e.g.: porder idlist -h . If you didn't install porder, then you can run it just by going to porder directory and running python porder.py [arguments go here]","title":"Getting started"},{"location":"projects/porder/#porder-simple-cli-for-planet-ordersv2-api_1","text":"The tool is designed to simplify using the ordersv2 API and allows the user to chain together tools and operations for multiple item and asset types and perform these operations and download the assets locally.","title":"porder Simple CLI for Planet ordersv2 API"},{"location":"projects/porder/#porder-version","text":"This prints the tool version and escapes. Simple use would be porder version","title":"porder version"},{"location":"projects/porder/#porder-quota","text":"Just a simple tool to print your planet subscription quota quickly.","title":"porder quota"},{"location":"projects/porder/#base64","text":"This does exactly as it sounds, it encodes your credential files to base64 for use with gcs.","title":"base64"},{"location":"projects/porder/#shape-to-geojson","text":"This tool allows you to convert from a folder with multiple shapefiles to a folder with geojson that can then be used with the tool. It makes use of geopandas and reprojects your shapefile to make it compatible while passing onto the API for search and download.","title":"shape to geojson"},{"location":"projects/porder/#simplify","text":"This reduces the number of vertices for a multi vertex and complex GeoJSON. Extremely high vertex count (over 500) seem to fail and hence this tool allows you to export a new geojson with reduced vertices. It uses an implementation of the Visvalingam-Wyatt line simplification algorithm. This tool does work with and without Fiona, but Fiona installation is recommended.","title":"simplify"},{"location":"projects/porder/#idlist","text":"Create an idlist for your geometry based on some basic filters,including geometry, start and end date and cloud cover. If no cloud cover is specified everything form 0 to 100% cloud cover is included. For now the tool can handle geojson,json and kml files. The output is a csv file with ids. The tool also allows you to make sure you get percentage overlap, when selecting image, for clip operations adjust it accordingly (usally --ovp 1 for orders not to fail during clip). The tool now also prints estimated area in Square kilometes for the download and estimated area if you clipped your area with the geometry you are searching (just estimates). I have changed the setup to now do the following two things The number option is optional, so it can look for all images in the time range, but be careful if the area is too large, use at own risk . A better option is to supply the number. It is possible to often forget about the different asset types, so you can now not pass an item and the script will return every possible type of asset for each item type depending on the bundle. A simple setup would be To run an experiment to add additional filter, you can now pass an additional string or range filter or both flag for string and range filters, a setup would be. The additional filters are optional porder idlist --input \"Path to geojson file\" --start \"YYYY-MM-DD\" --end \"YYYY-MM-DD\" --item \"PSScene4Band\" --asset \"analytic\" --outfile \"Path to idlist.csv\" --filters range:clear_percent:55:100 --number 20 porder idlist --input \"Path to geojson file\" --start \"YYYY-MM-DD\" --end \"YYYY-MM-DD\" --item \"PSScene4Band\" --asset \"analytic\" --outfile \"Path to idlist.csv\" --filters string:satellite_id:\"1003,1006,1012,1020,1038\" --number 20 porder idlist --input \"Path to geojson file\" --start \"YYYY-MM-DD\" --end \"YYYY-MM-DD\" --item \"PSScene4Band\" --asset \"analytic\" --outfile \"Path to idlist.csv\" --filters string:satellite_id:\"1003,1006,1012,1020,1038\" range:clear_percent:55:100 --number 20 The idlist tool can now use a multipolygon and iteratively look for scenes.","title":"idlist"},{"location":"projects/porder/#difflist","text":"It is possible you already downloaded some images or metadata files, and your you want a difference idlist to create orders for only assets and item types you do not have. It takes in your local folder path, type image or metadata and some basic filters,including geometry, start and end date and cloud cover. If no cloud cover is specified everything form 0 to 100% cloud cover is included. For now the tool can handle geojson,json and kml files. The output is a csv file with ids. A simple setup would be or without the cloud filter","title":"difflist"},{"location":"projects/porder/#idsplit","text":"This allows you to split your idlist into small csv files incase you wanted to created batches of orders. A simple setup would be","title":"idsplit"},{"location":"projects/porder/#idcheck","text":"It is possible for you to modify the idlist, add or remove ids. Once done, this tool allows you to estimate the total area of images and area that intersect with your geometry or area if clipped. A simple setup would be","title":"idcheck"},{"location":"projects/porder/#bundles","text":"Ordering using ordersv2 uses the concept of bundles. A bundle is a combination of multiple assets for an item that come together and are delivered as part of the overall fulfillment of the order. For example an analytic asset for PSScene4Band is a single tif file, however the analytic bundle for PSScene4Band includes analytic tiff file, the analytic_xml metadata and the udm data mask file as part of the bundle. You can find more information about bundles here . Thus the concept of bundles bring together single function to order and download multiple related assets. Since the list of bundles is long, this tool simply allows you to get every bundle type based on item type. The setup is simple A simple setup would be porder bundles --item \"PSScene4Band\"","title":"bundles"},{"location":"projects/porder/#order","text":"This tool allows you to actually place the order using the idlist that you created earlier. the --op argument allows you to take operations, delivery and notifications in a sequence for example --op toar clip email performs Top of Atmospheric reflectance, followed by clipping to your geometry and send you an email notification once the order has completed, failed or had any any change of status. An important changes is the concept of passing bundles instead of using assets. The list of operations for the --op are below and ** the order of these operations is important** clip|toar|comp osite|zip|zipall|compression|projection|kernel|aws|azu re|gcs|email : ndvi|gndvi|bndvi|ndwi|tvi|osavi|evi2|msavi2|sr op description clip Clip imagery can handle single and multi polygon verify or create geojson.io toar Top of Atmosphere Reflectance imagery generated for imagery harmonize Harmonize Dove R (instrument type PS2.SD) data to classic dove (instrument type PS) composite Composite number of images in a given order zip Zip bundles together and creates downloads (each asset has a single bundle so multiple zip files) zipall Create a single zip file containing all assets compression Use image compression projection Reproject before downloaing image aws Option called to specify delivery to AWS azure Option called to specify delivery to AZURE gcs Option called to specify delivery to GCS email Email notification to your planet registered email You can now add some predefined indices for PlanetScope 4 band items with a maximum of 5 indices for a single setup . This is experimental. The list of indices include Index Source Simple ratio (SR) Jordan 1969 Normalized Difference Vegetation Index (NDVI) Rouse et al 1973 Green Normalized Difference Index (GNDVI) Gitelson et al 1996 Blue Normalized Difference Vegetation Index (BNDVI) Wang et al 2007 Transformed Vegetation Index (TVI) Broge and Leblanc 2000 Optimized Soil Adjusted Vegetation Index (OSAVI) Rondeaux et al 1996 Enhanced Vegetation Index (EVI2) Jian et al 2008 Normalized Difference Water Index (NDWI) Gao 1996 Modified Soil-adjusted Vegetation Index v2 (MSAVI2) Qi 1994 A simple setup with image clip with email notification would be The same setup with delivery of each image, its metadata as a zip file would be. Note how we only added zip to the op list A simple setup with Top of Atmospher reflectance and a few indices along with email notification would be","title":"order"},{"location":"projects/porder/#ordersize","text":"The tool now allows you to estimate the total download size for a specific order. An example setup look like the following","title":"ordersize"},{"location":"projects/porder/#stats","text":"The tool allows you to check on number of running and queued orders for both organization and user level. Using this is simple porder stats output should look like this: Checking on all running orders... Total queued order for organization: 0 Total running orders for organization: 1 Total queued orders for user: 0 Total running orders for user: 0","title":"stats"},{"location":"projects/porder/#download","text":"The allows you to download the files in your order, to a local folder. It uses the order url generated using the orders tool to access and download the files.","title":"download"},{"location":"projects/porder/#multipart-download","text":"The allows you to multipart download the files in your order, this uses a multiprocessing downloader to quickly download your files to a local folder. It uses the order url generated using the orders tool to access and download the files.","title":"multipart download"},{"location":"projects/porder/#multiprocessing-download","text":"The uses the multiprocessing library to quickly download your files to a local folder. It uses the order url generated using the orders tool to access and download the files and includes an expotential rate limiting function to handle too many requests. To save on time it uses an extension filter so for example if you are using the zip operation you can use \".zip\" and if you are downloading only images, udm and xml you can use \".tif\" or \".xml\" accordingly. For python 3.4 or higher, this switches to using an true async downloader instead of using multiprocessing. A simple setup would be","title":"multiprocessing download"},{"location":"projects/porder/#changelog","text":"","title":"Changelog"},{"location":"projects/porder/#v052","text":"Added harmonization tool to harmonize PS2.SD to PS2. Improvements and error handling to quota tool Merged pull request 35 to keep download progress via enumerate.","title":"v0.5.2"},{"location":"projects/porder/#v051","text":"Added utf-8 encoding for shapefile to geojson conversion Merged pull request 34 to refresh url once expired.","title":"v0.5.1"},{"location":"projects/porder/#v049","text":"Fixed issue with gdal import for pipwin windows. Fixed import issue with stats endpoint.","title":"v0.4.9"},{"location":"projects/porder/#v048","text":"Replaced concurrency check with stats endpoint to get queued and running orders. Change pipwin cache refresh time to two weeks.","title":"v0.4.8"},{"location":"projects/porder/#v047","text":"Fixed issue with queuing state for orders and downloads.","title":"v0.4.7"},{"location":"projects/porder/#v046","text":"Handles refreshing pipwin cache and better error handling Fixed issue with downloading unique manifest ID for zip files. Updated ReadMe with improved documentation.","title":"v0.4.6"},{"location":"projects/porder/#v045","text":"Handles installation of windows specific libraries using pipwin . General improvements","title":"v0.4.5"},{"location":"projects/porder/#v044","text":"Manifest files for each asset is now written in format ItemID_manifest.json to avoid skipping manifest.json common file name. Simple and multipart downloader now show number of items remaining during the download. General improvements, bundles tool now prints Bundles:Name followed by assets included in the bundle","title":"v0.4.4"},{"location":"projects/porder/#v043","text":"Fixed issues with setup.py and pyproj version. Improved ReadMe instructions.","title":"v0.4.3"},{"location":"projects/porder/#v042","text":"Added geometry check functionality to multipolygon with shapely self intersection Issue 30 . For multipolygons this also performs a vertex count check and simplifies polygon to fit under 500 vertices. General improvements","title":"v0.4.2"},{"location":"projects/porder/#v041","text":"Fixed issue with shapely self intersection using buffer(0). General improvements","title":"v0.4.1"},{"location":"projects/porder/#v040","text":"Fixed issue with placing reprojection request. Downloader can now download partial as well as completely successful orders. Added retry method for rate limit during downloading General improvements","title":"v0.4.0"},{"location":"projects/porder/#v039","text":"Removed deprecated bundles from bundles list. Improved parameter description","title":"v0.3.9"},{"location":"projects/porder/#v037","text":"Added capability to pass subscription id when submitting order.","title":"v0.3.7"},{"location":"projects/porder/#v036","text":"Replaced asset in order tool with bundles. Created a new bundles tool to generate bundle list for an item type Improvements to the idlist tool now prints output as it makes progress.","title":"v0.3.6"},{"location":"projects/porder/#v035","text":"Better integration for quota tool Updates information while waiting for idlist Updated requirements","title":"v0.3.5"},{"location":"projects/porder/#v034","text":"Added async downloader for python 3.4 Checks for existing files before spawning processes Better handling of multiprocessing output Added a quick version tool","title":"v0.3.4"},{"location":"projects/porder/#v033","text":"Fixed issue with order name when no ops are used. Used file basename for splitting the idlist.","title":"v0.3.3"},{"location":"projects/porder/#v032","text":"idlist tool can no use a multipolygon and iteratively look for scenes Orders clip tool can now handle multipolygon clip Added new tool zipall to handle single archive download in format ordername_date.zip","title":"v0.3.2"},{"location":"projects/porder/#v031","text":"Can now support an additional string and range filter Check total area and clip area estimates from any idlist using idcheck. General improvements to the tool.","title":"v0.3.1"},{"location":"projects/porder/#v030","text":"Enhances idlist to execute faster search and return using Planet CLI Included better error handling while placing order.","title":"v0.3.0"},{"location":"projects/porder/#v028","text":"Added tool to convert folder with shapefiles to GeoJSONs","title":"v0.2.8"},{"location":"projects/porder/#v027","text":"Improved overlap calculations for larger geometries Added a geometry simplification tool to reduce number of vertices","title":"v0.2.7"},{"location":"projects/porder/#v026","text":"Skysat area are calculated using EPSG:3857 to resolve metadata EPSG issue General improvements","title":"v0.2.6"},{"location":"projects/porder/#v025","text":"Fixed issue with area calculation estimates General improvements","title":"v0.2.5"},{"location":"projects/porder/#v024","text":"Now functions without limit on the number of assets in the idlist Parses possible asset types if only item type is supplied for idlist","title":"v0.2.4"},{"location":"projects/porder/#v023","text":"Now estimates area before and after clip when you run idlist General improvements","title":"v0.2.3"},{"location":"projects/porder/#v021","text":"Now exports only csv idlist Fixed count with concurrency check","title":"v0.2.1"},{"location":"projects/porder/#v020","text":"Fixed pysmartdl install issues Added concurrent orders check version and os resolve for shapely","title":"v0.2.0"},{"location":"projects/porder/#v019","text":"Added msavi Fixed issues with GeoJSON read","title":"v0.1.9"},{"location":"projects/porder/#v018","text":"Fixed issues with empty JSON append General improvements to the tool","title":"v0.1.8"},{"location":"projects/porder/#v017","text":"Added band math indices for PlanetScope item Fixed issues with retry for downloader General improvements to the tool","title":"v0.1.7"},{"location":"projects/porder/#v016","text":"Made fixes to have python 3.X compatability","title":"v0.1.6"},{"location":"projects/porder/#v015","text":"General improvements and bug fixes","title":"v0.1.5"},{"location":"projects/porder/#v014","text":"Fixed issue with Python 3 CSV write compatability Fixed issues with Shapely instance issue","title":"v0.1.4"},{"location":"projects/porder/#v013","text":"Fixed issue with clipboard access in headless setup","title":"v0.1.3"},{"location":"projects/porder/#v012","text":"Fixed issue and extension for multiprocessing downloader Overall general improvements to the tool","title":"v0.1.2"},{"location":"projects/porder/#v010","text":"Fixed issue and improved idlist and sort Fixed issue with clip tool Overall general improvements to the tool","title":"v0.1.0"},{"location":"projects/porder/#v008","text":"Improvements to operations in order tool Now supports export to gcs/azure/aws along with kernel, projection and compression base64 encoding tool for encoding gcs credentials Overall general improvements to the tool","title":"v0.0.8"},{"location":"projects/porder/#v007","text":"Now allows for all downloads or download using extension Polling for order to complete and automatically download General improvements","title":"v0.0.7"},{"location":"projects/porder/#v006","text":"Merged contribution by David Shean Fixed issues with op equals None Fixed issues with relative import Improved Py3 compatability General improvements","title":"v0.0.6"},{"location":"projects/porder/#v005","text":"Added exponential backoff for pydl Fixed issues with dependency General overall improvements","title":"v0.0.5"},{"location":"projects/porder/#v004","text":"Created strict geoinstersection to avoid orders to fail Improvements to overlap function General overall improvements","title":"v0.0.4"},{"location":"projects/porder/#v003","text":"Added overlap function to idlist Added multiprocessing downloader with rate limit and extension filter General overall improvements","title":"v0.0.3"},{"location":"projects/porder/#v002","text":"Fixed issues with import modules","title":"v0.0.2"},{"location":"projects/pydrop/","text":"pydrop: Minimal Python Client for Digital Ocean Droplets \u00b6 This is a minimal tool designed to interact with the Digital Ocean API. This does not translate all functionalities of the API but is a template I created for some of the most common operations I could perform. New tools will be added in the future as I familiarize myself further with the API structure and use as a student. For now this tool allows you to summarize all your droplets running, including and necessarily a price summary to keep tabs on your droplets monthly and hourly rates. The tool also allows you to seach by tags, delete a drop or perfrom actions such as start, stop or shutdown a droplet. Table of contents \u00b6 Installation Getting started Digital Ocean Python CLI Tools Digital Ocean Key Account Info Droplets Info Volume Info Snapshot Info ssh read ssh post ssh delete Droplets Delete Droplets Reset Droplets Action Installation \u00b6 This assumes that you have native python & pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have tested this only on python 2.7.15 but can be easily modified for python 3. To install Python CLI for Digital Ocean you can install using two methods pip install pydrop or you can also try git clone https://github.com/samapriya/pydrop.git cd pydrop python setup.py install For linux use sudo. Installation is an optional step; the application can be also run directly by executing pydrop.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. If you don't want to install, browse into the pydrop folder and try python pydrop.py to get to the same result. Getting started \u00b6 As usual, to print help: usage: pydrop [-h] {dokey,accinfo,dropinfo,volinfo,snapinfo,sshread,sshpost,sshdelete,dropaction,dropdelete,dropreset} ... Digital Ocean API Python CLI positional arguments: {dokey,accinfo,dropinfo,volinfo,snapinfo,sshread,sshpost,sshdelete,dropaction,dropdelete,dropreset} dokey Enter your Digital Ocean API Key accinfo Prints your account info dropinfo Prints information about all your droplets volinfo Prints information about all your volumes snapinfo Prints information about all your snapshots sshread Prints information about your ssh keys sshpost Adds new ssh keys to account sshdelete Deletes a ssh keys from account dropaction Performs an action on your droplets dropdelete Permanently deletes the droplet dropreset Resets password for the droplet optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: pydrop dropinfo -h . If you didn't install pydrop, then you can run it just by going to pydrop directory and running python pydrop.py [arguments go here] Digital Ocean Python CLI Tools \u00b6 The Digital Ocean Python CLI and tools setup contains minimal CLI in python to perform basic actions on droplets along with query and analyze your DO enviroment quickly. Digital Ocean Key \u00b6 This tool basically asks you to input your Digital Ocean API Key using a password prompt this is then used for all subsequent tools. This tool now includes an option for a quiet authentication using the API key incase it is unable to invoke an interactive environment such as in Google colaboratory. usage: pydrop dokey [-h] [--key KEY] optional arguments: -h, --help show this help message and exit Optional named arguments: --key KEY Your Digital Ocean API Key If using on a private machine the Key is saved as a csv file for all future runs of the tool. Droplets Info \u00b6 This tool prints account info about your account including, droplet and volume limit, your email, and so on. The setup would be pydrop accinfo Droplets Info \u00b6 The droplets info tool prints summary info about all your droplets. You can choose to narrow it down further using a droplet tag so only those droplets with speific tags will be printed. Since I wanted the ability of including price summaries, I have included prices summaries. usage: pydrop dropinfo [-h] [--tag TAG] optional arguments: -h, --help show this help message and exit Optional named arguments: --tag TAG Use a tag to refine your search Volume Info \u00b6 As the name suggest incase you have any volumes attched, this will print information about the volumes attached. If no volumes exists, it will print that and exit out. The setup would be pydrop volinfo Snapshot Info \u00b6 This will print information about any existing snapshot that you might have. Setup again is pydrop snapinfo ssh read \u00b6 This setup will simply read all your ssh keys and print them including the key, the id , name and so on. Setup would be pydrop sshread ssh post \u00b6 This will allow you to add a new ssh key to your existing ssh keys. The required inputs are name and the key usage: pydrop sshpost [-h] --name NAME --keyfile KEYFILE optional arguments: -h, --help show this help message and exit Required named arguments.: --name NAME name for ssh key --keyfile KEYFILE file with ssh key ssh delete \u00b6 This will delete an ssh key, if no key id is provided, the tool first prints out all the ssh keys and their ids to choose from. Setup with a keyid would be simply pydrop sshdelete The above command will essentially call a sshread function to get you the key ids. If you know the key id the command then takes that as an input usage: pydrop sshdelete [-h] [--keyid KEYID] optional arguments: -h, --help show this help message and exit Optional named arguments: --keyid KEYID ssh key id Droplets Delete \u00b6 This deletes a droplet and you can specify either the droplet name or id. Incase you don't remember the name or id, just run the tool without any arguments and it will list out all droplet id(s) and names. usage: pydrop dropdelete [-h] [--id ID] [--name NAME] optional arguments: -h, --help show this help message and exit Optional named arguments: --id ID Use an image ID to delete droplet --name NAME Use an image name to delete droplet Droplets Reset \u00b6 This resets the password of a droplet and you can specify either the droplet name or id. Incase you don't remember the name or id, just run the tool without any arguments and it will list out all droplet id(s) and names. usage: pydrop dropreset [-h] [--id ID] [--name NAME] optional arguments: -h, --help show this help message and exit Optional named arguments: --id ID Use an image ID to delete droplet --name NAME Use an image name to delete droplet Droplets Action \u00b6 The droplet action tool was designed to achieve and have more control over individual droplet actions and I included actions such as shutdown, power off, power on and rename. Just like the droplet delete tool, this tool will print the name and id of all droplets if no arguments are passed and you can then choose the one on which to perform the action. usage: pydrop dropaction [-h] [--id ID] [--name NAME] [--action ACTION] [--rename RENAME] optional arguments: -h, --help show this help message and exit Optional named arguments: --id ID Use an image ID to perform action --name NAME Use an image name to perform action --action ACTION Action type |shutdown=\"graceful shutdown\"|power_off=\"hard shutdown\"|power_on=\"power on\"|rename=\"rename --rename RENAME Incase you are renaming droplet you can provide new name Changelog \u00b6 v0.0.5 * Added additional tools like sshfunctions, volume and snapshot reads * General improvements to overall tool v0.0.4 * Now calculates total cost till date based on active droplets v0.0.3 * Now checks for keys and auto initializes if missing * Includes password reset tool","title":"pydrop Minimal Digital Ocean CLI"},{"location":"projects/pydrop/#pydrop-minimal-python-client-for-digital-ocean-droplets","text":"This is a minimal tool designed to interact with the Digital Ocean API. This does not translate all functionalities of the API but is a template I created for some of the most common operations I could perform. New tools will be added in the future as I familiarize myself further with the API structure and use as a student. For now this tool allows you to summarize all your droplets running, including and necessarily a price summary to keep tabs on your droplets monthly and hourly rates. The tool also allows you to seach by tags, delete a drop or perfrom actions such as start, stop or shutdown a droplet.","title":"pydrop: Minimal Python Client for Digital Ocean Droplets"},{"location":"projects/pydrop/#table-of-contents","text":"Installation Getting started Digital Ocean Python CLI Tools Digital Ocean Key Account Info Droplets Info Volume Info Snapshot Info ssh read ssh post ssh delete Droplets Delete Droplets Reset Droplets Action","title":"Table of contents"},{"location":"projects/pydrop/#installation","text":"This assumes that you have native python & pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have tested this only on python 2.7.15 but can be easily modified for python 3. To install Python CLI for Digital Ocean you can install using two methods pip install pydrop or you can also try git clone https://github.com/samapriya/pydrop.git cd pydrop python setup.py install For linux use sudo. Installation is an optional step; the application can be also run directly by executing pydrop.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. If you don't want to install, browse into the pydrop folder and try python pydrop.py to get to the same result.","title":"Installation"},{"location":"projects/pydrop/#getting-started","text":"As usual, to print help: usage: pydrop [-h] {dokey,accinfo,dropinfo,volinfo,snapinfo,sshread,sshpost,sshdelete,dropaction,dropdelete,dropreset} ... Digital Ocean API Python CLI positional arguments: {dokey,accinfo,dropinfo,volinfo,snapinfo,sshread,sshpost,sshdelete,dropaction,dropdelete,dropreset} dokey Enter your Digital Ocean API Key accinfo Prints your account info dropinfo Prints information about all your droplets volinfo Prints information about all your volumes snapinfo Prints information about all your snapshots sshread Prints information about your ssh keys sshpost Adds new ssh keys to account sshdelete Deletes a ssh keys from account dropaction Performs an action on your droplets dropdelete Permanently deletes the droplet dropreset Resets password for the droplet optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: pydrop dropinfo -h . If you didn't install pydrop, then you can run it just by going to pydrop directory and running python pydrop.py [arguments go here]","title":"Getting started"},{"location":"projects/pydrop/#digital-ocean-python-cli-tools","text":"The Digital Ocean Python CLI and tools setup contains minimal CLI in python to perform basic actions on droplets along with query and analyze your DO enviroment quickly.","title":"Digital Ocean Python CLI Tools"},{"location":"projects/pydrop/#digital-ocean-key","text":"This tool basically asks you to input your Digital Ocean API Key using a password prompt this is then used for all subsequent tools. This tool now includes an option for a quiet authentication using the API key incase it is unable to invoke an interactive environment such as in Google colaboratory. usage: pydrop dokey [-h] [--key KEY] optional arguments: -h, --help show this help message and exit Optional named arguments: --key KEY Your Digital Ocean API Key If using on a private machine the Key is saved as a csv file for all future runs of the tool.","title":"Digital Ocean Key"},{"location":"projects/pydrop/#droplets-info","text":"This tool prints account info about your account including, droplet and volume limit, your email, and so on. The setup would be pydrop accinfo","title":"Droplets Info"},{"location":"projects/pydrop/#droplets-info_1","text":"The droplets info tool prints summary info about all your droplets. You can choose to narrow it down further using a droplet tag so only those droplets with speific tags will be printed. Since I wanted the ability of including price summaries, I have included prices summaries. usage: pydrop dropinfo [-h] [--tag TAG] optional arguments: -h, --help show this help message and exit Optional named arguments: --tag TAG Use a tag to refine your search","title":"Droplets Info"},{"location":"projects/pydrop/#volume-info","text":"As the name suggest incase you have any volumes attched, this will print information about the volumes attached. If no volumes exists, it will print that and exit out. The setup would be pydrop volinfo","title":"Volume Info"},{"location":"projects/pydrop/#snapshot-info","text":"This will print information about any existing snapshot that you might have. Setup again is pydrop snapinfo","title":"Snapshot Info"},{"location":"projects/pydrop/#ssh-read","text":"This setup will simply read all your ssh keys and print them including the key, the id , name and so on. Setup would be pydrop sshread","title":"ssh read"},{"location":"projects/pydrop/#ssh-post","text":"This will allow you to add a new ssh key to your existing ssh keys. The required inputs are name and the key usage: pydrop sshpost [-h] --name NAME --keyfile KEYFILE optional arguments: -h, --help show this help message and exit Required named arguments.: --name NAME name for ssh key --keyfile KEYFILE file with ssh key","title":"ssh post"},{"location":"projects/pydrop/#ssh-delete","text":"This will delete an ssh key, if no key id is provided, the tool first prints out all the ssh keys and their ids to choose from. Setup with a keyid would be simply pydrop sshdelete The above command will essentially call a sshread function to get you the key ids. If you know the key id the command then takes that as an input usage: pydrop sshdelete [-h] [--keyid KEYID] optional arguments: -h, --help show this help message and exit Optional named arguments: --keyid KEYID ssh key id","title":"ssh delete"},{"location":"projects/pydrop/#droplets-delete","text":"This deletes a droplet and you can specify either the droplet name or id. Incase you don't remember the name or id, just run the tool without any arguments and it will list out all droplet id(s) and names. usage: pydrop dropdelete [-h] [--id ID] [--name NAME] optional arguments: -h, --help show this help message and exit Optional named arguments: --id ID Use an image ID to delete droplet --name NAME Use an image name to delete droplet","title":"Droplets Delete"},{"location":"projects/pydrop/#droplets-reset","text":"This resets the password of a droplet and you can specify either the droplet name or id. Incase you don't remember the name or id, just run the tool without any arguments and it will list out all droplet id(s) and names. usage: pydrop dropreset [-h] [--id ID] [--name NAME] optional arguments: -h, --help show this help message and exit Optional named arguments: --id ID Use an image ID to delete droplet --name NAME Use an image name to delete droplet","title":"Droplets Reset"},{"location":"projects/pydrop/#droplets-action","text":"The droplet action tool was designed to achieve and have more control over individual droplet actions and I included actions such as shutdown, power off, power on and rename. Just like the droplet delete tool, this tool will print the name and id of all droplets if no arguments are passed and you can then choose the one on which to perform the action. usage: pydrop dropaction [-h] [--id ID] [--name NAME] [--action ACTION] [--rename RENAME] optional arguments: -h, --help show this help message and exit Optional named arguments: --id ID Use an image ID to perform action --name NAME Use an image name to perform action --action ACTION Action type |shutdown=\"graceful shutdown\"|power_off=\"hard shutdown\"|power_on=\"power on\"|rename=\"rename --rename RENAME Incase you are renaming droplet you can provide new name","title":"Droplets Action"},{"location":"projects/pydrop/#changelog","text":"v0.0.5 * Added additional tools like sshfunctions, volume and snapshot reads * General improvements to overall tool v0.0.4 * Now calculates total cost till date based on active droplets v0.0.3 * Now checks for keys and auto initializes if missing * Includes password reset tool","title":"Changelog"},{"location":"projects/satadd/","text":"satadd: CLI pipeline for Planet, Satellogic, Google Earth Engine and Digital Globe Imagery \u00b6 Cite as Samapriya Roy. (2018, October 6). samapriya/satadd: satadd: CLI pipeline for Planet, Satellogic, Google Earth Engine and Digital Globe Imagery (Version 0.0.3). Zenodo. http://doi.org/10.5281/zenodo.1450622 Google Earth Engine opened the door for the possibility of getting a curated list of public datasets already ingested in an analysis platform. With over 450+ raster datasets alone, they form one of the most unique collections for publicly available datasets and are still growing. While this was happening for free and open source datasets more and more data was coming in and companies were opening their doors to open data and has missions to include researchers, users, developers and everyone else who wanted to use this datasets. This included but is not limited to companies like Planet, Digital Globe and Satellogic making large chunks of their datasets open for users. Also introduction of high temporal resolution datasets like PlanetScope, high spatial resolution like Skysat and Digital Globe and high spectral resolution images like hyperspectral data from Satellogic was changing our approach to problem solving. While there has been development in building standard API and data access methods there is still room for growth and standrardization and above all easy access to these resources. Planet's Open California Program , the Education and Research Program , Digital Globe's Open Data Program and Education and Research program under Satellogic and their Open Data it became obvious that questions we can ask from these sensors could get interesting. This tool was built with a focus on the same issues and borrow parts from my other projects such as ppipe for handling Planet's datasets, gee2drive to handle download collections already available in Google Earth Engine (GEE), pygbdx which is a relatively new project to explore Digital Globe assets and I have now integrated tools to access and download Satellogic imagery. Core components from a lot of these tools have gone into building satadd based on the idea of adding satelite data as needed. These tools include authentications setups for every account, and access to datasets, metadata among other tools. This was not build however for heavy lifting though I have tested this on hundreds and thousands of assets delivery so it behaves robustly for now. The tool is build and rebuilt as companies change their authentication protocal and delivery mechanisms and allow for improving many aspects of data delivery and preprocessing in the next iterations. While almost all of these tools allow for local export, GEE only exports for now to Google Drive or your Google Cloud Storage Buckets, though what is lost in the delivery endpoints is gained in the fact that GEE is already a mature platform to analyze and look at open datasets but also allows you to bring private datasets into GEE for analysis. So while data download and local analysis may have been the norm, it serves us well to think about posting analysis rather in analysis engines. But that is a discussion for a different time. At this point, I am hoping that this tool alows you to do exactly what the intentions might have been from different providers and to bring them together. Since this tool downloads data it is indeed bandwidth heavy and requires a steady internet connection. This tools handles authentication, downloading, and talking to different API end points and services. In the future I am hoping to include additional preprocessing and delivery to non local endpoints like existing ftp, servers or buckets. Table of contents \u00b6 Installation Getting started satadd Satellite Data Download Addon Initialize and Authenticate Planet Tools GBDX Tools Satellogic Tools GEE Tools Installation \u00b6 This assumes that you have native python & pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have released this as a python 2.7 but can be easily modified for python 3. This toolbox also uses some functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa && sudo apt-get update sudo apt-get install gdal-bin sudo apt-get install python-gdal For Windows I found this guide from UCLA It has been brought to my attention that installing shapely on windows is not simply pip install shapely so install Shapely separately and use instructions from their pypi project page for Windows installation Shapely is important requirement for the tool but since the installation varies based on the operating system install it using the earlier instructions anyways before the next steps . On other operating systems pip install shapely should work just fine. To install satadd You can install using two methods pip install satadd or you can also try git clone https://github.com/samapriya/satadd.git cd satadd python setup.py install For linux use sudo. This release also contains a windows installer which bypasses the need for you to have admin permission, it does however require you to have python in the system path meaning when you open up command prompt you should be able to type python and start it within the command prompt window. Post installation using the installer you can just call satadd using the command prompt similar to calling python. Give it a go post installation type satadd -h Installation is an optional step; the application can be also run directly by executing satadd.py script. The advantage of having it installed is being able to execute satadd as any command line tool. I recommend installation within virtual environment. If you don't want to install, browse into the satadd folder and try python satadd.py to get to the same result. Getting started \u00b6 As usual, to print help: usage: satadd.py [-h] {planetkey,dginit,satinit,eeinit,dasync,savedsearch,metadata,simple_search,footprint,satraster,satmeta,metalist,reproject,refresh,idsearch,intersect,band type,export} ... Simple CLI for piping Planet, Satellogic,GEE & GBDX Assets positional arguments: {planetkey,dginit,satinit,eeinit,dasync,savedsearch,metadata,simple_search,footprint,satraster,satmeta,metalist,reproject,refresh,idsearch,intersect,bandtype,export} planetkey Setting up planet API Key dginit Initialize Digital Globe GBDX satinit Initialize Satellogic Tokens eeinit Initialize Google Earth Engine credrefresh Refresh Satellogic & GBDX tokens dasync Uses the Planet Client Async Downloader to download Planet Assets: Does not require activation savedsearch Tool to download saved searches from Planet Explorer metadata Tool to tabulate and convert all metadata files from Planet Item and Asset types for Ingestion into GEE simple_search Simple search to look for DG assets that intersect your AOI handles KML/SHP/GEOJSON metadata Exports metadata for simple search into constitutent folders as JSON files footprint Exports footprint for metadata files extracted earlier and converts them to individual geometries (GeoJSON)and combined geometry (GeoJSON) file satraster Filter and download Satellogic Imagery satlist Get url for band list based on filtered Satellogic Imagery multiproc Multiprocess based downloader based on satlist satmeta Filter and download Satellogic Metadata metalist Generates Basic Metadata list per scene for Satellogic Imagery reproject Batch reproject rasters using EPSG code eerefresh Refreshes your personal asset list and GEE Asset list idsearch Does possible matches using asset name to give you asseth id/full path intersect Exports a report of all assets(Personal & GEE) intersecting with provided geometry bandtype Prints GEE bandtype and generates list to be used for export export Export GEE Collections based on filter optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: satadd idsearch -h . If you didn't install satadd, then you can run it just by going to satadd directory and running python satadd.py [arguments go here] satadd Satellite Data Download Addon \u00b6 This tool is designed to augment to the existing facilty of image export using a CLI, whereby you can pass it arguments to filter based on an area of interest geojson file, a start and end date for collection Initialize and Authenticate \u00b6 This is an autosuggestive terminal which uses the gee2add package to perform all of the functions but has autosuggest for Earth Engine catalog and your own personal catalog. This way you can get access to image id without needing the catalog id in the javascript codeeditor. planetkey Setting up planet API Key dginit Initialize Digital Globe GBDX satinit Initialize Satellogic Tokens eeinit Initialize Google Earth Engine credrefresh Refresh Satellogic & GBDX tokens Each of these authentication tools allow you to link and save credentials for each of these services you can check them by typing something like satadd planetkey . Certain services require the authentication tokens to be refreshed you can simply access it using satadd credrefresh . Planet Tools \u00b6 The Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications. These tools are designed to interact with Planet's Python Client and the saved search featured embedded in Planet Explorer and will allow you to access and download planet imagery and metadata as needed. This also allows you to process the metadata incase you are ingesting this to GEE. dasync Uses the Planet Client Async Downloader to download Planet Assets: Does not require activation savedsearch Tool to download saved searches from Planet Explorer metadata Tool to tabulate and convert all metadata files from Planet Item and Asset types for Ingestion into GEE GBDX Tools \u00b6 This is a simple cli to Digital Globe's GBDX platform, this was designed from the perspective of community user (the freely available tier). This platform allows you to access all of DG's Open data and also open Ikonos data along with Landsat and Sentinel datasets. You can create a notebook acccount here . The notebook setup offers additional tools, a GUI and interactive framework while CLI simplifies some of the operational needs of batch processing and performing calls using your own local machine. This tool will allow you to perform a simple seach using a geometry to get asset summary, export the metadata as json file and also image footprint as a combined and individual geojson files. simple_search Simple search to look for DG assets that intersect your AOI handles KML/SHP/GEOJSON metadata Exports metadata for simple search into constitutent folders as JSON files footprint Exports footprint for metadata files extracted earlier and converts them to individual geometries (GeoJSON)and combined geometry (GeoJSON) file Satellogic Tools \u00b6 This tool allows you to access the open data shared by Satellogic and filter and pass a geometry object to get both micro(multiband) and macro (hyperspectral) rasters, metadata and basic metadalist. The download tool is a multipart downloader to handle quick downloads. The metalist tool can be used to create a simple metadata list for you to batch upload imagery into GEE for analysis. The reproject tool is included to handle batch reprojections as needed. The tool uses geometry passed as a geojson object go to geojson.io . Satlist produces the band list urls and you can then use the multiproc tool to use multiprocessing to download the links. satraster Filter and download Satellogic Imagery satlist Get url for band list based on filtered Satellogic Imagery multiproc Multiprocess based downloader based on satlist satmeta Filter and download Satellogic Metadata metalist Generates Basic Metadata list per scene for Satellogic Imagery reproject Batch reproject rasters using EPSG code GEE Tools \u00b6 This tool allows you to use the gee2drive tool functionalities to explore, match and export existing collections in GEE. Export requires all the bandtypes to be of the same kind. For the past couple of months I have maintained a catalog of the most current Google Earth Engine assets , within their raster data catalog. I update this list every week. This tool downloads the most current version of this list, and allows the user to explore band types and export a collection as needed. eerefresh Refreshes your personal asset list and GEE Asset list idsearch Does possible matches using asset name to give you asseth id/full path intersect Exports a report of all assets(Personal & GEE) intersecting with provided geometry bandtype Prints GEE bandtype and generates list to be used for export export Export GEE Collections based on filter Changelog \u00b6 v0.0.4 \u00b6 Fixed issue with Shapely install on windows Updated credrefresh to better refresh gbdx tokens v0.0.3 \u00b6 Added better filename parsing for Satellogic images Added error handling for multiprocessing download of Satellogic images v0.0.2 \u00b6 Now searches for all DG and non DG assets available within GBDX Added capability to create url list for rasters and download support using multiprocessing","title":"Simple CLI for piping Planet, Satellogic, GEE & GBDX Assets"},{"location":"projects/satadd/#satadd-cli-pipeline-for-planet-satellogic-google-earth-engine-and-digital-globe-imagery","text":"Cite as Samapriya Roy. (2018, October 6). samapriya/satadd: satadd: CLI pipeline for Planet, Satellogic, Google Earth Engine and Digital Globe Imagery (Version 0.0.3). Zenodo. http://doi.org/10.5281/zenodo.1450622 Google Earth Engine opened the door for the possibility of getting a curated list of public datasets already ingested in an analysis platform. With over 450+ raster datasets alone, they form one of the most unique collections for publicly available datasets and are still growing. While this was happening for free and open source datasets more and more data was coming in and companies were opening their doors to open data and has missions to include researchers, users, developers and everyone else who wanted to use this datasets. This included but is not limited to companies like Planet, Digital Globe and Satellogic making large chunks of their datasets open for users. Also introduction of high temporal resolution datasets like PlanetScope, high spatial resolution like Skysat and Digital Globe and high spectral resolution images like hyperspectral data from Satellogic was changing our approach to problem solving. While there has been development in building standard API and data access methods there is still room for growth and standrardization and above all easy access to these resources. Planet's Open California Program , the Education and Research Program , Digital Globe's Open Data Program and Education and Research program under Satellogic and their Open Data it became obvious that questions we can ask from these sensors could get interesting. This tool was built with a focus on the same issues and borrow parts from my other projects such as ppipe for handling Planet's datasets, gee2drive to handle download collections already available in Google Earth Engine (GEE), pygbdx which is a relatively new project to explore Digital Globe assets and I have now integrated tools to access and download Satellogic imagery. Core components from a lot of these tools have gone into building satadd based on the idea of adding satelite data as needed. These tools include authentications setups for every account, and access to datasets, metadata among other tools. This was not build however for heavy lifting though I have tested this on hundreds and thousands of assets delivery so it behaves robustly for now. The tool is build and rebuilt as companies change their authentication protocal and delivery mechanisms and allow for improving many aspects of data delivery and preprocessing in the next iterations. While almost all of these tools allow for local export, GEE only exports for now to Google Drive or your Google Cloud Storage Buckets, though what is lost in the delivery endpoints is gained in the fact that GEE is already a mature platform to analyze and look at open datasets but also allows you to bring private datasets into GEE for analysis. So while data download and local analysis may have been the norm, it serves us well to think about posting analysis rather in analysis engines. But that is a discussion for a different time. At this point, I am hoping that this tool alows you to do exactly what the intentions might have been from different providers and to bring them together. Since this tool downloads data it is indeed bandwidth heavy and requires a steady internet connection. This tools handles authentication, downloading, and talking to different API end points and services. In the future I am hoping to include additional preprocessing and delivery to non local endpoints like existing ftp, servers or buckets.","title":"satadd: CLI pipeline for Planet, Satellogic, Google Earth Engine and Digital Globe Imagery"},{"location":"projects/satadd/#table-of-contents","text":"Installation Getting started satadd Satellite Data Download Addon Initialize and Authenticate Planet Tools GBDX Tools Satellogic Tools GEE Tools","title":"Table of contents"},{"location":"projects/satadd/#installation","text":"This assumes that you have native python & pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have released this as a python 2.7 but can be easily modified for python 3. This toolbox also uses some functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa && sudo apt-get update sudo apt-get install gdal-bin sudo apt-get install python-gdal For Windows I found this guide from UCLA It has been brought to my attention that installing shapely on windows is not simply pip install shapely so install Shapely separately and use instructions from their pypi project page for Windows installation Shapely is important requirement for the tool but since the installation varies based on the operating system install it using the earlier instructions anyways before the next steps . On other operating systems pip install shapely should work just fine. To install satadd You can install using two methods pip install satadd or you can also try git clone https://github.com/samapriya/satadd.git cd satadd python setup.py install For linux use sudo. This release also contains a windows installer which bypasses the need for you to have admin permission, it does however require you to have python in the system path meaning when you open up command prompt you should be able to type python and start it within the command prompt window. Post installation using the installer you can just call satadd using the command prompt similar to calling python. Give it a go post installation type satadd -h Installation is an optional step; the application can be also run directly by executing satadd.py script. The advantage of having it installed is being able to execute satadd as any command line tool. I recommend installation within virtual environment. If you don't want to install, browse into the satadd folder and try python satadd.py to get to the same result.","title":"Installation"},{"location":"projects/satadd/#getting-started","text":"As usual, to print help: usage: satadd.py [-h] {planetkey,dginit,satinit,eeinit,dasync,savedsearch,metadata,simple_search,footprint,satraster,satmeta,metalist,reproject,refresh,idsearch,intersect,band type,export} ... Simple CLI for piping Planet, Satellogic,GEE & GBDX Assets positional arguments: {planetkey,dginit,satinit,eeinit,dasync,savedsearch,metadata,simple_search,footprint,satraster,satmeta,metalist,reproject,refresh,idsearch,intersect,bandtype,export} planetkey Setting up planet API Key dginit Initialize Digital Globe GBDX satinit Initialize Satellogic Tokens eeinit Initialize Google Earth Engine credrefresh Refresh Satellogic & GBDX tokens dasync Uses the Planet Client Async Downloader to download Planet Assets: Does not require activation savedsearch Tool to download saved searches from Planet Explorer metadata Tool to tabulate and convert all metadata files from Planet Item and Asset types for Ingestion into GEE simple_search Simple search to look for DG assets that intersect your AOI handles KML/SHP/GEOJSON metadata Exports metadata for simple search into constitutent folders as JSON files footprint Exports footprint for metadata files extracted earlier and converts them to individual geometries (GeoJSON)and combined geometry (GeoJSON) file satraster Filter and download Satellogic Imagery satlist Get url for band list based on filtered Satellogic Imagery multiproc Multiprocess based downloader based on satlist satmeta Filter and download Satellogic Metadata metalist Generates Basic Metadata list per scene for Satellogic Imagery reproject Batch reproject rasters using EPSG code eerefresh Refreshes your personal asset list and GEE Asset list idsearch Does possible matches using asset name to give you asseth id/full path intersect Exports a report of all assets(Personal & GEE) intersecting with provided geometry bandtype Prints GEE bandtype and generates list to be used for export export Export GEE Collections based on filter optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: satadd idsearch -h . If you didn't install satadd, then you can run it just by going to satadd directory and running python satadd.py [arguments go here]","title":"Getting started"},{"location":"projects/satadd/#satadd-satellite-data-download-addon","text":"This tool is designed to augment to the existing facilty of image export using a CLI, whereby you can pass it arguments to filter based on an area of interest geojson file, a start and end date for collection","title":"satadd Satellite Data Download Addon"},{"location":"projects/satadd/#initialize-and-authenticate","text":"This is an autosuggestive terminal which uses the gee2add package to perform all of the functions but has autosuggest for Earth Engine catalog and your own personal catalog. This way you can get access to image id without needing the catalog id in the javascript codeeditor. planetkey Setting up planet API Key dginit Initialize Digital Globe GBDX satinit Initialize Satellogic Tokens eeinit Initialize Google Earth Engine credrefresh Refresh Satellogic & GBDX tokens Each of these authentication tools allow you to link and save credentials for each of these services you can check them by typing something like satadd planetkey . Certain services require the authentication tokens to be refreshed you can simply access it using satadd credrefresh .","title":"Initialize and Authenticate"},{"location":"projects/satadd/#planet-tools","text":"The Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications. These tools are designed to interact with Planet's Python Client and the saved search featured embedded in Planet Explorer and will allow you to access and download planet imagery and metadata as needed. This also allows you to process the metadata incase you are ingesting this to GEE. dasync Uses the Planet Client Async Downloader to download Planet Assets: Does not require activation savedsearch Tool to download saved searches from Planet Explorer metadata Tool to tabulate and convert all metadata files from Planet Item and Asset types for Ingestion into GEE","title":"Planet Tools"},{"location":"projects/satadd/#gbdx-tools","text":"This is a simple cli to Digital Globe's GBDX platform, this was designed from the perspective of community user (the freely available tier). This platform allows you to access all of DG's Open data and also open Ikonos data along with Landsat and Sentinel datasets. You can create a notebook acccount here . The notebook setup offers additional tools, a GUI and interactive framework while CLI simplifies some of the operational needs of batch processing and performing calls using your own local machine. This tool will allow you to perform a simple seach using a geometry to get asset summary, export the metadata as json file and also image footprint as a combined and individual geojson files. simple_search Simple search to look for DG assets that intersect your AOI handles KML/SHP/GEOJSON metadata Exports metadata for simple search into constitutent folders as JSON files footprint Exports footprint for metadata files extracted earlier and converts them to individual geometries (GeoJSON)and combined geometry (GeoJSON) file","title":"GBDX Tools"},{"location":"projects/satadd/#satellogic-tools","text":"This tool allows you to access the open data shared by Satellogic and filter and pass a geometry object to get both micro(multiband) and macro (hyperspectral) rasters, metadata and basic metadalist. The download tool is a multipart downloader to handle quick downloads. The metalist tool can be used to create a simple metadata list for you to batch upload imagery into GEE for analysis. The reproject tool is included to handle batch reprojections as needed. The tool uses geometry passed as a geojson object go to geojson.io . Satlist produces the band list urls and you can then use the multiproc tool to use multiprocessing to download the links. satraster Filter and download Satellogic Imagery satlist Get url for band list based on filtered Satellogic Imagery multiproc Multiprocess based downloader based on satlist satmeta Filter and download Satellogic Metadata metalist Generates Basic Metadata list per scene for Satellogic Imagery reproject Batch reproject rasters using EPSG code","title":"Satellogic Tools"},{"location":"projects/satadd/#gee-tools","text":"This tool allows you to use the gee2drive tool functionalities to explore, match and export existing collections in GEE. Export requires all the bandtypes to be of the same kind. For the past couple of months I have maintained a catalog of the most current Google Earth Engine assets , within their raster data catalog. I update this list every week. This tool downloads the most current version of this list, and allows the user to explore band types and export a collection as needed. eerefresh Refreshes your personal asset list and GEE Asset list idsearch Does possible matches using asset name to give you asseth id/full path intersect Exports a report of all assets(Personal & GEE) intersecting with provided geometry bandtype Prints GEE bandtype and generates list to be used for export export Export GEE Collections based on filter","title":"GEE Tools"},{"location":"projects/satadd/#changelog","text":"","title":"Changelog"},{"location":"projects/satadd/#v004","text":"Fixed issue with Shapely install on windows Updated credrefresh to better refresh gbdx tokens","title":"v0.0.4"},{"location":"projects/satadd/#v003","text":"Added better filename parsing for Satellogic images Added error handling for multiprocessing download of Satellogic images","title":"v0.0.3"},{"location":"projects/satadd/#v002","text":"Now searches for all DG and non DG assets available within GBDX Added capability to create url list for rasters and download support using multiprocessing","title":"v0.0.2"},{"location":"projects/slack_notifier_cli_addon/","text":"Slack Notifier-CLI Addon \u00b6 For those working with team collaborations and notifications slack is a quick alternative to group emails and chats. The need for a notification tool was also met with the use of the API which could be neatly tied up in clients. This CLI add-on was developed simply to function as an additional tool which can reside in an application folder and which can be called upon within a program and act as a notifier for events and updates. The tool combines simple methods in building channels and application bots and uses backends to send messages, files and to handle message history. The notifier was based on a comparison between available methods in tools such as pushbullet, pushover among a few for being able to have cross platform compatibility. Though this was designed as a means for getting process update for specific tools this CLI is essentially a plug and play into any system which can talk and pass arguments to this tool. In time additional and more refined implementation control might be included to handle specific functions. Table of contents \u00b6 Getting started Slack Credential Slack-Bot Credential Slack Messages Slack Message with Attachment Slack Delete All Getting started \u00b6 To get started you need a Slack account and you can create one here . Once you create the slack team you can further create an application and a bot within your team. This will allow you to get two API keys that you need for your tool. To access both these API tokens go to . On the features, tab should be an option called OAuth & Permissions and should provide you with OAuth Access Token and Bot User OAuth Access Token . Note that the bot can only post in those channels where you have given it permission. If you add the bot to multiple channels you can specify the channel when posting messages or files. Just browse to the folder and perform python slack_addon.py -h : usage: slack_addon.py [-h] { ,smain,sbot,botupdate,botfile,slackdelete} ... Slack API Addon positional arguments: { ,smain,sbot,botupdate,botfile,slackdelete} ------------------------------------------- -----Choose from Slack Tools Below----- ------------------------------------------- smain Allows you to save your Slack Main API Token sbot Allows you to save your Slack Bot API Token botupdate Allows your bot to post messages on slack channel botfile Allows you to post a file along with comments slackdelete Allows users to delete all messages and files posted by bots optional arguments: -h, --help show this help message and exit Slack Credential \u00b6 This tool allows the user to save slack credential(OAuth Access Token) into users/.config/slackkey making sure that they are user specific and are not shared on a system resource or location. It uses a getpass implementation and writes the password as a CSV and the other tools first try to read this and if not present asks for your password. usage: slack_addon.py smain [-h] optional arguments: -h, --help show this help message and exit Slack-Bot Credential \u00b6 This tool allows the user to save slack bot credential(Bot User OAuth Access Token) into users/.config/slackkey making sure that they are user specific and are not shared on a system resource or location. It uses a getpass implementation and writes the password as a CSV and the other tools first try to read this and if not present asks for your password. usage: slack_addon.py sbot [-h] optional arguments: -h, --help show this help message and exit Slack Messages \u00b6 The slack messaging application is the primary tool which uses the slacker backend and allows the user to send messages as a bot to specific channel(s). The messaging service reads your Bot User OAuth Access Token and allows you to send messages to all channels where the bot has been added or has permission to post. If you do not specify the channel the bot posts to the general channel. usage: slack_addon.py botupdate [-h] [--channel CHANNEL] [--msg MSG] optional arguments: -h, --help show this help message and exit --channel CHANNEL Slack Bot update channel --msg MSG Slack Bot update message Incase you have already saved your password sending a message is as simple as python slack_addon.py --channel \"#general\" --message \"Hello world\" The application can simple be added by a call command with any process running as a system and the bot can update you about system processes, about usage, about application status and file sizes. The possibilities are endless. Slack Message with Attachment \u00b6 One of the most interesting applications for me was to check that not only can I sent system and application updates but I could send snapshots or process outputs such as excel files and zip files and even error logs as needed. This tool allows the slack bot to not only send a message but only to include a file with the message. The filepath points to the location of the file, the fname allows you to name the file accordingly and the cmmt option is used to add a coment of message along with the file. The channel option allows you to choose a specific channel you want to post the message and as earlier it will post to general channel. usage: slack_addon.py botfile [-h] [--channel CHANNEL] [--filepath FILEPATH] [--cmmt CMMT] [--fname FNAME] optional arguments: -h, --help show this help message and exit --channel CHANNEL Slack Bot channel --filepath FILEPATH Slack Bot file path to upload --cmmt CMMT Slack Bot file comment --fname FNAME Slack Bot filename Incase you have already saved your password a setup would be simply python slack_addon.py --channel \"#general\" --filepath \"/users/myfilepath.csv\" --cmmt \"Check the error logs\" --fname \"errorlog\" Slack Delete All \u00b6 One of the current non existent methods within Slack is the capability to delete all messages. This is built using a backend cli tool to delete all messages and files if needed and I integrated that in the current CLI. The current tool is primarily related to deleting all messages and files for cleaning up a channel as needed. The tool uses your main slack channel API token and uses that to delete all messages from all users but can be modified to delete messages from specific bots if needed. For now the tool deletes all messages and files in the general channel. usage: slack_addon.py slackdelete [-h] optional arguments: -h, --help show this help message and exit to use this tool simple type python slack_addon.py slackdelete","title":"Slack Notifier CLI Addon"},{"location":"projects/slack_notifier_cli_addon/#slack-notifier-cli-addon","text":"For those working with team collaborations and notifications slack is a quick alternative to group emails and chats. The need for a notification tool was also met with the use of the API which could be neatly tied up in clients. This CLI add-on was developed simply to function as an additional tool which can reside in an application folder and which can be called upon within a program and act as a notifier for events and updates. The tool combines simple methods in building channels and application bots and uses backends to send messages, files and to handle message history. The notifier was based on a comparison between available methods in tools such as pushbullet, pushover among a few for being able to have cross platform compatibility. Though this was designed as a means for getting process update for specific tools this CLI is essentially a plug and play into any system which can talk and pass arguments to this tool. In time additional and more refined implementation control might be included to handle specific functions.","title":"Slack Notifier-CLI Addon"},{"location":"projects/slack_notifier_cli_addon/#table-of-contents","text":"Getting started Slack Credential Slack-Bot Credential Slack Messages Slack Message with Attachment Slack Delete All","title":"Table of contents"},{"location":"projects/slack_notifier_cli_addon/#getting-started","text":"To get started you need a Slack account and you can create one here . Once you create the slack team you can further create an application and a bot within your team. This will allow you to get two API keys that you need for your tool. To access both these API tokens go to . On the features, tab should be an option called OAuth & Permissions and should provide you with OAuth Access Token and Bot User OAuth Access Token . Note that the bot can only post in those channels where you have given it permission. If you add the bot to multiple channels you can specify the channel when posting messages or files. Just browse to the folder and perform python slack_addon.py -h : usage: slack_addon.py [-h] { ,smain,sbot,botupdate,botfile,slackdelete} ... Slack API Addon positional arguments: { ,smain,sbot,botupdate,botfile,slackdelete} ------------------------------------------- -----Choose from Slack Tools Below----- ------------------------------------------- smain Allows you to save your Slack Main API Token sbot Allows you to save your Slack Bot API Token botupdate Allows your bot to post messages on slack channel botfile Allows you to post a file along with comments slackdelete Allows users to delete all messages and files posted by bots optional arguments: -h, --help show this help message and exit","title":"Getting started"},{"location":"projects/slack_notifier_cli_addon/#slack-credential","text":"This tool allows the user to save slack credential(OAuth Access Token) into users/.config/slackkey making sure that they are user specific and are not shared on a system resource or location. It uses a getpass implementation and writes the password as a CSV and the other tools first try to read this and if not present asks for your password. usage: slack_addon.py smain [-h] optional arguments: -h, --help show this help message and exit","title":"Slack Credential"},{"location":"projects/slack_notifier_cli_addon/#slack-bot-credential","text":"This tool allows the user to save slack bot credential(Bot User OAuth Access Token) into users/.config/slackkey making sure that they are user specific and are not shared on a system resource or location. It uses a getpass implementation and writes the password as a CSV and the other tools first try to read this and if not present asks for your password. usage: slack_addon.py sbot [-h] optional arguments: -h, --help show this help message and exit","title":"Slack-Bot Credential"},{"location":"projects/slack_notifier_cli_addon/#slack-messages","text":"The slack messaging application is the primary tool which uses the slacker backend and allows the user to send messages as a bot to specific channel(s). The messaging service reads your Bot User OAuth Access Token and allows you to send messages to all channels where the bot has been added or has permission to post. If you do not specify the channel the bot posts to the general channel. usage: slack_addon.py botupdate [-h] [--channel CHANNEL] [--msg MSG] optional arguments: -h, --help show this help message and exit --channel CHANNEL Slack Bot update channel --msg MSG Slack Bot update message Incase you have already saved your password sending a message is as simple as python slack_addon.py --channel \"#general\" --message \"Hello world\" The application can simple be added by a call command with any process running as a system and the bot can update you about system processes, about usage, about application status and file sizes. The possibilities are endless.","title":"Slack Messages"},{"location":"projects/slack_notifier_cli_addon/#slack-message-with-attachment","text":"One of the most interesting applications for me was to check that not only can I sent system and application updates but I could send snapshots or process outputs such as excel files and zip files and even error logs as needed. This tool allows the slack bot to not only send a message but only to include a file with the message. The filepath points to the location of the file, the fname allows you to name the file accordingly and the cmmt option is used to add a coment of message along with the file. The channel option allows you to choose a specific channel you want to post the message and as earlier it will post to general channel. usage: slack_addon.py botfile [-h] [--channel CHANNEL] [--filepath FILEPATH] [--cmmt CMMT] [--fname FNAME] optional arguments: -h, --help show this help message and exit --channel CHANNEL Slack Bot channel --filepath FILEPATH Slack Bot file path to upload --cmmt CMMT Slack Bot file comment --fname FNAME Slack Bot filename Incase you have already saved your password a setup would be simply python slack_addon.py --channel \"#general\" --filepath \"/users/myfilepath.csv\" --cmmt \"Check the error logs\" --fname \"errorlog\"","title":"Slack Message with Attachment"},{"location":"projects/slack_notifier_cli_addon/#slack-delete-all","text":"One of the current non existent methods within Slack is the capability to delete all messages. This is built using a backend cli tool to delete all messages and files if needed and I integrated that in the current CLI. The current tool is primarily related to deleting all messages and files for cleaning up a channel as needed. The tool uses your main slack channel API token and uses that to delete all messages from all users but can be modified to delete messages from specific bots if needed. For now the tool deletes all messages and files in the general channel. usage: slack_addon.py slackdelete [-h] optional arguments: -h, --help show this help message and exit to use this tool simple type python slack_addon.py slackdelete","title":"Slack Delete All"},{"location":"projects/synthetic_models/","text":"Synthetic LandScape Generation \u00b6 While working on synthetic landscape generation or neutral landscape generation, it it interesting to try out different randomizing and clustering algorithms. The ones included here were part of the same experiments. The outputs are generally preceded by a header so that the ASCII files can be read easily in a GIS software such as ArcMap. The output file is ASCII type since this is commonly accepted by multiple tools. The number of rows and columns can be changed as can the no data value which for now is set to a 16 bit signed integer output. There are codes included to generate a video using landscapes as frames within the output file. Further work can include the nlmpy module Table of contents \u00b6 Installation Packages Clumped matrix algorithms Fragmentation Aggregation Algorithms NlmPy applications Noise Function Terrain Generation Random matrix algorithms Random matrix to video Credits Installation \u00b6 We assume that the user already has a copy of Matlab and has installed necessary libraries in python such as nlmpy and numpy+mlk and scipy. The toolbox can be modified to accomodate varying sizes of images to be produced and varying number of classes. The codes are mostly written in matlab and some implementation in NlmPy produced by Etherington et al are included as well. Packages \u00b6 Each folder has a variety of packages and tools available to run different kinds of analysis. Tools and methods include and are not limited to the following Clumped matrix algorithms \u00b6 This allows for clumping to occur in random matrix algorithms such that there is an element of non random allocation of class filename Description clumped_randi_land.m clumping algorithm applied to Uniformly distributed pseudorandom integers clumped_sprand_land clumping algorithm applied to Sparse uniformly distributed random matrix Fragmentation Aggregation Algorithms \u00b6 This one is more specific looking at different fragmentation and aggregation algorithms and allows the user to set convergence time for solution.It includes Gibbs model and Metropolis-Hastings algorithm. NlmPy applications \u00b6 NlmPy was created as a python library which allows the user to user different algorithms Etherington et al . The output files as ASCII to allow for easy read. Noise Function Terrain Generation \u00b6 These Matlab functions allows you to test noise functions to generate artificial terrains using noise function executables.Source codes for the c++ noise functions files are included in the adjoining folder and were provided kindly by Travis Archer for open use.The codes create a bmp output which are then modified to create binned responses and landscape classes as needed. Noise types include * Cell Noise * Diamond Square * Erosion * Midpoint Displacement * Perline Noise * Simplex Noise * Value Noise Random matrix algorithms \u00b6 This uses the random matrix functions within matlab to extract different landscape types. Once again they are exported as ASCII. filename Description randi_land.m Uniformly distributed pseudorandom integers sprand_land.m Sparse uniformly distributed random matrix rand_land.m Uniformly distributed random numbers rng_seed_land Random Seed Generation with Random Number Generator Random matrix to video \u00b6 These are experiments within matlab to use the landscapes generated as frames in a video output file from the landscapes. The video output is generally in avi format and includes slices of the landscapes produced. Credits \u00b6 I would like to thank Dr. Scott Robeson my mentor among others who got me interested in some topics on Landscape ecology.","title":"Synthetic Landscape Generation Models"},{"location":"projects/synthetic_models/#synthetic-landscape-generation","text":"While working on synthetic landscape generation or neutral landscape generation, it it interesting to try out different randomizing and clustering algorithms. The ones included here were part of the same experiments. The outputs are generally preceded by a header so that the ASCII files can be read easily in a GIS software such as ArcMap. The output file is ASCII type since this is commonly accepted by multiple tools. The number of rows and columns can be changed as can the no data value which for now is set to a 16 bit signed integer output. There are codes included to generate a video using landscapes as frames within the output file. Further work can include the nlmpy module","title":"Synthetic LandScape Generation"},{"location":"projects/synthetic_models/#table-of-contents","text":"Installation Packages Clumped matrix algorithms Fragmentation Aggregation Algorithms NlmPy applications Noise Function Terrain Generation Random matrix algorithms Random matrix to video Credits","title":"Table of contents"},{"location":"projects/synthetic_models/#installation","text":"We assume that the user already has a copy of Matlab and has installed necessary libraries in python such as nlmpy and numpy+mlk and scipy. The toolbox can be modified to accomodate varying sizes of images to be produced and varying number of classes. The codes are mostly written in matlab and some implementation in NlmPy produced by Etherington et al are included as well.","title":"Installation"},{"location":"projects/synthetic_models/#packages","text":"Each folder has a variety of packages and tools available to run different kinds of analysis. Tools and methods include and are not limited to the following","title":"Packages"},{"location":"projects/synthetic_models/#clumped-matrix-algorithms","text":"This allows for clumping to occur in random matrix algorithms such that there is an element of non random allocation of class filename Description clumped_randi_land.m clumping algorithm applied to Uniformly distributed pseudorandom integers clumped_sprand_land clumping algorithm applied to Sparse uniformly distributed random matrix","title":"Clumped matrix algorithms"},{"location":"projects/synthetic_models/#fragmentation-aggregation-algorithms","text":"This one is more specific looking at different fragmentation and aggregation algorithms and allows the user to set convergence time for solution.It includes Gibbs model and Metropolis-Hastings algorithm.","title":"Fragmentation Aggregation Algorithms"},{"location":"projects/synthetic_models/#nlmpy-applications","text":"NlmPy was created as a python library which allows the user to user different algorithms Etherington et al . The output files as ASCII to allow for easy read.","title":"NlmPy applications"},{"location":"projects/synthetic_models/#noise-function-terrain-generation","text":"These Matlab functions allows you to test noise functions to generate artificial terrains using noise function executables.Source codes for the c++ noise functions files are included in the adjoining folder and were provided kindly by Travis Archer for open use.The codes create a bmp output which are then modified to create binned responses and landscape classes as needed. Noise types include * Cell Noise * Diamond Square * Erosion * Midpoint Displacement * Perline Noise * Simplex Noise * Value Noise","title":"Noise Function Terrain Generation"},{"location":"projects/synthetic_models/#random-matrix-algorithms","text":"This uses the random matrix functions within matlab to extract different landscape types. Once again they are exported as ASCII. filename Description randi_land.m Uniformly distributed pseudorandom integers sprand_land.m Sparse uniformly distributed random matrix rand_land.m Uniformly distributed random numbers rng_seed_land Random Seed Generation with Random Number Generator","title":"Random matrix algorithms"},{"location":"projects/synthetic_models/#random-matrix-to-video","text":"These are experiments within matlab to use the landscapes generated as frames in a video output file from the landscapes. The video output is generally in avi format and includes slices of the landscapes produced.","title":"Random matrix to video"},{"location":"projects/synthetic_models/#credits","text":"I would like to thank Dr. Scott Robeson my mentor among others who got me interested in some topics on Landscape ecology.","title":"Credits"}]}