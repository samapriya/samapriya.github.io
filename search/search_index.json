{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction These github projects are a collection of tools and improvements made to accomplish tasks as well as potential tools and applications designed for future applications. While moving between assets from Planet Labs and Google Earth Engine it was imperative to create a pipeline that allows for easy transitions between the two service end points and hence tools were designed to link these two nodes and create effective pipelines. Since this required me to interact with Earth Engine I further developed addon tools the ambition of which is to helping users with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. Projects Include work on following Platforms Two projects included looking at different aspects of raster analysis and landscape modeling and hence I decided to create an ArcMap toolbox with tools that I have developed and continue to develop further. The toolbox can be downloaded and added to existing toolbox to create additional functionalities. The synthetic landscape models are designed to try different randomizing and clustering algorithms. An additional project was completed during the Polar Geospatial Bootcamp which allows users to bulk download and process 2m high resolution ArcticDEM provided by the Polar Geospatial Center. The list now also includes a notifier application using the Slack interface along with including an application of the Clips API from Planet. Project Name Digital Object Identification Number(DOI) Google Earth Engine Takeout Planet-Batch-Slack-Pipeline Clip-Ship-Planet-CLI Slack-Notifier-CLI-Addon Planet-GEE-Pipeline-CLI Planet-GEE-Pipeline-GUI Planet-Pipeline-GUI GEE Asset Manager Addons Synthetic LandScape Models ArcMap Addons ArcticDEM-Batch-Pipeline jetstream-unofficial-addon","title":"Introduction"},{"location":"#introduction","text":"These github projects are a collection of tools and improvements made to accomplish tasks as well as potential tools and applications designed for future applications. While moving between assets from Planet Labs and Google Earth Engine it was imperative to create a pipeline that allows for easy transitions between the two service end points and hence tools were designed to link these two nodes and create effective pipelines. Since this required me to interact with Earth Engine I further developed addon tools the ambition of which is to helping users with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. Projects Include work on following Platforms Two projects included looking at different aspects of raster analysis and landscape modeling and hence I decided to create an ArcMap toolbox with tools that I have developed and continue to develop further. The toolbox can be downloaded and added to existing toolbox to create additional functionalities. The synthetic landscape models are designed to try different randomizing and clustering algorithms. An additional project was completed during the Polar Geospatial Bootcamp which allows users to bulk download and process 2m high resolution ArcticDEM provided by the Polar Geospatial Center. The list now also includes a notifier application using the Slack interface along with including an application of the Clips API from Planet. Project Name Digital Object Identification Number(DOI) Google Earth Engine Takeout Planet-Batch-Slack-Pipeline Clip-Ship-Planet-CLI Slack-Notifier-CLI-Addon Planet-GEE-Pipeline-CLI Planet-GEE-Pipeline-GUI Planet-Pipeline-GUI GEE Asset Manager Addons Synthetic LandScape Models ArcMap Addons ArcticDEM-Batch-Pipeline jetstream-unofficial-addon","title":"Introduction"},{"location":"aboutme/","text":"My current work involves big data analysis and large scale data science applications utilizing geospatial technology. I work with different imagery types and I am currently involved with looking at time series data analysis of natural systems coupled with system modeling and periodicity. Most of my work includes but is not limited to remote sensing applications, large scale data processing and management, API support along with network analysis and geostatistical methods. Contact me: roysam[at]indiana.edu","title":"About me"},{"location":"credits/","text":"Credits JetStream A portion of the work is suported by JetStream Grant TG-GEO160014. ArcticDEM data provided by Polar Geospatial Center PGC Also supported by Planet Labs Ambassador Program","title":"Credits"},{"location":"credits/#credits","text":"JetStream A portion of the work is suported by JetStream Grant TG-GEO160014. ArcticDEM data provided by Polar Geospatial Center PGC Also supported by Planet Labs Ambassador Program","title":"Credits"},{"location":"customization/","text":"Customization A great starting point Project documentation is as diverse as the projects themselves and the Material theme is a good starting point for making it look great. However, as you write your documentation, you may reach a point where some small adjustments are necessary to preserve the desired style. Adding assets MkDocs provides several ways to interfere with themes. In order to make a few tweaks to an existing theme, you can just add your stylesheets and JavaScript files to the docs directory. Additional stylesheets If you want to tweak some colors or change the spacing of certain elements, you can do this in a separate stylesheet. The easiest way is by creating a new stylesheet file in your docs directory: mkdir docs/stylesheets touch docs/stylesheets/extra.css Then, add the following line to your mkdocs.yml : extra_css: - 'stylesheets/extra.css' Spin up the development server with mkdocs serve and start typing your changes in your additional stylesheet file \u2013 you can see them instantly after saving, as the MkDocs development server implements live reloading. Additional JavaScript The same is true for additional JavaScript. If you want to integrate another syntax highlighter or add some custom logic to your theme, create a new JavaScript file in your docs directory: mkdir docs/javascripts touch docs/javascripts/extra.js Then, add the following line to your mkdocs.yml : extra_javascript: - 'javascripts/extra.js' Further assistance can be found in the MkDocs documentation . Extending the theme If you want to alter the HTML source (e.g. add or remove some part), you can extend the theme. From version 0.16 on MkDocs implements theme extension , an easy way to override parts of a theme without forking and changing the main theme. Setup and theme structure Reference the Material theme as usual in your mkdocs.yml , and create a new folder for overrides, e.g. theme , which you reference using theme_dir : theme: 'material' theme_dir: 'theme' !!! warning \"Theme extension prerequisites\" As the `theme_dir` variable is used for the theme extension process, the Material theme needs to be installed via `pip` and referenced with the `theme` parameter in your `mkdocs.yml`. The structure in the theme directory must mirror the directory structure of the original theme, as any file in the theme directory will replace the file with the same name which is part of the original theme. Besides, further assets may also be put in the theme directory. The directory layout of the Material theme is as follows: . \u251c\u2500 assets/ \u2502 \u251c\u2500 images/ # Images and icons \u2502 \u251c\u2500 javascripts/ # JavaScript \u2502 \u2514\u2500 stylesheets/ # Stylesheets \u251c\u2500 partials/ \u2502 \u251c\u2500 disqus.html # Disqus integration \u2502 \u251c\u2500 footer.html # Footer bar \u2502 \u251c\u2500 header.html # Header bar \u2502 \u251c\u2500 language.html # Localized labels \u2502 \u251c\u2500 nav-item.html # Main navigation item \u2502 \u251c\u2500 nav.html # Main navigation \u2502 \u251c\u2500 search.html # Search box \u2502 \u251c\u2500 social.html # Social links \u2502 \u251c\u2500 source.html # Repository information \u2502 \u251c\u2500 tabs-item.html # Tabs navigation item \u2502 \u251c\u2500 tabs.html # Tabs navigation \u2502 \u251c\u2500 toc-item.html # Table of contents item \u2502 \u2514\u2500 toc.html # Table of contents \u251c\u2500 404.html # 404 error page \u251c\u2500 base.html # Base template \u2514\u2500 main.html # Default page Overriding partials In order to override the footer, we can replace the footer.html partial with our own partial. To do this, create the file partials/footer.html in the theme directory. MkDocs will now use the new partial when rendering the theme. This can be done with any file. Overriding template blocks Besides overriding partials, one can also override so called template blocks, which are defined inside the Material theme and wrap specific features. To override a template block, create a main.html inside the theme directory and define the block, e.g.: {% extends base.html %} {% block htmltitle %} title Lorem ipsum dolor sit amet /title {% endblock %} The Material theme provides the following template blocks: Block name Wrapped contents analytics Wraps the Google Analytics integration content Wraps the main content disqus Wraps the disqus integration extrahead Empty block to define additional meta tags fonts Wraps the webfont definitions footer Wraps the footer with navigation and copyright header Wraps the fixed header bar htmltitle Wraps the title tag libs Wraps the JavaScript libraries, e.g. Modernizr repo Wraps the repository link in the header bar scripts Wraps the JavaScript application logic source Wraps the linked source files search_box Wraps the search form in the header bar site_meta Wraps the meta tags in the document head site_name Wraps the site name in the header bar site_nav Wraps the site navigation and table of contents social Wraps the social links in the footer styles Wraps the stylesheets (also extra sources) For more on this topic refer to the MkDocs documentation Theme development The Material theme is built on modern technologies like ES6, Webpack , Babel and SASS . If you want to make more fundamental changes, it may be necessary to make the adjustments directly in the source of the Material theme and recompile it. This is fairly easy. Environment setup In order to start development on the Material theme, a Node.js version of at least 5 is required, as well as the package manager yarn which is a better version of npm . First, clone the repository: git clone https://github.com/squidfunk/mkdocs-material Next, all dependencies need to be installed, which is done with: cd mkdocs-material pip install -r requirements.txt yarn install Development mode The Material theme uses a sophisticated asset pipeline using Gulp and Webpack which can be started with the following command: yarn start This will also start the MkDocs development server which will monitor changes on assets, templates and documentation. Point your browser to localhost:8000 and you should see this documentation in front of you. For example, changing the color palette is as simple as changing the $md-color-primary and $md-color-accent variables in src/assets/stylesheets/_config.scss : $md-color-primary: $clr-red-400; $md-color-accent: $clr-teal-a700; !!! warning \"Automatically generated files\" Never make any changes in the `material` directory, as the contents of this directory are automatically generated from the `src` directory and will be overriden when the theme is built. Build process When you've finished making your changes, you can build the theme by invoking: yarn run build This triggers the production-level compilation and minification of all stylesheets and JavaScript sources. When the command exits, the final theme is located in the material directory. Add the theme_dir variable pointing to the aforementioned directory in your original mkdocs.yml . Now you can run mkdocs build and you should see your documentation with your changes to the original Material theme.","title":"Customization"},{"location":"customization/#customization","text":"","title":"Customization"},{"location":"customization/#a-great-starting-point","text":"Project documentation is as diverse as the projects themselves and the Material theme is a good starting point for making it look great. However, as you write your documentation, you may reach a point where some small adjustments are necessary to preserve the desired style.","title":"A great starting point"},{"location":"customization/#adding-assets","text":"MkDocs provides several ways to interfere with themes. In order to make a few tweaks to an existing theme, you can just add your stylesheets and JavaScript files to the docs directory.","title":"Adding assets"},{"location":"customization/#additional-stylesheets","text":"If you want to tweak some colors or change the spacing of certain elements, you can do this in a separate stylesheet. The easiest way is by creating a new stylesheet file in your docs directory: mkdir docs/stylesheets touch docs/stylesheets/extra.css Then, add the following line to your mkdocs.yml : extra_css: - 'stylesheets/extra.css' Spin up the development server with mkdocs serve and start typing your changes in your additional stylesheet file \u2013 you can see them instantly after saving, as the MkDocs development server implements live reloading.","title":"Additional stylesheets"},{"location":"customization/#additional-javascript","text":"The same is true for additional JavaScript. If you want to integrate another syntax highlighter or add some custom logic to your theme, create a new JavaScript file in your docs directory: mkdir docs/javascripts touch docs/javascripts/extra.js Then, add the following line to your mkdocs.yml : extra_javascript: - 'javascripts/extra.js' Further assistance can be found in the MkDocs documentation .","title":"Additional JavaScript"},{"location":"customization/#extending-the-theme","text":"If you want to alter the HTML source (e.g. add or remove some part), you can extend the theme. From version 0.16 on MkDocs implements theme extension , an easy way to override parts of a theme without forking and changing the main theme.","title":"Extending the theme"},{"location":"customization/#setup-and-theme-structure","text":"Reference the Material theme as usual in your mkdocs.yml , and create a new folder for overrides, e.g. theme , which you reference using theme_dir : theme: 'material' theme_dir: 'theme' !!! warning \"Theme extension prerequisites\" As the `theme_dir` variable is used for the theme extension process, the Material theme needs to be installed via `pip` and referenced with the `theme` parameter in your `mkdocs.yml`. The structure in the theme directory must mirror the directory structure of the original theme, as any file in the theme directory will replace the file with the same name which is part of the original theme. Besides, further assets may also be put in the theme directory. The directory layout of the Material theme is as follows: . \u251c\u2500 assets/ \u2502 \u251c\u2500 images/ # Images and icons \u2502 \u251c\u2500 javascripts/ # JavaScript \u2502 \u2514\u2500 stylesheets/ # Stylesheets \u251c\u2500 partials/ \u2502 \u251c\u2500 disqus.html # Disqus integration \u2502 \u251c\u2500 footer.html # Footer bar \u2502 \u251c\u2500 header.html # Header bar \u2502 \u251c\u2500 language.html # Localized labels \u2502 \u251c\u2500 nav-item.html # Main navigation item \u2502 \u251c\u2500 nav.html # Main navigation \u2502 \u251c\u2500 search.html # Search box \u2502 \u251c\u2500 social.html # Social links \u2502 \u251c\u2500 source.html # Repository information \u2502 \u251c\u2500 tabs-item.html # Tabs navigation item \u2502 \u251c\u2500 tabs.html # Tabs navigation \u2502 \u251c\u2500 toc-item.html # Table of contents item \u2502 \u2514\u2500 toc.html # Table of contents \u251c\u2500 404.html # 404 error page \u251c\u2500 base.html # Base template \u2514\u2500 main.html # Default page","title":"Setup and theme structure"},{"location":"customization/#overriding-partials","text":"In order to override the footer, we can replace the footer.html partial with our own partial. To do this, create the file partials/footer.html in the theme directory. MkDocs will now use the new partial when rendering the theme. This can be done with any file.","title":"Overriding partials"},{"location":"customization/#overriding-template-blocks","text":"Besides overriding partials, one can also override so called template blocks, which are defined inside the Material theme and wrap specific features. To override a template block, create a main.html inside the theme directory and define the block, e.g.: {% extends base.html %} {% block htmltitle %} title Lorem ipsum dolor sit amet /title {% endblock %} The Material theme provides the following template blocks: Block name Wrapped contents analytics Wraps the Google Analytics integration content Wraps the main content disqus Wraps the disqus integration extrahead Empty block to define additional meta tags fonts Wraps the webfont definitions footer Wraps the footer with navigation and copyright header Wraps the fixed header bar htmltitle Wraps the title tag libs Wraps the JavaScript libraries, e.g. Modernizr repo Wraps the repository link in the header bar scripts Wraps the JavaScript application logic source Wraps the linked source files search_box Wraps the search form in the header bar site_meta Wraps the meta tags in the document head site_name Wraps the site name in the header bar site_nav Wraps the site navigation and table of contents social Wraps the social links in the footer styles Wraps the stylesheets (also extra sources) For more on this topic refer to the MkDocs documentation","title":"Overriding template blocks"},{"location":"customization/#theme-development","text":"The Material theme is built on modern technologies like ES6, Webpack , Babel and SASS . If you want to make more fundamental changes, it may be necessary to make the adjustments directly in the source of the Material theme and recompile it. This is fairly easy.","title":"Theme development"},{"location":"customization/#environment-setup","text":"In order to start development on the Material theme, a Node.js version of at least 5 is required, as well as the package manager yarn which is a better version of npm . First, clone the repository: git clone https://github.com/squidfunk/mkdocs-material Next, all dependencies need to be installed, which is done with: cd mkdocs-material pip install -r requirements.txt yarn install","title":"Environment setup"},{"location":"customization/#development-mode","text":"The Material theme uses a sophisticated asset pipeline using Gulp and Webpack which can be started with the following command: yarn start This will also start the MkDocs development server which will monitor changes on assets, templates and documentation. Point your browser to localhost:8000 and you should see this documentation in front of you. For example, changing the color palette is as simple as changing the $md-color-primary and $md-color-accent variables in src/assets/stylesheets/_config.scss : $md-color-primary: $clr-red-400; $md-color-accent: $clr-teal-a700; !!! warning \"Automatically generated files\" Never make any changes in the `material` directory, as the contents of this directory are automatically generated from the `src` directory and will be overriden when the theme is built.","title":"Development mode"},{"location":"customization/#build-process","text":"When you've finished making your changes, you can build the theme by invoking: yarn run build This triggers the production-level compilation and minification of all stylesheets and JavaScript sources. When the command exits, the final theme is located in the material directory. Add the theme_dir variable pointing to the aforementioned directory in your original mkdocs.yml . Now you can run mkdocs build and you should see your documentation with your changes to the original Material theme.","title":"Build process"},{"location":"cv/","text":"Samapriya Roy Github Projects: github.com/samapriya Github Site: samapriya.github.io City: Raleigh email: samapriya@gmail.com last updated: 12th August 2018 Research Specialization and Interest Remote Sensing and GIS, Urban systems,patterns and hydrology, Land Change Science Education Degree University Year Research Interest or Thesis PhD Candidate Indiana University Expected 2018 Land cover change modeling and Coastal system dynamics using large scale spatio-temporal data analysis MS Earth Sciences Indiana University 2013 Thesis: Remote sensing GIS applications for drainage detection and Modeling in agricultural watersheds B.Tech VNIT 2010 Thesis: Social Network Analysis for Mapping Segmented Growth in Urban Cities in India. Technical Extensive experience with Google Earth Engine, ESRI ArcGIS, ERDAS IMAGINE, ENVI. Intermediate experience with Javascript, Python, Shell Scripts Experience with QGIS, eCognition, MATLAB, Fragstats, Patch Analyst, SPSS, QUAL2Kw, SocNetV, Expert Choice 2000, GRASS, ILWIS. Employment May 2018- 2019: Senior Developer Advocate Intern, Planet Labs Responsibilites include Growing and supporting Planet\u2019s technical user communities and developing new analytical tools and tutorials. Teaching workshops and delivering conference talks to technologists in academic communities and to developers in the geospatial and cloud industries. Collaborating on remote sensing science, including primary research on the evolution, geomorphology, and long-term welfare of the world\u2019s coastal ecosystems. January 2018- May 2019: Developer Advocate Intern, Planet Labs Responsibilities include growing and supporting user communities for Planet\u2019s Developer Center and the Education and Research Program.Developing new analytical tools, tutorials, and workshops for technical users of Planet data and tools. 2017-2018: Research Assistant, Indiana University funded by National Science Foundation(NSF) Coastal SEES Collaborative Research: Changes in actual and perceived coastal flood risks due to river management strategies (NSF: 1426997). Partner-PI. National Science Foundation. Responsible for looking at land loss models and remote sensing application to coastal land loss. Includes model building and assessment along with hydrological model based vulnerability assessment of same area looking at landscape pattern and progress. 2015-2016: Research Assistant, Indiana University funded by Deltas Belmont Forum and National Science Foundation(NSF) Catalyzing action towards sustainability of deltaic systems with an integrated modeling framework for risk assessment (DELTAS: 1342946). Partner-PI. National Science Foundation, Belmont Forum. Partner PI [International collaborative network of 24 institutions] (2015-2016) Responsible for looking at physical models of vulnerability and risk assessment in deltaic areas and among populated regions within Brazil. Includes model building and assessment along with hydrological model based vulnerability assessment of same area. 2013-2014: Reseach Assistant , Indiana University funded by National Aeronautics and Space Administration(NASA) National Aeronautics and Space Administration(NASA: NNX11AF50G) (01/01/2013\u201312/31/2014) Carbon Dynamics, Land Cover Change, and Vulnerability of the World's Largest Coastal Mangrove Ecosystem to Climate Change, with Dr. Rinku Roy Chowdhury. Responsibilities includes review of existing datasets and literature. Evaluation of existing LULC classes using Landsat ETM+, and Geoeye classified raster datasets. Assessment of existing census and socio ecological data. 2013-2015: Research Assistant, Indiana University funded by National Science Foundation(NSF) National Science Foundation (NSF: 1065785) (07/01/2013\u201306/30/2015). Collaborative Research: Ecological Homogenization of Urban America, with Dr. Rinku Roy Chowdhury Responsibilities include analysis of land cover data at census block group and parcel levels using landscape metrics and analysis of land cover heterogeneity or homogeneity at both levels. This includes developing batch models for running large datasets and creating consistent data and analytical output for all 6 urban sites in the project. 2011-2012: Research Assistant, Indiana University Purdue University at Indinapolis funded by United States Department of Agriculture Natural Resource Conservation(USDA NRCS) United States Department of Agriculture Natural Resource Conservation (USDA NRCS: 68-52KY-11-058) project for Creating and Automating Potential Polygon location and Site Characterization for Upland Storage in Watersheds, using Arc Map. A CTI (Compound Topographic Index) based Toolbox was developed using Model Builder (\u00ae ESRI) as a part of project deliverables to automate location of these potential sites. (September 2011- August 2012) January 2011-June 2011: Research Assistant, Indian Institute of Technology (IIT) funder by Ministry of Environment and Forests Research Associate with Water Resource Management Group at IIT Kanpur, for the Ganga River Basin Management Plan (GRBMP), under Ministry of Environment and Forests Responsibilities: Included assessment of the hydrology of the Upper Ganga Basin and working on flow simulation and scenario development for flow calculation. Organization: Indian Institute of Technology (IIT), Kanpur, January to June 2011. June 2010- August 2010: Research Intern at Risk Management Solution India (RMSI) Research Intern at the Core Geospatial and Utilities(CGO) business Unit Responsibilities at Risk Management Solution in India (RMSI), Dehradun: Created a characteristic model called SMCPM (Scenario Modeling with Catchment Priority Model) which utilizes the SCN method accompanied with SMART and MAUT utilization for user based criterion input and priority output. 22nd June to 21st August 2010 \\newpage Peer-Reviewed Journal Articles Roy, S., L. Yoder, V.M. Dias, E. Brondizio. Spatial Clustering using Multiplex Geo-constrained Networks in Amazon River Delta In Preparation Roy, S., D. A. Edmonds, S. Robeson, A. C. Ortiz. \u201cDecadal Changes in Mississippi Delta Morphology: Analyzing Landscape Patterns using Satellite Time Series Data\u201d In preparation Ortiz, A. C., S. Roy, and D. A. Edmonds (2017), Land loss by pond expansion on the Mississippi River Delta Plain, Geophys. Res. Lett., 44, doi:10.1002/2017GL073079. link Mansur, A. V., Brondizio, E. S., Roy, S., Soares, P. P. D. M. A., Newton, A. (2018). Adapting to urban challenges in the Amazon: flood risk and infrastructure deficiencies in Bel\u00e9m, Brazil. Regional Environmental Change, 18(5), 1411-1426. link Mansur, A. V., Brond\u00edzio, E. S., Roy, S., Hetrick, S., Vogt, N. D., Newton, A. (2016). An assessment of urban vulnerability in the Amazon Delta and Estuary: a multi-criterion index of flood exposure, socio-economic conditions and infrastructure. Sustainability Science, 11(4), 625-643. link Roy, S., Katpatal, Y. B. (2011). Cyclical Hierarchical Modeling for Water Quality Model\u2013Based DSS Module in an Urban River System. Journal of Environmental Engineering, 137(12), 1176-1184. link Open Source Tools Products Samapriya Roy. (2018, August 12). samapriya/geeup: geeup: Simple CLI for Earth Engine Uploads (Version 0.0.3). Zenodo. http://doi.org/10.5281/zenodo.1344130 Samapriya Roy. (2018, July 31). samapriya/gee2drive: gee2drive: Google Earth Engine to Drive Export Manager (Version 0.0.5). Zenodo. http://doi.org/10.5281/zenodo.132447 Samapriya Roy. (2018, July 23). samapriya/pydrop: pydrop: Minimal Python Client for Digital Ocean Droplets (Version 0.0.2). Zenodo. http://doi.org/10.5281/zenodo.1319799 Google Earth Engine Account Transfer Tool,Samapriya Roy. (2018, February 27). samapriya/gee-takeout: Google Earth Engine Account Transfer Tool (Version 0.1). Zenodo. http://doi.org/10.5281/zenodo.1185158 Clip-Ship-Planet-CLI,Samapriya Roy. (2017, December 20). samapriya/Clip-Ship-Planet-CLI: Clip-Ship-Batch Planet Command Line Interface (Version 0.2.1). Zenodo. http://doi.org/10.5281/zenodo.1119192 Slack-Notifier-CLI-Addon,Samapriya Roy. (2017, September 5). samapriya/Slack-Notifier-CLI-Addon: Slack Notifier-CLI Addon (Version 0.1.0). Zenodo. http://doi.org/10.5281/zenodo.885505 Planet-GEE-Pipeline-CLI,Samapriya Roy. (2018, March 8). samapriya/Planet-GEE-Pipeline-CLI: Planet-GEE-Pipeline-CLI (Version 0.2.1). Zenodo. http://doi.org/10.5281/zenodo.1194323 Planet-GEE-Pipeline-GUI,Samapriya Roy. (2017, June 25). samapriya/Planet-GEE-Pipeline-GUI: Planet-GEE-Pipeline-GUI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.817739 Planet-Pipeline-GUI,Samapriya Roy. (2017, August 17). samapriya/Planet-Pipeline-GUI: Planet-Pipeline-GUI (Version 0.3). Zenodo. http://doi.org/10.5281/zenodo.844149 GEE Asset Manager Addons,Samapriya Roy. (2018, March 8). samapriya/gee_asset_manager_addon: GEE Asset Manager with Addons (Version 0.2.3). Zenodo. http://doi.org/10.5281/zenodo.1194308 ArcMap Addons,Samapriya Roy. (2017, October 12). samapriya/arcmap-addons: ArcMap Addons (Version 0.2). Zenodo. http://doi.org/10.5281/zenodo.1009210 ArcticDEM-Batch-Pipeline,Samapriya Roy. (2018, May 3). samapriya/ArcticDEM-Batch-Pipeline: ArcticDEM-Batch-Pipeline (Version 0.1.2). Zenodo. http://doi.org/10.5281/zenodo.1240456 Jetstream-Unofficial-addon,Samapriya Roy. (2018, March 12). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon (Version 0.1.2). Zenodo. http://doi.org/10.5281/zenodo.1196653 Planet-Batch-Slack-CLI,Samapriya Roy. (2017, December 4). samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.1079887 Articles, Blogposts and Publications Roy, Samapriya, \"Google Earth Engine Takeout: Tools and Guide for Code and Asset Transfer\" , 4th December 2017, Read Here Roy, Samapriya, \"Talk Slack to Me: Integrating Planet and Slack API for Automation Batch Notifications\" , 4th December 2017, Read Here Roy, Samapriya, \"Baking API Clients in a Raspberry Pi: Planet and Earth Engine in a Box\" , 22nd November 2017, Read Here Roy, Samapriya, \"Planet, People and Pixels: A Data Pipeline to link Planet API to Google Earth Engine\" , 10 July 2017, Read Here Roy, Samapriya, \"Clip and Ship: Batch Clips using Planet\u2019s Clips API\" , 15 September 2017, Read Here Roy, Samapriya, \"Google Earth Engine Asset Manager and Addons: Building Tools of the Trade\" , 19 October 2017, Read Here Invited Talks and Trainings SatSummit September 19-20 2018: Hands-on Satellite Imagery Processing and Analysis at Scale Terra 2018 Pointcloud And Remote Sensing Workshop: Invited talk about Planet data, API mechanics and working with Planet Data inside Google Earth Engine . This was an online workshop along with Joseph Mascaro held at Ensenada, B.C. Github Link NEON Data Institute 2018: Remote Sensing with Reproducible Workflows using Python: Invited talk about Planet data, API mechanics and working with Google Earth Engine . This was an online talk as I joined remotely with participants. Github Link Stanford Big Earth Hackathon April 14-15th 2018, Stanford University: Invited to coordinate use of Planet data and API mechanics to formulate and work on Earth Sciences and big data problems with students as Developer advocate intern. Github Link CSDMS 2018 Annual Meeting, May 22 -24th 2018, Boulder Colorado, USA: Organized Clinic Introduction to Google Earth Engine Github Link American Geophysical Union Fall Meeting 2017: Invited Talk: Earth Science in Real Time with the Planet SmallSat Constellation. December 11-15th 2017 Conference presentations, Trainings and Attendance Cyverse Container Camp 7th to 9th March: Container Technology for Scientific Research Introduction to Docker and Singularity images in High Performance Computing environments. American Geophysical Union Annual Meeting 2017(AGU 2017) Spatial and Temporal Patterns of Land Loss in Mississipi River Delta. December 11-15th 2017. New Orleans, Louisiana Polar Geospatial BootCamp 2017: Focus on looking at high resolution satellite data along with using stereoscopic method to create high resolution digital elevation model. August 7-10th 2017, Minneapolis, Minnesota Google Earth Engine Summit 2017: Focus on methodology and large scale data analysis. Mountain View, California. June 12-14th 2017. Indiana Geographic Information Council Conference 2017. Deep Time Stack Analysis of Coastal Land loss: Case Study of Mississippi Delta using Earth Engine. May 9-11th May 2017 Edmonds,Douglas A., Caldwell,Rebecca L., Baumgardner,Sarah, Paola,Chris, Roy ,Samapriya, Nelson,Amelia: A global analysis of human habitation on river deltas. European Geosciences Union General Assembly , Vienna, 23-28th April 2017 Roy, Samapriya., Edmonds, Douglas., Visualizing land loss across high spatio-temporal scales across Louisiana Coast, Planet Labs Headquarters December 2016. Deltas Belmont Forum End of Project Meeting August 2016: Large Scale data analysis and visualization application to specific delta cases along with urban land cover change. Google Earth Engine Summit 2016: Focus on methodology and large scale data analysis. Mountain View, California. June 13-16th 2016. Community Surface Dynamics Modeling System (CSDMS) Annual Meeting , A Composite Vulnerability for Urban Areas in Deltaic Regions: An application in the Amazon Delta, Colorado. 17-19th May 2016. Indiana Geographic Information Council Conference 2016. Locally Globally Applied Classification Algorithms for Urban Land Cover Detection using Earth Engine. May 9-11th May 2016. Summer Training at the United States Army Core of Engineers (USACE) and South Florida Water Management District (SFWMD) joint Modeling Group (2015): South Florida Water Management Model, West Palm Beach, 26th July to 31st July 2015. Roy, Samapriya (2015) Hydrological Modeling for studying impacts of Urbanization: A cross site assessment at Community Surface Dynamics Modeling System (CSDMS) Annual Meeting , Colorado. 26-28nd May 2015 Roy, Samapriya et.al (2015) Urban Watersheds and urbanizing hydrology: Assessment through dynamic modeling, Oral presentation at the Association of American Geographers(AAG) Annual Meeting , April 21-April 25th, 2015 Roy, Samapriya, Roy Chowdhury, Rinku and Ficklin, Darren (2015) Dynamic Hydrological Modeling using SWAT within Florida Coastal Everglades. Florida Coastal Everglades LTER Mid Term Review Meeting , March 11-13, 2015. Roy, Samapriya, Darren L. Ficklin, Rinku Roy Chowdhury, Scott Robeson, Jarlath O Neil Dunn, James B Heffernan, Meredith K Steele, Peter M Groffman (2014) Dynamic Assessment of Urban Hydrologic Component using SWAT. International Long Term Ecological Research (ILTER): All Scientist Meeting of the Americas , Valdivia, Chile, December 1-3 , 2014 Field Research Experience Summer 2014 (05/15/2014-06/18/2014). Field research in Bangladesh: Conducted 140 household surveys, with data collection aimed at assessing socio-economic-political drivers of land use and livelihood vulnerability in 4 villages of the Sundarbans mangrove forest region. Grants and Fellowship College of Arts and Sciences Graduate Student Travel Award for attending Google Earth Engine Summit 2017 $200 April 2017. Awarded the Graduate Professional Student Government Travel Award for attending Google Earth Engine Summit 2017 $500 April 2017. Awarded the John Odland Graduate Research Fellowship to support graduate research $500 March 2017 Awarded the Lester Spicer Department of Geography Poster Award Fellowship to support graduate research $250 March 2017 Awarded the William R. Black Leadership Memorial Fellowship for $500 March 2017 Co-PI on Extreme Science and Engineering Discovery Environment (XSEDE) Allocation Grant along with Douglas Edmonds for 50,000 Super-computing Units and 500 GB storage volume for project \"Automated API based Pipeline for high volume satellite data\", grant number TG-GEO160014 at Indiana University. Co PI on the Planet Labs Ambassador program providing us unprecedented daily satellite data for entire Louisiana coast an estimated value of over 40,000 square kilometer and estimated access to over 500,000 square kilometer daily (approximately) maximum access value considering RapidEye is (approx. 500,000 x $1.28/sqkm = $640,000) Digital Globe Imagery Grant for an area of over 1500 square kilometer (estimated value at $25,500) April 2016. Awarded the John Odland Graduate Research Fellowship to support graduate research $500 March 2016. Awarded the IndianaView Student Scholarship Program to participate and conduct research $750 March 2016 Awarded the Partners of America (POA) 100,000 Strong award, for $1000 towards travel to the ILTER All Scientist Meeting of the Americas, Valdivia, Chile December 2014 Awarded the ILTER Travel Award, for $500 towards travel to the ILTER All Scientist Meeting of the Americas, Valdivia, Chile December 2014 Teaching Experience Lead Instructor, G338: Introduction to Geographic Information Systems, Summer 2017, Indiana University, Bloomington Lead Instructor, G237: Mapping our World, Spring 2017, Indiana University, Bloomington Lead Instructor, G237: Mapping our World, Fall 2016, Indiana University, Bloomington Teaching Assistant, G237: Mapping our World, Spring 2016, Indiana University, Bloomington Lead Instructor, G237: Mapping our World, Fall 2015, Indiana University, Bloomington Guest Lecture I202 Lecture Topic: Spatial Epidemiology September 25th 2014 at Indiana University, Bloomington Teaching Assistant: Environmental Geology Course, G117 Spring 2013, Indiana University Purdue University, Indianapolis Teaching Assistant: Environmental Geology Course, G117 Fall 2012, Indiana University Purdue University, Indianapolis Teaching Assistant: Environmental Geology Course, G117 Spring 2012, Indiana University Purdue University, Indianapolis Memberships and committees Co-Chair for GIS Day Day at Bloomington, Indiana University, 2016 College Committee on Graduate Education, Graduate Student Representative (2015-2016) Planning Committee for GIS Day at Bloomington, Indiana University 2015. Student Member, American Society of Civil Engineers (ASCE) 2008-present Environmental and Water Resources Institute (EWRI) 2010-present Student Member, American Association of Geographer (AAG) 2014-present Certifications and Trainings Collaborative Institutional Training Initiative (CITI) Human Research 2014 Trimble Geospatial Training: eCognition- analysis strategies August 14th- 15th, 2014 References Available on request.","title":"Current CV"},{"location":"cv/#research-specialization-and-interest","text":"Remote Sensing and GIS, Urban systems,patterns and hydrology, Land Change Science","title":"Research Specialization and Interest"},{"location":"cv/#education","text":"Degree University Year Research Interest or Thesis PhD Candidate Indiana University Expected 2018 Land cover change modeling and Coastal system dynamics using large scale spatio-temporal data analysis MS Earth Sciences Indiana University 2013 Thesis: Remote sensing GIS applications for drainage detection and Modeling in agricultural watersheds B.Tech VNIT 2010 Thesis: Social Network Analysis for Mapping Segmented Growth in Urban Cities in India.","title":"Education"},{"location":"cv/#technical","text":"Extensive experience with Google Earth Engine, ESRI ArcGIS, ERDAS IMAGINE, ENVI. Intermediate experience with Javascript, Python, Shell Scripts Experience with QGIS, eCognition, MATLAB, Fragstats, Patch Analyst, SPSS, QUAL2Kw, SocNetV, Expert Choice 2000, GRASS, ILWIS.","title":"Technical"},{"location":"cv/#employment","text":"May 2018- 2019: Senior Developer Advocate Intern, Planet Labs Responsibilites include Growing and supporting Planet\u2019s technical user communities and developing new analytical tools and tutorials. Teaching workshops and delivering conference talks to technologists in academic communities and to developers in the geospatial and cloud industries. Collaborating on remote sensing science, including primary research on the evolution, geomorphology, and long-term welfare of the world\u2019s coastal ecosystems. January 2018- May 2019: Developer Advocate Intern, Planet Labs Responsibilities include growing and supporting user communities for Planet\u2019s Developer Center and the Education and Research Program.Developing new analytical tools, tutorials, and workshops for technical users of Planet data and tools. 2017-2018: Research Assistant, Indiana University funded by National Science Foundation(NSF) Coastal SEES Collaborative Research: Changes in actual and perceived coastal flood risks due to river management strategies (NSF: 1426997). Partner-PI. National Science Foundation. Responsible for looking at land loss models and remote sensing application to coastal land loss. Includes model building and assessment along with hydrological model based vulnerability assessment of same area looking at landscape pattern and progress. 2015-2016: Research Assistant, Indiana University funded by Deltas Belmont Forum and National Science Foundation(NSF) Catalyzing action towards sustainability of deltaic systems with an integrated modeling framework for risk assessment (DELTAS: 1342946). Partner-PI. National Science Foundation, Belmont Forum. Partner PI [International collaborative network of 24 institutions] (2015-2016) Responsible for looking at physical models of vulnerability and risk assessment in deltaic areas and among populated regions within Brazil. Includes model building and assessment along with hydrological model based vulnerability assessment of same area. 2013-2014: Reseach Assistant , Indiana University funded by National Aeronautics and Space Administration(NASA) National Aeronautics and Space Administration(NASA: NNX11AF50G) (01/01/2013\u201312/31/2014) Carbon Dynamics, Land Cover Change, and Vulnerability of the World's Largest Coastal Mangrove Ecosystem to Climate Change, with Dr. Rinku Roy Chowdhury. Responsibilities includes review of existing datasets and literature. Evaluation of existing LULC classes using Landsat ETM+, and Geoeye classified raster datasets. Assessment of existing census and socio ecological data. 2013-2015: Research Assistant, Indiana University funded by National Science Foundation(NSF) National Science Foundation (NSF: 1065785) (07/01/2013\u201306/30/2015). Collaborative Research: Ecological Homogenization of Urban America, with Dr. Rinku Roy Chowdhury Responsibilities include analysis of land cover data at census block group and parcel levels using landscape metrics and analysis of land cover heterogeneity or homogeneity at both levels. This includes developing batch models for running large datasets and creating consistent data and analytical output for all 6 urban sites in the project. 2011-2012: Research Assistant, Indiana University Purdue University at Indinapolis funded by United States Department of Agriculture Natural Resource Conservation(USDA NRCS) United States Department of Agriculture Natural Resource Conservation (USDA NRCS: 68-52KY-11-058) project for Creating and Automating Potential Polygon location and Site Characterization for Upland Storage in Watersheds, using Arc Map. A CTI (Compound Topographic Index) based Toolbox was developed using Model Builder (\u00ae ESRI) as a part of project deliverables to automate location of these potential sites. (September 2011- August 2012) January 2011-June 2011: Research Assistant, Indian Institute of Technology (IIT) funder by Ministry of Environment and Forests Research Associate with Water Resource Management Group at IIT Kanpur, for the Ganga River Basin Management Plan (GRBMP), under Ministry of Environment and Forests Responsibilities: Included assessment of the hydrology of the Upper Ganga Basin and working on flow simulation and scenario development for flow calculation. Organization: Indian Institute of Technology (IIT), Kanpur, January to June 2011. June 2010- August 2010: Research Intern at Risk Management Solution India (RMSI) Research Intern at the Core Geospatial and Utilities(CGO) business Unit Responsibilities at Risk Management Solution in India (RMSI), Dehradun: Created a characteristic model called SMCPM (Scenario Modeling with Catchment Priority Model) which utilizes the SCN method accompanied with SMART and MAUT utilization for user based criterion input and priority output. 22nd June to 21st August 2010 \\newpage","title":"Employment"},{"location":"cv/#peer-reviewed-journal-articles","text":"Roy, S., L. Yoder, V.M. Dias, E. Brondizio. Spatial Clustering using Multiplex Geo-constrained Networks in Amazon River Delta In Preparation Roy, S., D. A. Edmonds, S. Robeson, A. C. Ortiz. \u201cDecadal Changes in Mississippi Delta Morphology: Analyzing Landscape Patterns using Satellite Time Series Data\u201d In preparation Ortiz, A. C., S. Roy, and D. A. Edmonds (2017), Land loss by pond expansion on the Mississippi River Delta Plain, Geophys. Res. Lett., 44, doi:10.1002/2017GL073079. link Mansur, A. V., Brondizio, E. S., Roy, S., Soares, P. P. D. M. A., Newton, A. (2018). Adapting to urban challenges in the Amazon: flood risk and infrastructure deficiencies in Bel\u00e9m, Brazil. Regional Environmental Change, 18(5), 1411-1426. link Mansur, A. V., Brond\u00edzio, E. S., Roy, S., Hetrick, S., Vogt, N. D., Newton, A. (2016). An assessment of urban vulnerability in the Amazon Delta and Estuary: a multi-criterion index of flood exposure, socio-economic conditions and infrastructure. Sustainability Science, 11(4), 625-643. link Roy, S., Katpatal, Y. B. (2011). Cyclical Hierarchical Modeling for Water Quality Model\u2013Based DSS Module in an Urban River System. Journal of Environmental Engineering, 137(12), 1176-1184. link","title":"Peer-Reviewed Journal Articles"},{"location":"cv/#open-source-tools-products","text":"Samapriya Roy. (2018, August 12). samapriya/geeup: geeup: Simple CLI for Earth Engine Uploads (Version 0.0.3). Zenodo. http://doi.org/10.5281/zenodo.1344130 Samapriya Roy. (2018, July 31). samapriya/gee2drive: gee2drive: Google Earth Engine to Drive Export Manager (Version 0.0.5). Zenodo. http://doi.org/10.5281/zenodo.132447 Samapriya Roy. (2018, July 23). samapriya/pydrop: pydrop: Minimal Python Client for Digital Ocean Droplets (Version 0.0.2). Zenodo. http://doi.org/10.5281/zenodo.1319799 Google Earth Engine Account Transfer Tool,Samapriya Roy. (2018, February 27). samapriya/gee-takeout: Google Earth Engine Account Transfer Tool (Version 0.1). Zenodo. http://doi.org/10.5281/zenodo.1185158 Clip-Ship-Planet-CLI,Samapriya Roy. (2017, December 20). samapriya/Clip-Ship-Planet-CLI: Clip-Ship-Batch Planet Command Line Interface (Version 0.2.1). Zenodo. http://doi.org/10.5281/zenodo.1119192 Slack-Notifier-CLI-Addon,Samapriya Roy. (2017, September 5). samapriya/Slack-Notifier-CLI-Addon: Slack Notifier-CLI Addon (Version 0.1.0). Zenodo. http://doi.org/10.5281/zenodo.885505 Planet-GEE-Pipeline-CLI,Samapriya Roy. (2018, March 8). samapriya/Planet-GEE-Pipeline-CLI: Planet-GEE-Pipeline-CLI (Version 0.2.1). Zenodo. http://doi.org/10.5281/zenodo.1194323 Planet-GEE-Pipeline-GUI,Samapriya Roy. (2017, June 25). samapriya/Planet-GEE-Pipeline-GUI: Planet-GEE-Pipeline-GUI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.817739 Planet-Pipeline-GUI,Samapriya Roy. (2017, August 17). samapriya/Planet-Pipeline-GUI: Planet-Pipeline-GUI (Version 0.3). Zenodo. http://doi.org/10.5281/zenodo.844149 GEE Asset Manager Addons,Samapriya Roy. (2018, March 8). samapriya/gee_asset_manager_addon: GEE Asset Manager with Addons (Version 0.2.3). Zenodo. http://doi.org/10.5281/zenodo.1194308 ArcMap Addons,Samapriya Roy. (2017, October 12). samapriya/arcmap-addons: ArcMap Addons (Version 0.2). Zenodo. http://doi.org/10.5281/zenodo.1009210 ArcticDEM-Batch-Pipeline,Samapriya Roy. (2018, May 3). samapriya/ArcticDEM-Batch-Pipeline: ArcticDEM-Batch-Pipeline (Version 0.1.2). Zenodo. http://doi.org/10.5281/zenodo.1240456 Jetstream-Unofficial-addon,Samapriya Roy. (2018, March 12). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon (Version 0.1.2). Zenodo. http://doi.org/10.5281/zenodo.1196653 Planet-Batch-Slack-CLI,Samapriya Roy. (2017, December 4). samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.1079887","title":"Open Source Tools &amp; Products"},{"location":"cv/#articles-blogposts-and-publications","text":"Roy, Samapriya, \"Google Earth Engine Takeout: Tools and Guide for Code and Asset Transfer\" , 4th December 2017, Read Here Roy, Samapriya, \"Talk Slack to Me: Integrating Planet and Slack API for Automation Batch Notifications\" , 4th December 2017, Read Here Roy, Samapriya, \"Baking API Clients in a Raspberry Pi: Planet and Earth Engine in a Box\" , 22nd November 2017, Read Here Roy, Samapriya, \"Planet, People and Pixels: A Data Pipeline to link Planet API to Google Earth Engine\" , 10 July 2017, Read Here Roy, Samapriya, \"Clip and Ship: Batch Clips using Planet\u2019s Clips API\" , 15 September 2017, Read Here Roy, Samapriya, \"Google Earth Engine Asset Manager and Addons: Building Tools of the Trade\" , 19 October 2017, Read Here","title":"Articles, Blogposts and Publications"},{"location":"cv/#invited-talks-and-trainings","text":"SatSummit September 19-20 2018: Hands-on Satellite Imagery Processing and Analysis at Scale Terra 2018 Pointcloud And Remote Sensing Workshop: Invited talk about Planet data, API mechanics and working with Planet Data inside Google Earth Engine . This was an online workshop along with Joseph Mascaro held at Ensenada, B.C. Github Link NEON Data Institute 2018: Remote Sensing with Reproducible Workflows using Python: Invited talk about Planet data, API mechanics and working with Google Earth Engine . This was an online talk as I joined remotely with participants. Github Link Stanford Big Earth Hackathon April 14-15th 2018, Stanford University: Invited to coordinate use of Planet data and API mechanics to formulate and work on Earth Sciences and big data problems with students as Developer advocate intern. Github Link CSDMS 2018 Annual Meeting, May 22 -24th 2018, Boulder Colorado, USA: Organized Clinic Introduction to Google Earth Engine Github Link American Geophysical Union Fall Meeting 2017: Invited Talk: Earth Science in Real Time with the Planet SmallSat Constellation. December 11-15th 2017","title":"Invited Talks and Trainings"},{"location":"cv/#conference-presentations-trainings-and-attendance","text":"Cyverse Container Camp 7th to 9th March: Container Technology for Scientific Research Introduction to Docker and Singularity images in High Performance Computing environments. American Geophysical Union Annual Meeting 2017(AGU 2017) Spatial and Temporal Patterns of Land Loss in Mississipi River Delta. December 11-15th 2017. New Orleans, Louisiana Polar Geospatial BootCamp 2017: Focus on looking at high resolution satellite data along with using stereoscopic method to create high resolution digital elevation model. August 7-10th 2017, Minneapolis, Minnesota Google Earth Engine Summit 2017: Focus on methodology and large scale data analysis. Mountain View, California. June 12-14th 2017. Indiana Geographic Information Council Conference 2017. Deep Time Stack Analysis of Coastal Land loss: Case Study of Mississippi Delta using Earth Engine. May 9-11th May 2017 Edmonds,Douglas A., Caldwell,Rebecca L., Baumgardner,Sarah, Paola,Chris, Roy ,Samapriya, Nelson,Amelia: A global analysis of human habitation on river deltas. European Geosciences Union General Assembly , Vienna, 23-28th April 2017 Roy, Samapriya., Edmonds, Douglas., Visualizing land loss across high spatio-temporal scales across Louisiana Coast, Planet Labs Headquarters December 2016. Deltas Belmont Forum End of Project Meeting August 2016: Large Scale data analysis and visualization application to specific delta cases along with urban land cover change. Google Earth Engine Summit 2016: Focus on methodology and large scale data analysis. Mountain View, California. June 13-16th 2016. Community Surface Dynamics Modeling System (CSDMS) Annual Meeting , A Composite Vulnerability for Urban Areas in Deltaic Regions: An application in the Amazon Delta, Colorado. 17-19th May 2016. Indiana Geographic Information Council Conference 2016. Locally Globally Applied Classification Algorithms for Urban Land Cover Detection using Earth Engine. May 9-11th May 2016. Summer Training at the United States Army Core of Engineers (USACE) and South Florida Water Management District (SFWMD) joint Modeling Group (2015): South Florida Water Management Model, West Palm Beach, 26th July to 31st July 2015. Roy, Samapriya (2015) Hydrological Modeling for studying impacts of Urbanization: A cross site assessment at Community Surface Dynamics Modeling System (CSDMS) Annual Meeting , Colorado. 26-28nd May 2015 Roy, Samapriya et.al (2015) Urban Watersheds and urbanizing hydrology: Assessment through dynamic modeling, Oral presentation at the Association of American Geographers(AAG) Annual Meeting , April 21-April 25th, 2015 Roy, Samapriya, Roy Chowdhury, Rinku and Ficklin, Darren (2015) Dynamic Hydrological Modeling using SWAT within Florida Coastal Everglades. Florida Coastal Everglades LTER Mid Term Review Meeting , March 11-13, 2015. Roy, Samapriya, Darren L. Ficklin, Rinku Roy Chowdhury, Scott Robeson, Jarlath O Neil Dunn, James B Heffernan, Meredith K Steele, Peter M Groffman (2014) Dynamic Assessment of Urban Hydrologic Component using SWAT. International Long Term Ecological Research (ILTER): All Scientist Meeting of the Americas , Valdivia, Chile, December 1-3 , 2014","title":"Conference presentations, Trainings and Attendance"},{"location":"cv/#field-research-experience","text":"Summer 2014 (05/15/2014-06/18/2014). Field research in Bangladesh: Conducted 140 household surveys, with data collection aimed at assessing socio-economic-political drivers of land use and livelihood vulnerability in 4 villages of the Sundarbans mangrove forest region.","title":"Field Research Experience"},{"location":"cv/#grants-and-fellowship","text":"College of Arts and Sciences Graduate Student Travel Award for attending Google Earth Engine Summit 2017 $200 April 2017. Awarded the Graduate Professional Student Government Travel Award for attending Google Earth Engine Summit 2017 $500 April 2017. Awarded the John Odland Graduate Research Fellowship to support graduate research $500 March 2017 Awarded the Lester Spicer Department of Geography Poster Award Fellowship to support graduate research $250 March 2017 Awarded the William R. Black Leadership Memorial Fellowship for $500 March 2017 Co-PI on Extreme Science and Engineering Discovery Environment (XSEDE) Allocation Grant along with Douglas Edmonds for 50,000 Super-computing Units and 500 GB storage volume for project \"Automated API based Pipeline for high volume satellite data\", grant number TG-GEO160014 at Indiana University. Co PI on the Planet Labs Ambassador program providing us unprecedented daily satellite data for entire Louisiana coast an estimated value of over 40,000 square kilometer and estimated access to over 500,000 square kilometer daily (approximately) maximum access value considering RapidEye is (approx. 500,000 x $1.28/sqkm = $640,000) Digital Globe Imagery Grant for an area of over 1500 square kilometer (estimated value at $25,500) April 2016. Awarded the John Odland Graduate Research Fellowship to support graduate research $500 March 2016. Awarded the IndianaView Student Scholarship Program to participate and conduct research $750 March 2016 Awarded the Partners of America (POA) 100,000 Strong award, for $1000 towards travel to the ILTER All Scientist Meeting of the Americas, Valdivia, Chile December 2014 Awarded the ILTER Travel Award, for $500 towards travel to the ILTER All Scientist Meeting of the Americas, Valdivia, Chile December 2014","title":"Grants and Fellowship"},{"location":"cv/#teaching-experience","text":"Lead Instructor, G338: Introduction to Geographic Information Systems, Summer 2017, Indiana University, Bloomington Lead Instructor, G237: Mapping our World, Spring 2017, Indiana University, Bloomington Lead Instructor, G237: Mapping our World, Fall 2016, Indiana University, Bloomington Teaching Assistant, G237: Mapping our World, Spring 2016, Indiana University, Bloomington Lead Instructor, G237: Mapping our World, Fall 2015, Indiana University, Bloomington Guest Lecture I202 Lecture Topic: Spatial Epidemiology September 25th 2014 at Indiana University, Bloomington Teaching Assistant: Environmental Geology Course, G117 Spring 2013, Indiana University Purdue University, Indianapolis Teaching Assistant: Environmental Geology Course, G117 Fall 2012, Indiana University Purdue University, Indianapolis Teaching Assistant: Environmental Geology Course, G117 Spring 2012, Indiana University Purdue University, Indianapolis","title":"Teaching Experience"},{"location":"cv/#memberships-and-committees","text":"Co-Chair for GIS Day Day at Bloomington, Indiana University, 2016 College Committee on Graduate Education, Graduate Student Representative (2015-2016) Planning Committee for GIS Day at Bloomington, Indiana University 2015. Student Member, American Society of Civil Engineers (ASCE) 2008-present Environmental and Water Resources Institute (EWRI) 2010-present Student Member, American Association of Geographer (AAG) 2014-present","title":"Memberships and committees"},{"location":"cv/#certifications-and-trainings","text":"Collaborative Institutional Training Initiative (CITI) Human Research 2014 Trimble Geospatial Training: eCognition- analysis strategies August 14th- 15th, 2014","title":"Certifications and Trainings"},{"location":"cv/#references","text":"Available on request.","title":"References"},{"location":"getting-started/","text":"Getting started As usual, to print help: Planet Pipeline with Google Earth Engine Batch Addons positional arguments: { ,planetkey,aoijson,activatepl,downloadpl,metadata,ee_user,upload,delete,tasks,taskquery,report,cancel,mover,copy,access,collprop,convert2ft,cleanout} --------------------------------------- -----Choose from Planet Tools Below----- --------------------------------------- planetkey Enter your planet API Key aoijson Tool to convert KML, Shapefile,WKT,GeoJSON or Landsat WRS PathRow file to AreaOfInterest.JSON file with structured query for use with Planet API 1.0 activatepl Tool to query and/or activate Planet Assets downloadpl Tool to download Planet Assets metadata Tool to tabulate and convert all metadata files from Planet or Digital Globe Assets ------------------------------------------- ----Choose from Earth Engine Tools Below---- ------------------------------------------- ee_user Get Earth Engine API Key Paste it back to Command line/shell to change user upload Batch Asset Uploader to Earth Engine. delete Deletes collection and all items inside. Supports Unix-like wildcards. tasks Queries currently running, enqued,failed taskquery Queries currently running, enqued,failed ingestions and uploaded assets report Create a report of all tasks and exports to a CSV file cancel Cancel all running tasks mover Moves all assets from one collection to another copy Copies all assets from one collection to another: Including copying from other users if you have read permission to their assets access Sets Permissions for Images, Collection or all assets in EE Folder Example: python ee_permissions.py --mode folder --asset users/john/doe --user jimmy@doe.com:R collprop Sets Overall Properties for Image Collection convert2ft Uploads a given feature collection to Google Fusion Table. cleanout Clear folders with datasets from earlier downloaded optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: geeadd upload -h . If you didn't install geeadd, then you can run it just by going to geeadd directory and running python geeadd.py [arguments go here]","title":"Getting started"},{"location":"getting-started/#getting-started","text":"As usual, to print help: Planet Pipeline with Google Earth Engine Batch Addons positional arguments: { ,planetkey,aoijson,activatepl,downloadpl,metadata,ee_user,upload,delete,tasks,taskquery,report,cancel,mover,copy,access,collprop,convert2ft,cleanout} --------------------------------------- -----Choose from Planet Tools Below----- --------------------------------------- planetkey Enter your planet API Key aoijson Tool to convert KML, Shapefile,WKT,GeoJSON or Landsat WRS PathRow file to AreaOfInterest.JSON file with structured query for use with Planet API 1.0 activatepl Tool to query and/or activate Planet Assets downloadpl Tool to download Planet Assets metadata Tool to tabulate and convert all metadata files from Planet or Digital Globe Assets ------------------------------------------- ----Choose from Earth Engine Tools Below---- ------------------------------------------- ee_user Get Earth Engine API Key Paste it back to Command line/shell to change user upload Batch Asset Uploader to Earth Engine. delete Deletes collection and all items inside. Supports Unix-like wildcards. tasks Queries currently running, enqued,failed taskquery Queries currently running, enqued,failed ingestions and uploaded assets report Create a report of all tasks and exports to a CSV file cancel Cancel all running tasks mover Moves all assets from one collection to another copy Copies all assets from one collection to another: Including copying from other users if you have read permission to their assets access Sets Permissions for Images, Collection or all assets in EE Folder Example: python ee_permissions.py --mode folder --asset users/john/doe --user jimmy@doe.com:R collprop Sets Overall Properties for Image Collection convert2ft Uploads a given feature collection to Google Fusion Table. cleanout Clear folders with datasets from earlier downloaded optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: geeadd upload -h . If you didn't install geeadd, then you can run it just by going to geeadd directory and running python geeadd.py [arguments go here]","title":"Getting started"},{"location":"license/","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"{}\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright {2017} {Samapriya Roy} Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"project-citations/","text":"Project Citations geeup: Simple Google Earth Engine Upload Tool Samapriya Roy. (2018, August 12). samapriya/geeup: geeup: Simple CLI for Earth Engine Uploads (Version 0.0.3). Zenodo. http://doi.org/10.5281/zenodo.1344130 gee2drive: Simple CLI to download Earth Engine Assets Samapriya Roy. (2018, July 31). samapriya/gee2drive: gee2drive: Google Earth Engine to Drive Export Manager (Version 0.0.5). Zenodo. http://doi.org/10.5281/zenodo.1324470 pydrop: Simple Python CLI for Digital Ocean Droplets Samapriya Roy. (2018, July 29). samapriya/pydrop: pydrop: Minimal Python Client for Digital Ocean Droplets (Version 0.0.3). Zenodo. http://doi.org/10.5281/zenodo.1323340 Google Earth Engine Account Transfer Tool Samapriya Roy. (2018, February 27). samapriya/gee-takeout: Google Earth Engine Account Transfer Tool (Version 0.1). Zenodo. http://doi.org/10.5281/zenodo.1185158 Planet-Batch-Slack-Pipeline-CLI Samapriya Roy. (2017, December 4). samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.1079887 Clip-Ship-Planet-CLI Samapriya Roy. (2018, June 30). samapriya/Clip-Ship-Planet-CLI: Clip-Ship-Batch Planet Command Line Interface (Version 0.2.5). Zenodo. http://doi.org/10.5281/zenodo.1302068 Slack-Notifier-CLI-Addon Samapriya Roy. (2017, September 5). samapriya/Slack-Notifier-CLI-Addon: Slack Notifier-CLI Addon (Version 0.1.0). Zenodo. http://doi.org/10.5281/zenodo.885505 Planet-GEE-Pipeline-CLI Samapriya Roy. (2018, August 8). samapriya/Planet-GEE-Pipeline-CLI: Planet-GEE-Pipeline-CLI (Version 0.3.9). Zenodo. http://doi.org/10.5281/zenodo.1341565 Planet-GEE-Pipeline-GUI Samapriya Roy. (2017, June 25). samapriya/Planet-GEE-Pipeline-GUI:Planet-GEE-Pipeline-GUI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.817739 Planet-Pipeline-GUI Samapriya Roy. (2017, August 17). samapriya/Planet-Pipeline-GUI: Planet-Pipeline-GUI (Version 0.3). Zenodo. http://doi.org/10.5281/zenodo.844149 GEE Asset Manager Addons Samapriya Roy. (2017, November 29). samapriya/gee_asset_manager_addon: GEE Asset Manager with Addons (Version 0.2.2). Zenodo. http://doi.org/10.5281/zenodo.1068184 Synthetic LandScape Models Samapriya Roy. (2017, May 21). samapriya/synthetic-landScape-generation-nlm: Synthetic LandScape Generation (Version 0.1.1). Zenodo. http://doi.org/10.5281/zenodo.581701 ArcMap Addons Samapriya Roy. (2017, October 12). samapriya/arcmap-addons: ArcMap Addons (Version 0.2). Zenodo. http://doi.org/10.5281/zenodo.1009210 ArcticDEM-Batch-Pipeline Samapriya Roy. (2017, August 12). samapriya/ArcticDEM-Batch-Pipeline: ArcticDEM-Batch-Pipeline (Version 0.1.1). Zenodo. http://doi.org/10.5281/zenodo.842056 Jetstream-Unofficial-addon Samapriya Roy. (2017, August 16). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon (Version 0.1.1). Zenodo. http://doi.org/10.5281/zenodo.844018","title":"Project Citations"},{"location":"project-citations/#project-citations","text":"geeup: Simple Google Earth Engine Upload Tool Samapriya Roy. (2018, August 12). samapriya/geeup: geeup: Simple CLI for Earth Engine Uploads (Version 0.0.3). Zenodo. http://doi.org/10.5281/zenodo.1344130 gee2drive: Simple CLI to download Earth Engine Assets Samapriya Roy. (2018, July 31). samapriya/gee2drive: gee2drive: Google Earth Engine to Drive Export Manager (Version 0.0.5). Zenodo. http://doi.org/10.5281/zenodo.1324470 pydrop: Simple Python CLI for Digital Ocean Droplets Samapriya Roy. (2018, July 29). samapriya/pydrop: pydrop: Minimal Python Client for Digital Ocean Droplets (Version 0.0.3). Zenodo. http://doi.org/10.5281/zenodo.1323340 Google Earth Engine Account Transfer Tool Samapriya Roy. (2018, February 27). samapriya/gee-takeout: Google Earth Engine Account Transfer Tool (Version 0.1). Zenodo. http://doi.org/10.5281/zenodo.1185158 Planet-Batch-Slack-Pipeline-CLI Samapriya Roy. (2017, December 4). samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.1079887 Clip-Ship-Planet-CLI Samapriya Roy. (2018, June 30). samapriya/Clip-Ship-Planet-CLI: Clip-Ship-Batch Planet Command Line Interface (Version 0.2.5). Zenodo. http://doi.org/10.5281/zenodo.1302068 Slack-Notifier-CLI-Addon Samapriya Roy. (2017, September 5). samapriya/Slack-Notifier-CLI-Addon: Slack Notifier-CLI Addon (Version 0.1.0). Zenodo. http://doi.org/10.5281/zenodo.885505 Planet-GEE-Pipeline-CLI Samapriya Roy. (2018, August 8). samapriya/Planet-GEE-Pipeline-CLI: Planet-GEE-Pipeline-CLI (Version 0.3.9). Zenodo. http://doi.org/10.5281/zenodo.1341565 Planet-GEE-Pipeline-GUI Samapriya Roy. (2017, June 25). samapriya/Planet-GEE-Pipeline-GUI:Planet-GEE-Pipeline-GUI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.817739 Planet-Pipeline-GUI Samapriya Roy. (2017, August 17). samapriya/Planet-Pipeline-GUI: Planet-Pipeline-GUI (Version 0.3). Zenodo. http://doi.org/10.5281/zenodo.844149 GEE Asset Manager Addons Samapriya Roy. (2017, November 29). samapriya/gee_asset_manager_addon: GEE Asset Manager with Addons (Version 0.2.2). Zenodo. http://doi.org/10.5281/zenodo.1068184 Synthetic LandScape Models Samapriya Roy. (2017, May 21). samapriya/synthetic-landScape-generation-nlm: Synthetic LandScape Generation (Version 0.1.1). Zenodo. http://doi.org/10.5281/zenodo.581701 ArcMap Addons Samapriya Roy. (2017, October 12). samapriya/arcmap-addons: ArcMap Addons (Version 0.2). Zenodo. http://doi.org/10.5281/zenodo.1009210 ArcticDEM-Batch-Pipeline Samapriya Roy. (2017, August 12). samapriya/ArcticDEM-Batch-Pipeline: ArcticDEM-Batch-Pipeline (Version 0.1.1). Zenodo. http://doi.org/10.5281/zenodo.842056 Jetstream-Unofficial-addon Samapriya Roy. (2017, August 16). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon (Version 0.1.1). Zenodo. http://doi.org/10.5281/zenodo.844018","title":"Project Citations"},{"location":"pypi/","text":"PyPI Releases I have successfully converted some of the github projects into PyPI projects for allowing easier installation and usage. Some of these have been tested for compatibility with Google Colaboratory and for building within a Docker image. Currently maintained pypi projects include geeup: Simple CLI for Google Earth Engine Uploads This came of the simple need to handle batch uploads of both image assets to collections but also thanks to the new table feature the possibility of batch uploading shapefiles into a folder. Though a lot of these tools including batch image uploader is part of my other project geeadd which also includes additional features to add to the python CLI, this tool was designed to be minimal so as to allow the user to simply query their quota, upload images or tables and also to query ongoing tasks and delete assets. I am hoping this tool with a simple objective proves useful to a few users of Google Earth Engine. pip install geeup gee2drive: Google Earth Engine to Drive Export Manager I created this tool which allows you to run a terminal environment where your personal and general catalog images are part of a autosuggest feature. This tool allows you to look for images based on names for example \" you can search for Sentinel and it will show you full path of images which have the word sentinel in the title\". It also creates a report for your image collections and images so apart from the public datasets this can also find your own datasets as well. You can then generate bandlist to make sure all bands you are exporting are of the same type and then export all images that intersect you aoi. pip install gee2drive Planet API Pipeline Google Earth Engine Batch Assets Manager with Addons The tool allows the user to upload Planet assets to Google Earth Engine from your local system. The tool has been modified and tested in Docker environment and Colab environments. It makes use of the manifest feature of image upload so that it can choose image and asset type appropriately. It also figures out property type automatically to avoid conflict in metadata property type in successive images. You can read more about the project here pip install ppipe pydrop: Minimal Python Client for Digital Ocean Droplets This is a minimal tool designed to interact with the Digital Ocean API. This does not translate all functionalities of the API but is a template I created for some of the most common operations I could perform. New tools will be added in the future as I familiarize myself further with the API structure and use as a student. For now this tool allows you to summarize all your droplets running, including and necessarily a price summary to keep tabs on your droplets monthly and hourly rates. The tool also allows you to seach by tags, delete a drop or perfrom actions such as start, stop or shutdown a droplet. pip install pydrop Planet Clip-Ship Tools CLI The Clips API has been deprecated and will no longer be supporting the standalone Clip and Ship service. Deprecation means that there is intention to remove the service at some point; it DOES NOT mean end-of-life for the service, yet. (Deprecate and end-of-life are two distinct terms.) There should be a plan to replace Clips API with new pre-processing functionality, but no new announcement or timetable for the launch of the new service and the removal (end-of-life) for Clips API has been made. Planet\u2019s Clip API was a compute API designed to allowed users to clip the images to their area of interest. This would save them time in preprocessing and also allow the user to save on their area quota which might have restrictions. You can read more about the project here pip install pclip Google Earth Engine Batch Assets Manager with Addons Google Earth Engine Batch Asset Manager with Addons is an extension of the existing CLI and additional tools were added to include functionality that was missing from the EarthEngine CLI. This includes printing quota, moving and batch copying images, print asset report and task lists to name a few of the features. It is developed case by case basis to include more features in the future as it becomes available or as need arises.You can read more about the project here pip install geeadd Google Takeout and Transfer: Tools and Guide for Code and Asset Transfer This tool replicates a Google Earth Engine account and transfer everything over to a new account. Even if you are not replicating your account, think of this as an add on which allows you to download your entire code repositories, create an asset report and best of all iteratively change permissions to all image-collection and images whether or not within a folder.You can read more about the project here pip install geetakeout","title":"PyPI Projects"},{"location":"pypi/#pypi-releases","text":"I have successfully converted some of the github projects into PyPI projects for allowing easier installation and usage. Some of these have been tested for compatibility with Google Colaboratory and for building within a Docker image. Currently maintained pypi projects include","title":"PyPI Releases"},{"location":"pypi/#geeup-simple-cli-for-google-earth-engine-uploads","text":"This came of the simple need to handle batch uploads of both image assets to collections but also thanks to the new table feature the possibility of batch uploading shapefiles into a folder. Though a lot of these tools including batch image uploader is part of my other project geeadd which also includes additional features to add to the python CLI, this tool was designed to be minimal so as to allow the user to simply query their quota, upload images or tables and also to query ongoing tasks and delete assets. I am hoping this tool with a simple objective proves useful to a few users of Google Earth Engine. pip install geeup","title":"geeup: Simple CLI for Google Earth Engine Uploads"},{"location":"pypi/#gee2drive-google-earth-engine-to-drive-export-manager","text":"I created this tool which allows you to run a terminal environment where your personal and general catalog images are part of a autosuggest feature. This tool allows you to look for images based on names for example \" you can search for Sentinel and it will show you full path of images which have the word sentinel in the title\". It also creates a report for your image collections and images so apart from the public datasets this can also find your own datasets as well. You can then generate bandlist to make sure all bands you are exporting are of the same type and then export all images that intersect you aoi. pip install gee2drive","title":"gee2drive: Google Earth Engine to Drive Export Manager"},{"location":"pypi/#planet-api-pipeline-google-earth-engine-batch-assets-manager-with-addons","text":"The tool allows the user to upload Planet assets to Google Earth Engine from your local system. The tool has been modified and tested in Docker environment and Colab environments. It makes use of the manifest feature of image upload so that it can choose image and asset type appropriately. It also figures out property type automatically to avoid conflict in metadata property type in successive images. You can read more about the project here pip install ppipe","title":"Planet API Pipeline &amp; Google Earth Engine Batch Assets Manager with Addons"},{"location":"pypi/#pydrop-minimal-python-client-for-digital-ocean-droplets","text":"This is a minimal tool designed to interact with the Digital Ocean API. This does not translate all functionalities of the API but is a template I created for some of the most common operations I could perform. New tools will be added in the future as I familiarize myself further with the API structure and use as a student. For now this tool allows you to summarize all your droplets running, including and necessarily a price summary to keep tabs on your droplets monthly and hourly rates. The tool also allows you to seach by tags, delete a drop or perfrom actions such as start, stop or shutdown a droplet. pip install pydrop","title":"pydrop: Minimal Python Client for Digital Ocean Droplets"},{"location":"pypi/#planet-clip-ship-tools-cli","text":"The Clips API has been deprecated and will no longer be supporting the standalone Clip and Ship service. Deprecation means that there is intention to remove the service at some point; it DOES NOT mean end-of-life for the service, yet. (Deprecate and end-of-life are two distinct terms.) There should be a plan to replace Clips API with new pre-processing functionality, but no new announcement or timetable for the launch of the new service and the removal (end-of-life) for Clips API has been made. Planet\u2019s Clip API was a compute API designed to allowed users to clip the images to their area of interest. This would save them time in preprocessing and also allow the user to save on their area quota which might have restrictions. You can read more about the project here pip install pclip","title":"Planet Clip-Ship Tools CLI"},{"location":"pypi/#google-earth-engine-batch-assets-manager-with-addons","text":"Google Earth Engine Batch Asset Manager with Addons is an extension of the existing CLI and additional tools were added to include functionality that was missing from the EarthEngine CLI. This includes printing quota, moving and batch copying images, print asset report and task lists to name a few of the features. It is developed case by case basis to include more features in the future as it becomes available or as need arises.You can read more about the project here pip install geeadd","title":"Google Earth Engine Batch Assets Manager with Addons"},{"location":"pypi/#google-takeout-and-transfer-tools-and-guide-for-code-and-asset-transfer","text":"This tool replicates a Google Earth Engine account and transfer everything over to a new account. Even if you are not replicating your account, think of this as an add on which allows you to download your entire code repositories, create an asset report and best of all iteratively change permissions to all image-collection and images whether or not within a folder.You can read more about the project here pip install geetakeout","title":"Google Takeout and Transfer: Tools and Guide for Code and Asset Transfer"},{"location":"release-notes/","text":"Release notes Project Name Digital Object Identification Number(DOI) Release geeup: GEE Upload CLI 0.0.3 gee2drive: GEE Download CLI 0.0.5 pydrop: Digital Ocean CLI 0.0.3 Google Earth Engine Takeout 0.1 Planet-Batch-Slack-Pipeline 0.1.4 Clip-Ship-Planet-CLI 0.2.5 Slack-Notifier-CLI-Addon 0.1.0 Planet-GEE-Pipeline-CLI 0.3.5 Planet-GEE-Pipeline-GUI 0.1.4 Planet-Pipeline-GUI 0.3 GEE Asset Manager Addons 0.2.3 Synthetic LandScape Models 0.1.1 ArcMap Addons 0.2 ArcticDEM-Batch-Pipeline 0.1.1 Jetstream-Unofficial-addon 0.1.1","title":"Release notes"},{"location":"release-notes/#release-notes","text":"Project Name Digital Object Identification Number(DOI) Release geeup: GEE Upload CLI 0.0.3 gee2drive: GEE Download CLI 0.0.5 pydrop: Digital Ocean CLI 0.0.3 Google Earth Engine Takeout 0.1 Planet-Batch-Slack-Pipeline 0.1.4 Clip-Ship-Planet-CLI 0.2.5 Slack-Notifier-CLI-Addon 0.1.0 Planet-GEE-Pipeline-CLI 0.3.5 Planet-GEE-Pipeline-GUI 0.1.4 Planet-Pipeline-GUI 0.3 GEE Asset Manager Addons 0.2.3 Synthetic LandScape Models 0.1.1 ArcMap Addons 0.2 ArcticDEM-Batch-Pipeline 0.1.1 Jetstream-Unofficial-addon 0.1.1","title":"Release notes"},{"location":"projects/arcmap_addon/","text":"ArcMap Addons While working on different projects over sometime I had to create and use some tools which acted as valuable addons to arcmap toolbox. I am sharing a few and I will keep on updating these as I keep working on new models.This toolbox was created on ArcMap 10.4 and is not backward compatible. The toolbox can be downloaded and added to existing toolbox to create additional functionalities. Table of contents Installation Usage examples Batch Raster to Point Batch Table to CSV Email Notification Iterative Clip MultiBand to Single Images Raster Properties as CSV Raster Copy Iterative Feature Select and Copy Select and Calculate Field Credits Installation We assume that the user already has a copy of ArcMap =10.4. The toolbox can be downloaded along with adjoining script and added onto existing toolbox. You can keep this toolbox as part of the default toolboxes by clicking Save Settings Save as Default Usage examples Usage examples will vary and only continue to grow as new tools are added to the toolbox. Batch Raster to Point This tool allows you to convert all Raster datasets in a folder into a point with the value field converted to GRID code. The tool supports all raster formats supported by ArcMap and renames the files automatically to the source file name. The raster pixel is converted to a centroid value. Batch Table to CSV This tool allows you to batch convert all table files(in this case it looks for '.dbf' files) and converts the fields into csv columns. This is an effective way when you want to handle a large number of dbf files to be imported into other softwares and or processing chains. Email Notification I found this to be one of the most useful tools when running a long process and not having to wait for your results. The tool makes use of the smtp library and allows the user to set up an email service for an email to be sent out after a process has been completed. Iterative Clip This tool does exactl what the name suggests it uses an iterator tool to clip a raster to different boundaries using shapefiles. Requires a single raster file and a folder with multiple polygons for clips. If using on a private machine the Key is saved as a csv file for all future runs of the tool. MultiBand to Single Images The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you. Raster Properties as CSV The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier Raster Copy Iterative Having metadata helps in organising your asstets, but is not mandatory - you can skip it. The downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types. Feature Select and Copy These tools allow the users to iteratively select features using a given SQL expression and then extract that as a new feature class. This can be done iteratively for all feature classes in the folder. Select and Calculate Field This tool allows you to select a field and then recalculate or calculate it based on expressions. The user has the option of simply filling empty fields based on selection of a different field. Meaning I can calculate a field B based on a selection of a seperate field A if I want. Credits I would like to thank all those who bring me interesting problems to solve, there is little that is needed to keep one inspired.","title":"ArcMap Addon Tools"},{"location":"projects/arcmap_addon/#arcmap-addons","text":"While working on different projects over sometime I had to create and use some tools which acted as valuable addons to arcmap toolbox. I am sharing a few and I will keep on updating these as I keep working on new models.This toolbox was created on ArcMap 10.4 and is not backward compatible. The toolbox can be downloaded and added to existing toolbox to create additional functionalities.","title":"ArcMap Addons"},{"location":"projects/arcmap_addon/#table-of-contents","text":"Installation Usage examples Batch Raster to Point Batch Table to CSV Email Notification Iterative Clip MultiBand to Single Images Raster Properties as CSV Raster Copy Iterative Feature Select and Copy Select and Calculate Field Credits","title":"Table of contents"},{"location":"projects/arcmap_addon/#installation","text":"We assume that the user already has a copy of ArcMap =10.4. The toolbox can be downloaded along with adjoining script and added onto existing toolbox. You can keep this toolbox as part of the default toolboxes by clicking Save Settings Save as Default","title":"Installation"},{"location":"projects/arcmap_addon/#usage-examples","text":"Usage examples will vary and only continue to grow as new tools are added to the toolbox.","title":"Usage examples"},{"location":"projects/arcmap_addon/#batch-raster-to-point","text":"This tool allows you to convert all Raster datasets in a folder into a point with the value field converted to GRID code. The tool supports all raster formats supported by ArcMap and renames the files automatically to the source file name. The raster pixel is converted to a centroid value.","title":"Batch Raster to Point"},{"location":"projects/arcmap_addon/#batch-table-to-csv","text":"This tool allows you to batch convert all table files(in this case it looks for '.dbf' files) and converts the fields into csv columns. This is an effective way when you want to handle a large number of dbf files to be imported into other softwares and or processing chains.","title":"Batch Table to CSV"},{"location":"projects/arcmap_addon/#email-notification","text":"I found this to be one of the most useful tools when running a long process and not having to wait for your results. The tool makes use of the smtp library and allows the user to set up an email service for an email to be sent out after a process has been completed.","title":"Email Notification"},{"location":"projects/arcmap_addon/#iterative-clip","text":"This tool does exactl what the name suggests it uses an iterator tool to clip a raster to different boundaries using shapefiles. Requires a single raster file and a folder with multiple polygons for clips. If using on a private machine the Key is saved as a csv file for all future runs of the tool.","title":"Iterative Clip"},{"location":"projects/arcmap_addon/#multiband-to-single-images","text":"The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.","title":"MultiBand to Single Images"},{"location":"projects/arcmap_addon/#raster-properties-as-csv","text":"The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier","title":"Raster Properties as CSV"},{"location":"projects/arcmap_addon/#raster-copy-iterative","text":"Having metadata helps in organising your asstets, but is not mandatory - you can skip it. The downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types.","title":"Raster Copy Iterative"},{"location":"projects/arcmap_addon/#feature-select-and-copy","text":"These tools allow the users to iteratively select features using a given SQL expression and then extract that as a new feature class. This can be done iteratively for all feature classes in the folder.","title":"Feature Select and Copy"},{"location":"projects/arcmap_addon/#select-and-calculate-field","text":"This tool allows you to select a field and then recalculate or calculate it based on expressions. The user has the option of simply filling empty fields based on selection of a different field. Meaning I can calculate a field B based on a selection of a seperate field A if I want.","title":"Select and Calculate Field"},{"location":"projects/arcmap_addon/#credits","text":"I would like to thank all those who bring me interesting problems to solve, there is little that is needed to keep one inspired.","title":"Credits"},{"location":"projects/arcticdem_download/","text":"ArcticDEM Batch Download Processing Tools ArcticDEM project was a joint project supported by both the National Geospatial-Intelligence Agency(NGA) and the National Science Foundation(NSF) with the idea of creating a high resolution and high quality digital surface model(DSM). The product is distributed free of cost as time-dependent DEM strips and is hosted as https links that a user can use to download each strip. As per their policy The seamless terrain mosaic can be distributed without restriction. The created product is a 2-by-2 meter elevation cells over an over of over 20 million square kilometers and uses digital globe stereo imagery to create these high resolution DSM. The method used for the 2m derivate is Surface Extraction with TIN-based Search-space Minimization(SETSM). Based on their acknowledgements requests you can use Acknowledging PGC services(including data access) Geospatial support for this work provided by the Polar Geospatial Center under NSF OPP awards 1043681 1559691. Acknowledging DEMS created from the ArcticDEM project DEMs provided by the Polar Geospatial Center under NSF OPP awards 1043681, 1559691 and 1542736. You can find details on the background, scope and methods among other details here A detailed acknowledgement link can be found here With this in mind and with the potential applications of using these toolsets there was a need to batch download the DEM files for your area of interest and to be able to extract, clean and process metadata. In all fairness this tool has a motive of extending this as an input to Google Earth Engine and hence the last tool which is the metadata parser is designed to create a metadata manifest in a csv file which GEE can understand and associate during asset upload. Table of contents Installation Getting started Usage examples Subset to AOI Estimate Download Size Download DEM Extract DEM Metadata Parsing for GEE Installation We assume that you have installed the requirements files to install all the necessary packages and libraries required to use this tool. To install packages from the requirements.txt file you can simply use pip install -r requirements.txt . Remember that installation is an optional step and you can run this program by simply browsing to the pgcdem-cli file and typing python arcticdem.py . One of the only other requirement for this tool is the Master Shapefile for all DEM footprints(make sure to use the most updated version which can be found here ) This toolbox also uses some functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa sudo apt-get update sudo apt-get install gdal-bin For Windows I found this guide from UCLA To install ArcticDEM Batch Download Processing Tools: git clone https://github.com/samapriya/ArcticDEM-Batch-Pipeline.git cd ArcticDEM-Batch-Pipeline pip install . This release also contains a windows installer which bypasses the need for you to have admin permission, it does however require you to have python in the system path meaning when you open up command prompt you should be able to type python and start it within the command prompt window. Post installation using the installer you can just call ppipe using the command prompt similar to calling python. Give it a go post installation type arcticdem -h The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. As a extra addon feature I have wrapped the cli into a Graphical User Interface(GUI) so some people will find it easier to use. To install run python setup.py develop or python setup.py install In a linux distribution sudo python setup.py develop or sudo python setup.py install Getting started To obtain help for a specific functionality, simply call it with help switch, e.g.: arcticdem demextract -h . If you didn't install arcticdem, then you can run it just by going to arcticdem-cli directory and running python arcticdem.py [arguments go here] As usual, to print help arcticdem -h : ArcticDEM Batch Download Processing Tools positional arguments: { ,demaoi,demsize,demdownload,demextract,demmeta} --------------------------------------- -----Choose from ArcticDEM-Download Tools Below----- --------------------------------------- demaoi Allows user to subset Master ArcticDEM to their AOI demsize Allows users to estimate total download size and space left in your destination folder demdownload Allows users to batch download ArcticDEM Strips using aoi shapefile demextract Allows users to extract both image and metadata files from the zipped tar files demmeta Tool to process metadata files into CSV for all strips[For use with Google Earth Engine] optional arguments: -h, --help show this help message and exit Subset to AOI The script clips the master ArcticDEM strip file to a smaller subset usgin an area of interest shapefile. This allows to get the strip DEM(s) for only the area of interest and to use that to download these files. The subset allows the user to limit the total amount of strips to be downloaded and processed. The script will create a new shapefile with the clipped subset of the master ArcticDEM strip file. Make sure you reproject your aoi shapefile to the same projection as the ArcticDEM strip file usage: arcticdem.py demaoi [-h] [--source SOURCE] [--target TARGET] [--output OUTPUT] optional arguments: -h, --help show this help message and exit --source SOURCE Choose location of your AOI shapefile --target TARGET Choose the location of the master ArcticDEM strip file --output OUTPUT Choose the location of the output shapefile based on your AOI An example setup would be arcticdem demaoi --source C:\\users\\aoi.shp --target C:\\users\\masterdem.shp --output C:\\users\\master_aoi.shp Estimate Download Size One of the most common things you want to do is to know if the destination where you want to save these files has enough space before you begin the download process. This script allows you to query the total download size for your area of interest and the destination drive where you want to save the compressed files. It also recursively updates overall download size on the screen and print total size needed along with total download size in GB. usage: arcticdem.py demsize [-h] [--infile INFILE] [--path PATH] optional arguments: -h, --help show this help message and exit --infile INFILE Choose the clipped aoi file you clipped from demaoi tool[This is the subset of the master ArcticDEM Strip] --path PATH Choose the destination folder where you want your dem files to be saved[This checks available disk space] An example setup would be arcticdem demsize --infile C:\\users\\master_aoi.shp --path C:\\users\\ArcticDEM The program might misbehave if the area of interest is extremely large or be sluggish in nature. Download DEM What we were mainly interested after we know that we have enough space to download is to download the files. The script used a multi part download library to download the files quicker and in a more managed style to the destination given by the user. usage: arcticdem.py demdownload [-h] [--subset SUBSET] [--desination DESINATION] optional arguments: -h, --help show this help message and exit --subset SUBSET Choose the location of the output shapefile based on your AOI[You got this from demaoi tool] --desination DESINATION Choose the destination where you want to download your files An example setup would be arcticdem demdownload --subset C:\\users\\master_aoi.shp --destination C:\\users\\ArcticDEM Extract DEM This downloaded DEM files are tar or tar gz files and need to be extracted. The important thing to note is that the script retains the dem file, the matchtag file and the metadata text files in separate directories within the destination directory. usage: arcticdem.py demextract [-h] [--folder FOLDER] [--destination DESTINATION] [--action ACTION] optional arguments: -h, --help show this help message and exit --folder FOLDER Choose the download file where you downloaded your tar zipped files --destination DESTINATION Choose the destination folder where you want your images and metadata files to be extracted --action ACTION Choose if you want your zipped files to be deleted post extraction yes | no An example setup would be arcticdem demdextract --folder C:\\users\\ArcticDEM --destination C:\\users\\ArcticDEM\\Extract --action yes Metadata Parsing for GEE One of my key interest in working on the ArcticDEM parsing was to be able to upload this to Google Earth Engine(GEE). The metadata parser script allows you to parse all metadata into a combined csv file to be used to upload images to GEE. The manifest and upload tools are separate from this package tool and included in my gee_asset_manager_addon. usage: arcticdem.py demmeta [-h] [--folder FOLDER] [--metadata METADATA] [--error ERROR] optional arguments: -h, --help show this help message and exit --folder FOLDER Choose where you unzipped and extracted your DEM and metadata files --metadata METADATA Choose a path to the metadata file example: users/desktop/metadata.csv --error ERROR Choose a path to the errorlog file example: users/desktop/errorlog.csv An example setup would be arcticdem demmeta --folder C:\\users\\ArcticDEM\\Extract\\pgcmeta --metadata C:\\users\\arcticdem_metadata.csv --error C:\\users\\arcticdem_errorlog.csv Changelog [0.1.1] - 2017-08-12 Added Can now handle ogr input and includes instruction to project aoi in same projection as DEM strip. Added the capability of skipping over already downloaded files and continues with left over downloads. Completed recompiling executable to include changes.","title":"ArcticDEM-Batch-Pipeline"},{"location":"projects/arcticdem_download/#arcticdem-batch-download-processing-tools","text":"ArcticDEM project was a joint project supported by both the National Geospatial-Intelligence Agency(NGA) and the National Science Foundation(NSF) with the idea of creating a high resolution and high quality digital surface model(DSM). The product is distributed free of cost as time-dependent DEM strips and is hosted as https links that a user can use to download each strip. As per their policy The seamless terrain mosaic can be distributed without restriction. The created product is a 2-by-2 meter elevation cells over an over of over 20 million square kilometers and uses digital globe stereo imagery to create these high resolution DSM. The method used for the 2m derivate is Surface Extraction with TIN-based Search-space Minimization(SETSM). Based on their acknowledgements requests you can use Acknowledging PGC services(including data access) Geospatial support for this work provided by the Polar Geospatial Center under NSF OPP awards 1043681 1559691. Acknowledging DEMS created from the ArcticDEM project DEMs provided by the Polar Geospatial Center under NSF OPP awards 1043681, 1559691 and 1542736. You can find details on the background, scope and methods among other details here A detailed acknowledgement link can be found here With this in mind and with the potential applications of using these toolsets there was a need to batch download the DEM files for your area of interest and to be able to extract, clean and process metadata. In all fairness this tool has a motive of extending this as an input to Google Earth Engine and hence the last tool which is the metadata parser is designed to create a metadata manifest in a csv file which GEE can understand and associate during asset upload.","title":"ArcticDEM Batch Download &amp; Processing Tools"},{"location":"projects/arcticdem_download/#table-of-contents","text":"Installation Getting started Usage examples Subset to AOI Estimate Download Size Download DEM Extract DEM Metadata Parsing for GEE","title":"Table of contents"},{"location":"projects/arcticdem_download/#installation","text":"We assume that you have installed the requirements files to install all the necessary packages and libraries required to use this tool. To install packages from the requirements.txt file you can simply use pip install -r requirements.txt . Remember that installation is an optional step and you can run this program by simply browsing to the pgcdem-cli file and typing python arcticdem.py . One of the only other requirement for this tool is the Master Shapefile for all DEM footprints(make sure to use the most updated version which can be found here ) This toolbox also uses some functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa sudo apt-get update sudo apt-get install gdal-bin For Windows I found this guide from UCLA To install ArcticDEM Batch Download Processing Tools: git clone https://github.com/samapriya/ArcticDEM-Batch-Pipeline.git cd ArcticDEM-Batch-Pipeline pip install . This release also contains a windows installer which bypasses the need for you to have admin permission, it does however require you to have python in the system path meaning when you open up command prompt you should be able to type python and start it within the command prompt window. Post installation using the installer you can just call ppipe using the command prompt similar to calling python. Give it a go post installation type arcticdem -h The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. As a extra addon feature I have wrapped the cli into a Graphical User Interface(GUI) so some people will find it easier to use. To install run python setup.py develop or python setup.py install In a linux distribution sudo python setup.py develop or sudo python setup.py install","title":"Installation"},{"location":"projects/arcticdem_download/#getting-started","text":"To obtain help for a specific functionality, simply call it with help switch, e.g.: arcticdem demextract -h . If you didn't install arcticdem, then you can run it just by going to arcticdem-cli directory and running python arcticdem.py [arguments go here] As usual, to print help arcticdem -h : ArcticDEM Batch Download Processing Tools positional arguments: { ,demaoi,demsize,demdownload,demextract,demmeta} --------------------------------------- -----Choose from ArcticDEM-Download Tools Below----- --------------------------------------- demaoi Allows user to subset Master ArcticDEM to their AOI demsize Allows users to estimate total download size and space left in your destination folder demdownload Allows users to batch download ArcticDEM Strips using aoi shapefile demextract Allows users to extract both image and metadata files from the zipped tar files demmeta Tool to process metadata files into CSV for all strips[For use with Google Earth Engine] optional arguments: -h, --help show this help message and exit","title":"Getting started"},{"location":"projects/arcticdem_download/#subset-to-aoi","text":"The script clips the master ArcticDEM strip file to a smaller subset usgin an area of interest shapefile. This allows to get the strip DEM(s) for only the area of interest and to use that to download these files. The subset allows the user to limit the total amount of strips to be downloaded and processed. The script will create a new shapefile with the clipped subset of the master ArcticDEM strip file. Make sure you reproject your aoi shapefile to the same projection as the ArcticDEM strip file usage: arcticdem.py demaoi [-h] [--source SOURCE] [--target TARGET] [--output OUTPUT] optional arguments: -h, --help show this help message and exit --source SOURCE Choose location of your AOI shapefile --target TARGET Choose the location of the master ArcticDEM strip file --output OUTPUT Choose the location of the output shapefile based on your AOI An example setup would be arcticdem demaoi --source C:\\users\\aoi.shp --target C:\\users\\masterdem.shp --output C:\\users\\master_aoi.shp","title":"Subset to AOI"},{"location":"projects/arcticdem_download/#estimate-download-size","text":"One of the most common things you want to do is to know if the destination where you want to save these files has enough space before you begin the download process. This script allows you to query the total download size for your area of interest and the destination drive where you want to save the compressed files. It also recursively updates overall download size on the screen and print total size needed along with total download size in GB. usage: arcticdem.py demsize [-h] [--infile INFILE] [--path PATH] optional arguments: -h, --help show this help message and exit --infile INFILE Choose the clipped aoi file you clipped from demaoi tool[This is the subset of the master ArcticDEM Strip] --path PATH Choose the destination folder where you want your dem files to be saved[This checks available disk space] An example setup would be arcticdem demsize --infile C:\\users\\master_aoi.shp --path C:\\users\\ArcticDEM The program might misbehave if the area of interest is extremely large or be sluggish in nature.","title":"Estimate Download Size"},{"location":"projects/arcticdem_download/#download-dem","text":"What we were mainly interested after we know that we have enough space to download is to download the files. The script used a multi part download library to download the files quicker and in a more managed style to the destination given by the user. usage: arcticdem.py demdownload [-h] [--subset SUBSET] [--desination DESINATION] optional arguments: -h, --help show this help message and exit --subset SUBSET Choose the location of the output shapefile based on your AOI[You got this from demaoi tool] --desination DESINATION Choose the destination where you want to download your files An example setup would be arcticdem demdownload --subset C:\\users\\master_aoi.shp --destination C:\\users\\ArcticDEM","title":"Download DEM"},{"location":"projects/arcticdem_download/#extract-dem","text":"This downloaded DEM files are tar or tar gz files and need to be extracted. The important thing to note is that the script retains the dem file, the matchtag file and the metadata text files in separate directories within the destination directory. usage: arcticdem.py demextract [-h] [--folder FOLDER] [--destination DESTINATION] [--action ACTION] optional arguments: -h, --help show this help message and exit --folder FOLDER Choose the download file where you downloaded your tar zipped files --destination DESTINATION Choose the destination folder where you want your images and metadata files to be extracted --action ACTION Choose if you want your zipped files to be deleted post extraction yes | no An example setup would be arcticdem demdextract --folder C:\\users\\ArcticDEM --destination C:\\users\\ArcticDEM\\Extract --action yes","title":"Extract DEM"},{"location":"projects/arcticdem_download/#metadata-parsing-for-gee","text":"One of my key interest in working on the ArcticDEM parsing was to be able to upload this to Google Earth Engine(GEE). The metadata parser script allows you to parse all metadata into a combined csv file to be used to upload images to GEE. The manifest and upload tools are separate from this package tool and included in my gee_asset_manager_addon. usage: arcticdem.py demmeta [-h] [--folder FOLDER] [--metadata METADATA] [--error ERROR] optional arguments: -h, --help show this help message and exit --folder FOLDER Choose where you unzipped and extracted your DEM and metadata files --metadata METADATA Choose a path to the metadata file example: users/desktop/metadata.csv --error ERROR Choose a path to the errorlog file example: users/desktop/errorlog.csv An example setup would be arcticdem demmeta --folder C:\\users\\ArcticDEM\\Extract\\pgcmeta --metadata C:\\users\\arcticdem_metadata.csv --error C:\\users\\arcticdem_errorlog.csv","title":"Metadata Parsing for GEE"},{"location":"projects/arcticdem_download/#changelog","text":"","title":"Changelog"},{"location":"projects/arcticdem_download/#011-2017-08-12","text":"","title":"[0.1.1] - 2017-08-12"},{"location":"projects/arcticdem_download/#added","text":"Can now handle ogr input and includes instruction to project aoi in same projection as DEM strip. Added the capability of skipping over already downloaded files and continues with left over downloads. Completed recompiling executable to include changes.","title":"Added"},{"location":"projects/clip_ship_planet_cli/","text":"Clip Ship Planet CLI addon Note: The Clips API has been deprecated - and will no longer be supporting the standalone Clip and Ship service. Deprecation means that there is intention to remove the service at some point; it DOES NOT mean end-of-life for the service, yet. (Deprecate and end-of-life are two distinct terms.) There should be a plan to replace Clips API with new pre-processing functionality, but no new announcement or timetable for the launch of the new service and the removal (end-of-life) for Clips API has been made. Planet's Clip API was a compute API designed to allowed users to clip the images to their area of interest. This would save them time in preprocessing and also allow the user to save on their area quota which might have restrictions. Based on Planet's Education and Research Program this quota is set at 10,000 square kilometers a month, which means saving up on quota is very useful. The discussion also led to an important clarification that users are in fact charged only for the area downloaded post clip if using the clip operation and hence this tool. This tool takes a sequential approach from activation to generating a clip request for multiple images activated and then processing the download tokens to actually download the clipped image files. The tool also consists of a sort function which allows the user to extract the files and sort them by type and deleting the original files to save on space. Installation To install the Clip-Ship-Planet-CLI you can simply perform the following action with Linux(Tested on Ubuntu 16): git clone https://github.com/samapriya/Clip-Ship-Planet-CLI.git cd Clip-Ship-Planet-CLI pip install -r requirements.txt On a windows as well as a linux machine, installation is an optional step; the application can also be run directly by executing pclip.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment but you can also install it to system python and should not create any conflicts. To install on windows download the setup files as a zip package, unpack and run python setup.py develop or python setup.py install In a linux distribution sudo python setup.py develop or sudo python setup.py install pclip -h Table of contents Getting started Usage examples Planet Key Planet Quota AOI JSON Activate or Check Asset List IDs Clipping with GeoJSON Clipping with JSON Downloading Clipped Imagery Sorting Getting started As usual, to print help: Planet Clip Tools CLI positional arguments: { ,planetkey,aoijson,activate,aoiupdate,idlist,geojsonc,jsonc,downloadclips,sort} ------------------------------------------- -----Choose from Planet Clip Tools----- ------------------------------------------- planetkey Enter your planet API Key quota Prints your quota details aoijson Tool to convert KML, Shapefile,WKT,GeoJSON or Landsat WRS PathRow file to AreaOfInterest.JSON file with structured query for use with Planet API 1.0 activate Tool to query and/or activate Planet Assets idlist Allows users to generate an id list for the selected item and asset type for example item_asset= PSOrthoTile analytic/PSScene3Band visual. This is used with the clip tool geojsonc Allows users to batch submit clipping request to the Planet Clip API using geometry in geojson file jsonc Allows users to batch submit clipping request to the Planet Clip API using geometry in structured json file. This is preferred because the structured JSON allows the activate tool to stream line asset ids being requested and to extract geometry from the same file downloadclips Allows users to batch download clipped assets post computation using a directory path(Requires you to first activate and run geojson or json tool) sort Allows users to unzip downloaded files to new folder and sorts into images and metadata optional arguments: -h, --help show this help message and exit Usage examples The tools have been designed to follow a sequential setup from activation, clip, download and even sort and includes steps that help resolve additional issues a user might face trying to download clipped area of interests instead of entire scenes. The system will ask you to enter your API key before the CLI starts(this will prompt you only once to change API key use the Planet Key tool). Planet Key This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools. Ites makes use of the Planet client and esentially executes planet init usage: pclip planetkey [-h] optional arguments: -h, --help show this help message and exit Planet Quota This tool prints details on your existing quota and your area remaining usage: pclip quota optional arguments: -h, --help show this help message and exit AOI JSON The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you. usage: pclip aoijson [-h] [--start START] [--end END] [--cloud CLOUD] [--inputfile INPUTFILE] [--geo GEO] [--loc LOC] optional arguments: -h, --help show this help message and exit --start START Start date in YYYY-MM-DD? --end END End date in YYYY-MM-DD? --cloud CLOUD Maximum Cloud Cover(0-1) representing 0-100 --inputfile INPUTFILE Choose a kml/shapefile/geojson or WKT file for AOI(KML/SHP/GJSON/WKT) or WRS (6 digit RowPath Example: 023042) --geo GEO map.geojson/aoi.kml/aoi.shp/aoi.wkt file --loc LOC Location where aoi.json file is to be stored As with the Planet-GEE-Pipeline-CLI the aoijson tool allows the user to bring any filetype of interest, which includes GEOJSON, WKT, KML or SHP file including but not limited to WRS rowpath setup and structures it to enable filtered query using Planet's data API. A simple setup would be pclip aoijson --start \"2017-06-01\" --end \"2017-12-31\" --cloud \"0.15\" --inputfile \"GJSON\" --geo \"C:\\planet\\myarea.geojson\" --loc \"C:\\planet\" the output is always named as aoi.json. Activate or Check Asset The activate tool allows the users to either check or activate planet assets. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier. This is a necessary step since the clip API can only work with those ID(s) which have been activated. In the future the list ID tool will check for number of activated id and wait for all of them to be activated before generating an ID list. usage: pclip activate [-h] [--aoi AOI] [--action ACTION] [--asst ASST] optional arguments: -h, --help show this help message and exit --aoi AOI Choose aoi.json file created earlier --action ACTION choose between check/activate --asset ASST Choose between planet asset types (PSOrthoTile analytic/PSOrthoTile analytic_dn/PSOrthoTile visual/PSScene4Band analytic/PSScene4Band analytic_dn/PSScene3Band analytic/PSScene3Band analytic_dn/PSScene3Band visual/REOrthoTile analytic/REOrthoTile visual An example setup for asset activation is the following pclip activate --aoi \"C:\\planet\\aoi.json\" --action \"activate\" --asset \"PSOrthoTile analytic\" List IDs The next step is to list ID(s) that you have activated, this creates a temporary file containing the list of ID(s) which can be used to iteratively call the clips API. This is a modification of the activation function to use only the item id instead of item type and asset id and write to file for future use. usage: pclip idlist [-h] [--aoi AOI] [--asset ASSET] optional arguments: -h, --help show this help message and exit --aoi AOI Input path to the structured json file from which we will generate the clips --asset ASSET Choose from asset type for example: PSOrthoTile analytic | REOrthoTile analytic The example setup for this command is the following pclip idlist --aoi \u201cC:\\planet\\aoi.json\u201d --asset \u201cPSOrthoTile analytic\u201d Clipping with GeoJSON A geejson file can be used directly to clip and query the area of interest and then submit clip process. I added this is a functionality but want to make clear that this does not take into consideration any other filters such as cloud cover or start and end date, and hence should be used only when you do not need to apply any filter. usage: pclip geojsonc [-h] [--path PATH] [--item ITEM] [--asset ASSET] optional arguments: -h, --help show this help message and exit --path PATH Path to the geojson file including filename (Example: C:\\users ile.geojson) --item ITEM Choose from item type for example: PSOrthoTile , REOrthoTile --asset ASSET Choose from asset type for example: visual , analytic ``` A simple setup for the JSON tool is the following ```pclip geojsonc --path \u201cC:\\planet\\aoi.geojson\u201d --item \u201cPSOrthoTile\u201d --asset \u201canalytic ``` ### Clipping with JSON This is the preferred style of submitting the clip requests using the IDlist we generated earlier. This is already structured before even activating assets and includes the additional filters you might have used for selecting the images. usage: pclip jsonc [-h] [--path PATH] [--item ITEM] [--asset ASSET] optional arguments: -h, --help show this help message and exit --path PATH Path to the json file including filename (Example: C:\\users ile.json) --item ITEM Choose from item type for example:\"PSOrthoTile\",\"REOrthoTile\" --asset ASSET Choose from asset type for example: \"visual\",\"analytic\" ``` A simple setup for the JSON tool is the following pclip jsonc --path \u201cC:\\planet\\aoi.json\u201d --item \u201cPSOrthoTile\u201d --asset \u201canalytic\" Downloading Clipped Imagery The last step includes providing a location where the clipped imagery can be downloaded. This includes the zip files that are generated from the earlier step and include a download token that expires over time. This batch downloads the clipped zip files to destination directory usage: pclip downloadclips [-h] [--dir DIR] optional arguments: -h, --help show this help message and exit --dir DIR Output directory to save the assets. All files are zipped and include metadata A simple setup includes just the location to the download directory for the zipped clipped files to be downloaded pclip downloadclips --dir \u201cC:\\planet\\zipped Sorting As an additional measure and because it makes arranging and handling datasets easily, this setup comes completed with a sort tool. If a output directory is provided for the unzipped files, the tool unzips all files, moves the images and metadata to seperate directories and then deletes the original zipped files to save space. usage: pclip sort [-h] [--zipped ZIPPED] [--unzipped UNZIPPED] optional arguments: -h, --help show this help message and exit --zipped ZIPPED Folder containing downloaded clipped files which are zipped --unzipped UNZIPPED Folder where you want your files to be unzipped and sorted A simple would be the following (Images and metadata are sorted into an image and metadata folder inside the unzipped files folder) pclip sort --zipped \u201cC:\\planet\\zipped\u201d --unzipped \u201cC:\\planet\\unzipped\u201d Changelog v0.2.2 Improved Planet Key Handler Added new tool to insepect planet account quota v0.2.1 Thanks to commit suggested by Rabscuttler Fixed issues with help text and installer v0.2.0 Fixed issues with config files v0.1.9 Now handles running and succeeded status better Now enumerates during clip and download to allow user estimates on number of assets clipped and/or downloaded v0.1.8 Includes required packages list within installer Robust GEOJSON Parsing v0.1.7 Fixed issues with processing visual asset types The Clip function now handles error codes if the post response code is not 202(accepted for processing) then the error code and item and asset type is printed. v0.1.6 Handles single time input API Key, this is needed only once to start the program Fixed issue with base metadata folder during sort Updated asset argument for asset activation to match styles v0.1.5 Updated Requirements.txt to include pyshp Fixed subprocess shell error, for now shell=True v0.1.4 General Improvements v0.1.3 General Improvements v0.1.2 Tested on Ubuntu 16.04 and now handles permissions problem Temporary files now written to config folders to avoid admin permission v0.1.1 General Improvements","title":"Clip Ship Planet CLI"},{"location":"projects/clip_ship_planet_cli/#clip-ship-planet-cli-addon","text":"Note: The Clips API has been deprecated - and will no longer be supporting the standalone Clip and Ship service. Deprecation means that there is intention to remove the service at some point; it DOES NOT mean end-of-life for the service, yet. (Deprecate and end-of-life are two distinct terms.) There should be a plan to replace Clips API with new pre-processing functionality, but no new announcement or timetable for the launch of the new service and the removal (end-of-life) for Clips API has been made. Planet's Clip API was a compute API designed to allowed users to clip the images to their area of interest. This would save them time in preprocessing and also allow the user to save on their area quota which might have restrictions. Based on Planet's Education and Research Program this quota is set at 10,000 square kilometers a month, which means saving up on quota is very useful. The discussion also led to an important clarification that users are in fact charged only for the area downloaded post clip if using the clip operation and hence this tool. This tool takes a sequential approach from activation to generating a clip request for multiple images activated and then processing the download tokens to actually download the clipped image files. The tool also consists of a sort function which allows the user to extract the files and sort them by type and deleting the original files to save on space.","title":"Clip Ship Planet CLI addon"},{"location":"projects/clip_ship_planet_cli/#installation","text":"To install the Clip-Ship-Planet-CLI you can simply perform the following action with Linux(Tested on Ubuntu 16): git clone https://github.com/samapriya/Clip-Ship-Planet-CLI.git cd Clip-Ship-Planet-CLI pip install -r requirements.txt On a windows as well as a linux machine, installation is an optional step; the application can also be run directly by executing pclip.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment but you can also install it to system python and should not create any conflicts. To install on windows download the setup files as a zip package, unpack and run python setup.py develop or python setup.py install In a linux distribution sudo python setup.py develop or sudo python setup.py install pclip -h","title":"Installation"},{"location":"projects/clip_ship_planet_cli/#table-of-contents","text":"Getting started Usage examples Planet Key Planet Quota AOI JSON Activate or Check Asset List IDs Clipping with GeoJSON Clipping with JSON Downloading Clipped Imagery Sorting","title":"Table of contents"},{"location":"projects/clip_ship_planet_cli/#getting-started","text":"As usual, to print help: Planet Clip Tools CLI positional arguments: { ,planetkey,aoijson,activate,aoiupdate,idlist,geojsonc,jsonc,downloadclips,sort} ------------------------------------------- -----Choose from Planet Clip Tools----- ------------------------------------------- planetkey Enter your planet API Key quota Prints your quota details aoijson Tool to convert KML, Shapefile,WKT,GeoJSON or Landsat WRS PathRow file to AreaOfInterest.JSON file with structured query for use with Planet API 1.0 activate Tool to query and/or activate Planet Assets idlist Allows users to generate an id list for the selected item and asset type for example item_asset= PSOrthoTile analytic/PSScene3Band visual. This is used with the clip tool geojsonc Allows users to batch submit clipping request to the Planet Clip API using geometry in geojson file jsonc Allows users to batch submit clipping request to the Planet Clip API using geometry in structured json file. This is preferred because the structured JSON allows the activate tool to stream line asset ids being requested and to extract geometry from the same file downloadclips Allows users to batch download clipped assets post computation using a directory path(Requires you to first activate and run geojson or json tool) sort Allows users to unzip downloaded files to new folder and sorts into images and metadata optional arguments: -h, --help show this help message and exit","title":"Getting started"},{"location":"projects/clip_ship_planet_cli/#usage-examples","text":"The tools have been designed to follow a sequential setup from activation, clip, download and even sort and includes steps that help resolve additional issues a user might face trying to download clipped area of interests instead of entire scenes. The system will ask you to enter your API key before the CLI starts(this will prompt you only once to change API key use the Planet Key tool).","title":"Usage examples"},{"location":"projects/clip_ship_planet_cli/#planet-key","text":"This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools. Ites makes use of the Planet client and esentially executes planet init usage: pclip planetkey [-h] optional arguments: -h, --help show this help message and exit","title":"Planet Key"},{"location":"projects/clip_ship_planet_cli/#planet-quota","text":"This tool prints details on your existing quota and your area remaining usage: pclip quota optional arguments: -h, --help show this help message and exit","title":"Planet Quota"},{"location":"projects/clip_ship_planet_cli/#aoi-json","text":"The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you. usage: pclip aoijson [-h] [--start START] [--end END] [--cloud CLOUD] [--inputfile INPUTFILE] [--geo GEO] [--loc LOC] optional arguments: -h, --help show this help message and exit --start START Start date in YYYY-MM-DD? --end END End date in YYYY-MM-DD? --cloud CLOUD Maximum Cloud Cover(0-1) representing 0-100 --inputfile INPUTFILE Choose a kml/shapefile/geojson or WKT file for AOI(KML/SHP/GJSON/WKT) or WRS (6 digit RowPath Example: 023042) --geo GEO map.geojson/aoi.kml/aoi.shp/aoi.wkt file --loc LOC Location where aoi.json file is to be stored As with the Planet-GEE-Pipeline-CLI the aoijson tool allows the user to bring any filetype of interest, which includes GEOJSON, WKT, KML or SHP file including but not limited to WRS rowpath setup and structures it to enable filtered query using Planet's data API. A simple setup would be pclip aoijson --start \"2017-06-01\" --end \"2017-12-31\" --cloud \"0.15\" --inputfile \"GJSON\" --geo \"C:\\planet\\myarea.geojson\" --loc \"C:\\planet\" the output is always named as aoi.json.","title":"AOI JSON"},{"location":"projects/clip_ship_planet_cli/#activate-or-check-asset","text":"The activate tool allows the users to either check or activate planet assets. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier. This is a necessary step since the clip API can only work with those ID(s) which have been activated. In the future the list ID tool will check for number of activated id and wait for all of them to be activated before generating an ID list. usage: pclip activate [-h] [--aoi AOI] [--action ACTION] [--asst ASST] optional arguments: -h, --help show this help message and exit --aoi AOI Choose aoi.json file created earlier --action ACTION choose between check/activate --asset ASST Choose between planet asset types (PSOrthoTile analytic/PSOrthoTile analytic_dn/PSOrthoTile visual/PSScene4Band analytic/PSScene4Band analytic_dn/PSScene3Band analytic/PSScene3Band analytic_dn/PSScene3Band visual/REOrthoTile analytic/REOrthoTile visual An example setup for asset activation is the following pclip activate --aoi \"C:\\planet\\aoi.json\" --action \"activate\" --asset \"PSOrthoTile analytic\"","title":"Activate or Check Asset"},{"location":"projects/clip_ship_planet_cli/#list-ids","text":"The next step is to list ID(s) that you have activated, this creates a temporary file containing the list of ID(s) which can be used to iteratively call the clips API. This is a modification of the activation function to use only the item id instead of item type and asset id and write to file for future use. usage: pclip idlist [-h] [--aoi AOI] [--asset ASSET] optional arguments: -h, --help show this help message and exit --aoi AOI Input path to the structured json file from which we will generate the clips --asset ASSET Choose from asset type for example: PSOrthoTile analytic | REOrthoTile analytic The example setup for this command is the following pclip idlist --aoi \u201cC:\\planet\\aoi.json\u201d --asset \u201cPSOrthoTile analytic\u201d","title":"List IDs"},{"location":"projects/clip_ship_planet_cli/#clipping-with-geojson","text":"A geejson file can be used directly to clip and query the area of interest and then submit clip process. I added this is a functionality but want to make clear that this does not take into consideration any other filters such as cloud cover or start and end date, and hence should be used only when you do not need to apply any filter. usage: pclip geojsonc [-h] [--path PATH] [--item ITEM] [--asset ASSET] optional arguments: -h, --help show this help message and exit --path PATH Path to the geojson file including filename (Example: C:\\users ile.geojson) --item ITEM Choose from item type for example: PSOrthoTile , REOrthoTile --asset ASSET Choose from asset type for example: visual , analytic ``` A simple setup for the JSON tool is the following ```pclip geojsonc --path \u201cC:\\planet\\aoi.geojson\u201d --item \u201cPSOrthoTile\u201d --asset \u201canalytic ``` ### Clipping with JSON This is the preferred style of submitting the clip requests using the IDlist we generated earlier. This is already structured before even activating assets and includes the additional filters you might have used for selecting the images. usage: pclip jsonc [-h] [--path PATH] [--item ITEM] [--asset ASSET] optional arguments: -h, --help show this help message and exit --path PATH Path to the json file including filename (Example: C:\\users ile.json) --item ITEM Choose from item type for example:\"PSOrthoTile\",\"REOrthoTile\" --asset ASSET Choose from asset type for example: \"visual\",\"analytic\" ``` A simple setup for the JSON tool is the following pclip jsonc --path \u201cC:\\planet\\aoi.json\u201d --item \u201cPSOrthoTile\u201d --asset \u201canalytic\"","title":"Clipping with GeoJSON"},{"location":"projects/clip_ship_planet_cli/#downloading-clipped-imagery","text":"The last step includes providing a location where the clipped imagery can be downloaded. This includes the zip files that are generated from the earlier step and include a download token that expires over time. This batch downloads the clipped zip files to destination directory usage: pclip downloadclips [-h] [--dir DIR] optional arguments: -h, --help show this help message and exit --dir DIR Output directory to save the assets. All files are zipped and include metadata A simple setup includes just the location to the download directory for the zipped clipped files to be downloaded pclip downloadclips --dir \u201cC:\\planet\\zipped","title":"Downloading Clipped Imagery"},{"location":"projects/clip_ship_planet_cli/#sorting","text":"As an additional measure and because it makes arranging and handling datasets easily, this setup comes completed with a sort tool. If a output directory is provided for the unzipped files, the tool unzips all files, moves the images and metadata to seperate directories and then deletes the original zipped files to save space. usage: pclip sort [-h] [--zipped ZIPPED] [--unzipped UNZIPPED] optional arguments: -h, --help show this help message and exit --zipped ZIPPED Folder containing downloaded clipped files which are zipped --unzipped UNZIPPED Folder where you want your files to be unzipped and sorted A simple would be the following (Images and metadata are sorted into an image and metadata folder inside the unzipped files folder) pclip sort --zipped \u201cC:\\planet\\zipped\u201d --unzipped \u201cC:\\planet\\unzipped\u201d","title":"Sorting"},{"location":"projects/clip_ship_planet_cli/#changelog","text":"","title":"Changelog"},{"location":"projects/clip_ship_planet_cli/#v022","text":"Improved Planet Key Handler Added new tool to insepect planet account quota","title":"v0.2.2"},{"location":"projects/clip_ship_planet_cli/#v021","text":"Thanks to commit suggested by Rabscuttler Fixed issues with help text and installer","title":"v0.2.1"},{"location":"projects/clip_ship_planet_cli/#v020","text":"Fixed issues with config files","title":"v0.2.0"},{"location":"projects/clip_ship_planet_cli/#v019","text":"Now handles running and succeeded status better Now enumerates during clip and download to allow user estimates on number of assets clipped and/or downloaded","title":"v0.1.9"},{"location":"projects/clip_ship_planet_cli/#v018","text":"Includes required packages list within installer Robust GEOJSON Parsing","title":"v0.1.8"},{"location":"projects/clip_ship_planet_cli/#v017","text":"Fixed issues with processing visual asset types The Clip function now handles error codes if the post response code is not 202(accepted for processing) then the error code and item and asset type is printed.","title":"v0.1.7"},{"location":"projects/clip_ship_planet_cli/#v016","text":"Handles single time input API Key, this is needed only once to start the program Fixed issue with base metadata folder during sort Updated asset argument for asset activation to match styles","title":"v0.1.6"},{"location":"projects/clip_ship_planet_cli/#v015","text":"Updated Requirements.txt to include pyshp Fixed subprocess shell error, for now shell=True","title":"v0.1.5"},{"location":"projects/clip_ship_planet_cli/#v014","text":"General Improvements","title":"v0.1.4"},{"location":"projects/clip_ship_planet_cli/#v013","text":"General Improvements","title":"v0.1.3"},{"location":"projects/clip_ship_planet_cli/#v012","text":"Tested on Ubuntu 16.04 and now handles permissions problem Temporary files now written to config folders to avoid admin permission","title":"v0.1.2"},{"location":"projects/clip_ship_planet_cli/#v011","text":"General Improvements","title":"v0.1.1"},{"location":"projects/gee2drive/","text":"gee2drive: Download Earth Engine Public and Private assets to Google Drive Google Earth Engine currently allows you to export images and assets as either GeoTiff files or TFrecords. The system splits the files if the estimated size is greater than 2GB which is the upper limit and needs the geometry to be parsed in the form of either a fusion table, a user drawn geometry or a table imported into the user's assets. While the javascript frontend is great owing to the queryable catalog whereby you can search and and export your personal and private assets, the limitation lies in batch exports. To resolve this the python API access allows you to call batch export functions but now it is limited to checking for itersects first and running without having a queryable catalog. With the same idea I created this tool which allows you to run a terminal environment where your personal and general catalog images are part of a autosuggest feature. This tool allows you to look for images based on names for example \" you can search for Sentinel and it will show you full path of images which have the word sentinel in the title\". It also creates a report for your image collections and images so apart from the public datasets this can also find your own datasets as well. You can then generate bandlist to make sure all bands you are exporting are of the same type and then export all images that intersect you aoi. The assumption here is Every image in the give image have the same band structure, choose the bandlist that you know to common to all images If the geomery is too complex use the operator feature to use a bounding box instead. * For now all it filters is geometry and date, and it is does not filter based on metadata (however in the examples folder I have shown how to import and use additional filter before exporting an image collection) In the future I will try to integrate some other functionalities to this environment and you can indeed run the tool without the use of the autosuggest terminal as a simple CLI. Hence the terminal feature is optional. Table of contents Installation Getting started Google Earth Engine to Drive Manager GEE to Google Drive CLI gee2drive Terminal gee2drive refresh gee2drive idsearch gee2drive bandtype gee2drive export Installation This assumes that you have native python pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying. This assumes that you are also well aware of Google Earth Engine Python setup and have it installed and authetenticated on your system. If not you can read about it here python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have tested this only on python 2.7.15 but can be easily modified for python 3. To install Python CLI for Digital Ocean you can install using two methods pip install gee2drive or you can also try git clone https://github.com/samapriya/gee2drive.git cd gee2drive python setup.py install Use might have to use sudo privileges Installation is an optional step; the application can be also run directly by executing gee2drive.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. If you don't want to install, browse into the gee2drive folder and try python gee2drive.py to get to the same result. Getting started As usual, to print help: usage: gee2drive [-h] {terminal,refresh,idsearch,bandtype,export} ... Google Earth Engine to Drive Exporter positional arguments: {terminal,refresh,idsearch,bandtype,export} terminal Starts the interactive terminal with autosuggest refresh Refreshes your personal asset list and GEE Asset list idsearch Does possible matches using asset name to give you asseth id/full path bandtype Prints bandtype and generates list to be used for export export Export Collections based on filter optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: gee2drive idsearch -h . If you didn't install gee2drive, then you can run it just by going to gee2drive directory and running python gee2drive.py [arguments go here] GEE to Google Drive CLI This tool is designed to augment to the existing facilty of image export using a CLI, whereby you can pass it arguments to filter based on an area of interest geojson file, a start and end date for collection gee2drive Terminal This is an autosuggestive terminal which uses the gee2add package to perform all of the functions but has autosuggest for Earth Engine catalog and your own personal catalog. This way you can get access to image id without needing the catalog id in the javascript codeeditor. usage: gee2drive terminal [-h] optional arguments: -h, --help show this help message and exit Once you type gee2drive terminal you get a shell inside your current terminal where you get autosuggest for image and have full functionality of the terminal. gee2drive refresh For the past couple of months I have maintained a catalog of the most current Google Earth Engine assets , within their raster data catalog. I update this list every week. This tool downloads the most current version of this list, and also looks into your personal assets to generate your very own asset report which then serve as a master dataset to feed into autosuggestions. gee2drive refresh -h usage: gee2drive refresh [-h] optional arguments: -h, --help show this help message and exit gee2drive idsearch There is a possibility that you don't really remember the full path to your asset or the public asset. Fortunately when I parse and collect the image list and path for you they have names that are searchable so use a keyword. for example search using \"MODIS\" or \"sentinel\". Also it is not case sensitive, so you should be able to type \"SENTINEl\" or \"Sentinel\" or \"sentinel\" and it should still work gee2drive idsearch -h usage: gee2drive idsearch [-h] [--name NAME] optional arguments: -h, --help show this help message and exit --name NAME Name or part of name to search for gee2drive bandtype Export requires all the bandtypes to be of the same kind. To do this, I simply generate the band types for you and you can select the band list you want , remember to paste it as a list. usage: gee2drive bandtype [-h] [--id ID] optional arguments: -h, --help show this help message and exit --id ID full path for collection or image gee2drive export Finally the export tool, that lets you export an image or a collection clipped to your AOI. This makes use of the bandlist you exported. Incase you are exporting an image and not a collection you don't need a start and end date. The tool uses the bounds() function to use a bounding box incase the geometry has a complex geometry or too many vertices simply use the operator bb . If the geojson/json/kml keeps giving parsing error go to geojson.io usage: gee2drive export [-h] [--id ID] [--type TYPE] [--folder FOLDER] [--aoi AOI] [--start START] [--end END] [--bandlist BANDLIST] [--operator OPERATOR] optional arguments: -h, --help show this help message and exit --id ID Full path for collection or image --type TYPE Type whether image or collection --folder FOLDER Drive folder path --aoi AOI Full path to geojson/json/kml to be used for bounds Optional named arguments for image collection only: --start START Start date to filter image --end END End date to filter image --bandlist BANDLIST Bandlist we generated from bandtype export must be same bandtype --operator OPERATOR Use bb for Bounding box incase the geometry is complex or has too many vertices A typical setup would be ```gee2drive export --id \"COPERNICUS/S2\" --folder \"sentinel-export\" --aoi \"C:\\Users\\sam\\boulder.geojson\" --start \"2018-02-01\" --end \"2018-03-01\" --bandlist ['B2','B3','B4'] --operator \"bb\" --type \"collection\" Also as promised earlier , there is a way to add additional filters and then pass it through the export function here is how and I have included this in the Examples folder. This for example uses the Landsat collection but applies the Cloud cover filter before passing it for export ```python import ee import os import sys import gee2drive [head,tail]=os.path.split(gee2drive.__file__) os.chdir(head) sys.path.append(head) from export import exp ee.Initialize() exp(collection=ee.ImageCollection('LANDSAT/LC08/C01/T1_SR').filterMetadata('CLOUD_COVER','less_than',20), folderpath= l8-out ,start= 2018-02-01 ,end= 2018-06-01 , geojson=r C:\\Users\\sam\\boulder.geojson ,bandnames= ['B1','B2'] , operator= bb ,typ= ImageCollection ) Changelog v0.0.4 Can now parse gejson, json,kml Minor fixes and general improvements v0.0.3 Minor Fixes","title":"Google Earth Engine to Drive Export Manager"},{"location":"projects/gee2drive/#gee2drive-download-earth-engine-public-and-private-assets-to-google-drive","text":"Google Earth Engine currently allows you to export images and assets as either GeoTiff files or TFrecords. The system splits the files if the estimated size is greater than 2GB which is the upper limit and needs the geometry to be parsed in the form of either a fusion table, a user drawn geometry or a table imported into the user's assets. While the javascript frontend is great owing to the queryable catalog whereby you can search and and export your personal and private assets, the limitation lies in batch exports. To resolve this the python API access allows you to call batch export functions but now it is limited to checking for itersects first and running without having a queryable catalog. With the same idea I created this tool which allows you to run a terminal environment where your personal and general catalog images are part of a autosuggest feature. This tool allows you to look for images based on names for example \" you can search for Sentinel and it will show you full path of images which have the word sentinel in the title\". It also creates a report for your image collections and images so apart from the public datasets this can also find your own datasets as well. You can then generate bandlist to make sure all bands you are exporting are of the same type and then export all images that intersect you aoi. The assumption here is Every image in the give image have the same band structure, choose the bandlist that you know to common to all images If the geomery is too complex use the operator feature to use a bounding box instead. * For now all it filters is geometry and date, and it is does not filter based on metadata (however in the examples folder I have shown how to import and use additional filter before exporting an image collection) In the future I will try to integrate some other functionalities to this environment and you can indeed run the tool without the use of the autosuggest terminal as a simple CLI. Hence the terminal feature is optional.","title":"gee2drive: Download Earth Engine Public and Private assets to Google Drive"},{"location":"projects/gee2drive/#table-of-contents","text":"Installation Getting started Google Earth Engine to Drive Manager GEE to Google Drive CLI gee2drive Terminal gee2drive refresh gee2drive idsearch gee2drive bandtype gee2drive export","title":"Table of contents"},{"location":"projects/gee2drive/#installation","text":"This assumes that you have native python pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying. This assumes that you are also well aware of Google Earth Engine Python setup and have it installed and authetenticated on your system. If not you can read about it here python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have tested this only on python 2.7.15 but can be easily modified for python 3. To install Python CLI for Digital Ocean you can install using two methods pip install gee2drive or you can also try git clone https://github.com/samapriya/gee2drive.git cd gee2drive python setup.py install Use might have to use sudo privileges Installation is an optional step; the application can be also run directly by executing gee2drive.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. If you don't want to install, browse into the gee2drive folder and try python gee2drive.py to get to the same result.","title":"Installation"},{"location":"projects/gee2drive/#getting-started","text":"As usual, to print help: usage: gee2drive [-h] {terminal,refresh,idsearch,bandtype,export} ... Google Earth Engine to Drive Exporter positional arguments: {terminal,refresh,idsearch,bandtype,export} terminal Starts the interactive terminal with autosuggest refresh Refreshes your personal asset list and GEE Asset list idsearch Does possible matches using asset name to give you asseth id/full path bandtype Prints bandtype and generates list to be used for export export Export Collections based on filter optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: gee2drive idsearch -h . If you didn't install gee2drive, then you can run it just by going to gee2drive directory and running python gee2drive.py [arguments go here]","title":"Getting started"},{"location":"projects/gee2drive/#gee-to-google-drive-cli","text":"This tool is designed to augment to the existing facilty of image export using a CLI, whereby you can pass it arguments to filter based on an area of interest geojson file, a start and end date for collection","title":"GEE to Google Drive CLI"},{"location":"projects/gee2drive/#gee2drive-terminal","text":"This is an autosuggestive terminal which uses the gee2add package to perform all of the functions but has autosuggest for Earth Engine catalog and your own personal catalog. This way you can get access to image id without needing the catalog id in the javascript codeeditor. usage: gee2drive terminal [-h] optional arguments: -h, --help show this help message and exit Once you type gee2drive terminal you get a shell inside your current terminal where you get autosuggest for image and have full functionality of the terminal.","title":"gee2drive Terminal"},{"location":"projects/gee2drive/#gee2drive-refresh","text":"For the past couple of months I have maintained a catalog of the most current Google Earth Engine assets , within their raster data catalog. I update this list every week. This tool downloads the most current version of this list, and also looks into your personal assets to generate your very own asset report which then serve as a master dataset to feed into autosuggestions. gee2drive refresh -h usage: gee2drive refresh [-h] optional arguments: -h, --help show this help message and exit","title":"gee2drive refresh"},{"location":"projects/gee2drive/#gee2drive-idsearch","text":"There is a possibility that you don't really remember the full path to your asset or the public asset. Fortunately when I parse and collect the image list and path for you they have names that are searchable so use a keyword. for example search using \"MODIS\" or \"sentinel\". Also it is not case sensitive, so you should be able to type \"SENTINEl\" or \"Sentinel\" or \"sentinel\" and it should still work gee2drive idsearch -h usage: gee2drive idsearch [-h] [--name NAME] optional arguments: -h, --help show this help message and exit --name NAME Name or part of name to search for","title":"gee2drive idsearch"},{"location":"projects/gee2drive/#gee2drive-bandtype","text":"Export requires all the bandtypes to be of the same kind. To do this, I simply generate the band types for you and you can select the band list you want , remember to paste it as a list. usage: gee2drive bandtype [-h] [--id ID] optional arguments: -h, --help show this help message and exit --id ID full path for collection or image","title":"gee2drive bandtype"},{"location":"projects/gee2drive/#gee2drive-export","text":"Finally the export tool, that lets you export an image or a collection clipped to your AOI. This makes use of the bandlist you exported. Incase you are exporting an image and not a collection you don't need a start and end date. The tool uses the bounds() function to use a bounding box incase the geometry has a complex geometry or too many vertices simply use the operator bb . If the geojson/json/kml keeps giving parsing error go to geojson.io usage: gee2drive export [-h] [--id ID] [--type TYPE] [--folder FOLDER] [--aoi AOI] [--start START] [--end END] [--bandlist BANDLIST] [--operator OPERATOR] optional arguments: -h, --help show this help message and exit --id ID Full path for collection or image --type TYPE Type whether image or collection --folder FOLDER Drive folder path --aoi AOI Full path to geojson/json/kml to be used for bounds Optional named arguments for image collection only: --start START Start date to filter image --end END End date to filter image --bandlist BANDLIST Bandlist we generated from bandtype export must be same bandtype --operator OPERATOR Use bb for Bounding box incase the geometry is complex or has too many vertices A typical setup would be ```gee2drive export --id \"COPERNICUS/S2\" --folder \"sentinel-export\" --aoi \"C:\\Users\\sam\\boulder.geojson\" --start \"2018-02-01\" --end \"2018-03-01\" --bandlist ['B2','B3','B4'] --operator \"bb\" --type \"collection\" Also as promised earlier , there is a way to add additional filters and then pass it through the export function here is how and I have included this in the Examples folder. This for example uses the Landsat collection but applies the Cloud cover filter before passing it for export ```python import ee import os import sys import gee2drive [head,tail]=os.path.split(gee2drive.__file__) os.chdir(head) sys.path.append(head) from export import exp ee.Initialize() exp(collection=ee.ImageCollection('LANDSAT/LC08/C01/T1_SR').filterMetadata('CLOUD_COVER','less_than',20), folderpath= l8-out ,start= 2018-02-01 ,end= 2018-06-01 , geojson=r C:\\Users\\sam\\boulder.geojson ,bandnames= ['B1','B2'] , operator= bb ,typ= ImageCollection )","title":"gee2drive export"},{"location":"projects/gee2drive/#changelog","text":"","title":"Changelog"},{"location":"projects/gee2drive/#v004","text":"Can now parse gejson, json,kml Minor fixes and general improvements","title":"v0.0.4"},{"location":"projects/gee2drive/#v003","text":"Minor Fixes","title":"v0.0.3"},{"location":"projects/gee_asset_manager_addon/","text":"Google Earth Engine Batch Asset Manager with Addons Google Earth Engine Batch Asset Manager with Addons is an extension of the one developed by Lukasz here and additional tools were added to include functionality for moving assets, conversion of objects to fusion table, cleaning folders, querying tasks. The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. tab. Table of contents Installation Getting started Batch uploader Parsing metadata Usage examples EE User Create Upload a directory with images and associate properties with each image: Upload a directory with images with specific NoData value to a selected destination: Asset List Earth Engine Asset Report Task Query Task Query during ingestion Task Report Delete a collection with content: Assets Move Assets Copy Assets Access Set Collection Property Cleanup Utility Cancel all tasks Installation We assume Earth Engine Python API is installed and EE authorised as desribed here . To install: git clone https://github.com/samapriya/gee_asset_manager_addon cd gee_asset_manager pip install . Installation is an optional step; the application can be also run directly by executing geeadd.py script. The advantage of having it installed is being able to execute geeadd as any command line tool. I recommend installation within virtual environment. To install run python setup.py develop or python setup.py install In a linux distribution sudo python setup.py develop or sudo python setup.py install Getting started As usual, to print help: usage: geeadd.py [-h] {ee_user,create,upload,lst,ee_report,collsize,tasks,taskquery,report,delete,mover,copy,access,collprop,cleanout,cancel} ... Google Earth Engine Batch Asset Manager with Addons positional arguments: {ee_user,create,upload,lst,ee_report,collsize,tasks,taskquery,report,delete,mover,copy,access,collprop,cleanout,cancel} ee_user Allows you to associate/change GEE account to system create Allows the user to create an asset collection or folder in Google Earth Engine upload Batch Asset Uploader. lst List assets in a folder/collection or write as text file ee_report Prints a detailed report of all Earth Engine Assets includes Asset Type, Path,Number of Assets,size(MB),unit,owner,readers,writers collsize Collects collection size in Human Readable form Number of assets tasks Queries currently running, enqued,failed taskquery Queries currently running, enqued,failed ingestions and uploaded assets report Create a report of all tasks and exports to a CSV file delete Deletes collection and all items inside. Supports Unix-like wildcards. mover Moves all assets from one collection to another copy Copies all assets from one collection to another: Including copying from other users if you have read permission to their assets access Sets Permissions for Images, Collection or all assets in EE Folder Example: python ee_permissions.py --mode folder --asset users/john/doe --user jimmy@doe.com:R collprop Sets Overall Properties for Image Collection cleanout Clear folders with datasets from earlier downloaded cancel Cancel all running tasks optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: geeadd upload -h . If you didn't install geeadd, then you can run it just by going to geeadd directory and running python geeadd.py [arguments go here] Batch uploader The script creates an Image Collection from GeoTIFFs in your local directory. By default, the collection name is the same as the local directory name; with optional parameter you can provide a different name. Another optional parameter is a path to a CSV file with metadata for images, which is covered in the next section: Parsing metadata . usage: geeadd.py upload [-h] -u USER --source SOURCE --dest DEST [-m METADATA] [--large] [--nodata NODATA] optional arguments: -h, --help show this help message and exit Required named arguments.: -u USER, --user USER Google account name (gmail address). --source SOURCE Path to the directory with images for upload. --dest DEST Destination. Full path for upload to Google Earth Engine, e.g. users/johndoe/myponycollection Optional named arguments: -m METADATA, --metadata METADATA Path to CSV with metadata. --large (Advanced) Use multipart upload. Might help if upload of large files is failing on some systems. Might cause other issues. --nodata NODATA The value to burn into the raster as NoData (missing data) Parsing metadata By metadata we understand here the properties associated with each image. Thanks to these, GEE user can easily filter collection based on specified criteria. The file with metadata should be organised as follows: filename (without extension) property1 header property2 header file1 value1 value2 file2 value3 value4 Note that header can contain only letters, digits and underscores. Example: id_no class category binomial system:time_start my_file_1 GASTROPODA EN Aaadonta constricta 1478943081000 my_file_2 GASTROPODA CR Aaadonta irregularis 1478943081000 The corresponding files are my_file_1.tif and my_file_2.tif. With each of the files five properties are associated: id_no, class, category, binomial and system:time_start. The latter is time in Unix epoch format, in milliseconds, as documented in GEE glosary. The program will match the file names from the upload directory with ones provided in the CSV and pass the metadata in JSON format: { id_no: my_file_1, class: GASTROPODA, category: EN, binomial: Aaadonta constricta, system:time_start: 1478943081000} The program will report any illegal fields, it will also complain if not all of the images passed for upload have metadata associated. User can opt to ignore it, in which case some assets will have no properties. Having metadata helps in organising your asstets, but is not mandatory - you can skip it. Usage examples EE User This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back Create This tool allows you to create a collection or folder in your earth engine root directory. The tool uses the system cli to achieve this and this has been included so as to reduce the need to switch between multiple tools and CLI. usage: ppipe.py create [-h] --typ TYP --path PATH optional arguments: -h, --help show this help message and exit --typ TYP Specify type: collection or folder --path PATH This is the path for the earth engine asset to be created full path is needsed eg: users/johndoe/collection Upload a directory with images to your myfolder/mycollection and associate properties with each image: geeadd upload -u johndoe@gmail.com --source path_to_directory_with_tif -m path_to_metadata.csv --dest users/johndoe/myfolder/myponycollection The script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option. Upload a directory with images with specific NoData value to a selected destination geeadd upload -u johndoe@gmail.com --source path_to_directory_with_tif --dest users/johndoe/myfolder/myponycollection --nodata 222 In this case we need to supply full path to the destination, which is helpful when we upload to a shared folder. In the provided example we also burn value 222 into all rasters for missing data (NoData). Asset List This tool is designed to either print or output asset lists within folders or collections using earthengine ls tool functions. usage: geeadd lst [-h] --location LOCATION --type TYPE [--items ITEMS] optional arguments: -h, --help show this help message and exit --location LOCATION This it the location of your folder/collection --type TYPE Whether you want the list to be printed or output as text(print/report) --items ITEMS Number of items to list Earth Engine Asset Report This tool recursively goes through all your assets(Includes Images, ImageCollection,Table,) and generates a report containing the following fields [Type,Asset Type, Path,Number of Assets,size(MB),unit,owner,readers,writers]. usage: geeadd.py ee_report [-h] --outfile OUTFILE optional arguments: -h, --help show this help message and exit --outfile OUTFILE This it the location of your report csv file A simple setup is the following geeadd --outfile \"C:\\johndoe\\report.csv\" Task Query This script counts all currently running and ready tasks along with failed tasks. usage: geeadd.py tasks [-h] optional arguments: -h, --help show this help message and exit geeadd.py tasks Task Query during ingestion This script can be used intermittently to look at running, failed and ready(waiting) tasks during ingestion. This script is a special case using query tasks only when uploading assets to collection by providing collection pathway to see how collection size increases. usage: geeadd.py taskquery [-h] [--destination DESTINATION] optional arguments: -h, --help show this help message and exit --destination DESTINATION Full path to asset where you are uploading files geeadd.py taskquery users/johndoe/myfolder/myponycollection Task Report Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type usage: geeadd.py report [-h] [--r R] [--e E] optional arguments: -h, --help show this help message and exit --r R Path CSV filename where the report will be saved --e E Path CSV filename where the errorlog will be saved geeadd.py report --r report.csv --e errorlog.csv Delete a collection with content: The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution! geeadd delete users/johndoe/test Console output: 2016-07-17 16:14:09,212 :: oauth2client.client :: INFO :: Attempting refresh to obtain initial access_token 2016-07-17 16:14:09,213 :: oauth2client.client :: INFO :: Refreshing access_token 2016-07-17 16:14:10,842 :: root :: INFO :: Attempting to delete collection test 2016-07-17 16:14:16,898 :: root :: INFO :: Collection users/johndoe/test removed Delete all directories / collections based on a Unix-like pattern geeadd delete users/johndoe/*weird[0-9]?name* Assets Move This script allows us to recursively move assets from one collection to the other. usage: geeadd.py mover [-h] [--assetpath ASSETPATH] [--finalpath FINALPATH] optional arguments: -h, --help show this help message and exit --assetpath ASSETPATH Existing path of assets --finalpath FINALPATH New path for assets geeadd.py mover --assetpath users/johndoe/myfolder/myponycollection --destination users/johndoe/myfolder/myotherponycollection Assets Copy This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections. usage: geeadd.py copy [-h] [--initial INITIAL] [--final FINAL] optional arguments: -h, --help show this help message and exit --initial INITIAL Existing path of assets --final FINAL New path for assets geeadd.py mover --initial users/johndoe/myfolder/myponycollection --final users/johndoe/myfolder/myotherponycollection Assets Access This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time. usage: geeadd access [-h] --mode MODE --asset ASSET --user USER optional arguments: -h, --help show this help message and exit --mode MODE This lets you select if you want to change permission or folder/collection/image --asset ASSET This is the path to the earth engine asset whose permission you are changing folder/collection/image --user USER This is the email address to whom you want to give read or write permission Usage: john@doe.com:R or john@doe.com:W R/W refers to read or write permission geeadd.py access --mode folder --asset folder/collection/image --user john@doe.com:R Set Collection Property This script is derived from the ee tool to set collection properties and will set overall properties for collection. usage: geeadd.py collprop [-h] [--coll COLL] [--p P] optional arguments: -h, --help show this help message and exit --coll COLL Path of Image Collection --p P system:description=Description / system:provider_url=url / sys tem:tags=tags / system:title=title Cleanup Utility This script is used to clean folders once all processes have been completed. In short this is a function to clear folder on local machine. usage: geeadd.py cleanout [-h] [--dirpath DIRPATH] optional arguments: -h, --help show this help message and exit --dirpath DIRPATH Folder you want to delete after all processes have been completed geeadd.py cleanout --dirpath ./folder Cancel all tasks This is a simpler tool, can be called directly from the earthengine cli as well earthengine cli command earthengine task cancel all usage: geeadd.py cancel [-h] optional arguments: -h, --help show this help message and exit Changelog v0.1.9 Added Earth Engine Asset Report Tool General improvements v0.1.8 Fixed issues with install Dependcies now part of setup.py Updated Parser and general improvements","title":"Google Earth Engine Asset Manager Addon"},{"location":"projects/gee_asset_manager_addon/#google-earth-engine-batch-asset-manager-with-addons","text":"Google Earth Engine Batch Asset Manager with Addons is an extension of the one developed by Lukasz here and additional tools were added to include functionality for moving assets, conversion of objects to fusion table, cleaning folders, querying tasks. The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. tab.","title":"Google Earth Engine Batch Asset Manager with Addons"},{"location":"projects/gee_asset_manager_addon/#table-of-contents","text":"Installation Getting started Batch uploader Parsing metadata Usage examples EE User Create Upload a directory with images and associate properties with each image: Upload a directory with images with specific NoData value to a selected destination: Asset List Earth Engine Asset Report Task Query Task Query during ingestion Task Report Delete a collection with content: Assets Move Assets Copy Assets Access Set Collection Property Cleanup Utility Cancel all tasks","title":"Table of contents"},{"location":"projects/gee_asset_manager_addon/#installation","text":"We assume Earth Engine Python API is installed and EE authorised as desribed here . To install: git clone https://github.com/samapriya/gee_asset_manager_addon cd gee_asset_manager pip install . Installation is an optional step; the application can be also run directly by executing geeadd.py script. The advantage of having it installed is being able to execute geeadd as any command line tool. I recommend installation within virtual environment. To install run python setup.py develop or python setup.py install In a linux distribution sudo python setup.py develop or sudo python setup.py install","title":"Installation"},{"location":"projects/gee_asset_manager_addon/#getting-started","text":"As usual, to print help: usage: geeadd.py [-h] {ee_user,create,upload,lst,ee_report,collsize,tasks,taskquery,report,delete,mover,copy,access,collprop,cleanout,cancel} ... Google Earth Engine Batch Asset Manager with Addons positional arguments: {ee_user,create,upload,lst,ee_report,collsize,tasks,taskquery,report,delete,mover,copy,access,collprop,cleanout,cancel} ee_user Allows you to associate/change GEE account to system create Allows the user to create an asset collection or folder in Google Earth Engine upload Batch Asset Uploader. lst List assets in a folder/collection or write as text file ee_report Prints a detailed report of all Earth Engine Assets includes Asset Type, Path,Number of Assets,size(MB),unit,owner,readers,writers collsize Collects collection size in Human Readable form Number of assets tasks Queries currently running, enqued,failed taskquery Queries currently running, enqued,failed ingestions and uploaded assets report Create a report of all tasks and exports to a CSV file delete Deletes collection and all items inside. Supports Unix-like wildcards. mover Moves all assets from one collection to another copy Copies all assets from one collection to another: Including copying from other users if you have read permission to their assets access Sets Permissions for Images, Collection or all assets in EE Folder Example: python ee_permissions.py --mode folder --asset users/john/doe --user jimmy@doe.com:R collprop Sets Overall Properties for Image Collection cleanout Clear folders with datasets from earlier downloaded cancel Cancel all running tasks optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: geeadd upload -h . If you didn't install geeadd, then you can run it just by going to geeadd directory and running python geeadd.py [arguments go here]","title":"Getting started"},{"location":"projects/gee_asset_manager_addon/#batch-uploader","text":"The script creates an Image Collection from GeoTIFFs in your local directory. By default, the collection name is the same as the local directory name; with optional parameter you can provide a different name. Another optional parameter is a path to a CSV file with metadata for images, which is covered in the next section: Parsing metadata . usage: geeadd.py upload [-h] -u USER --source SOURCE --dest DEST [-m METADATA] [--large] [--nodata NODATA] optional arguments: -h, --help show this help message and exit Required named arguments.: -u USER, --user USER Google account name (gmail address). --source SOURCE Path to the directory with images for upload. --dest DEST Destination. Full path for upload to Google Earth Engine, e.g. users/johndoe/myponycollection Optional named arguments: -m METADATA, --metadata METADATA Path to CSV with metadata. --large (Advanced) Use multipart upload. Might help if upload of large files is failing on some systems. Might cause other issues. --nodata NODATA The value to burn into the raster as NoData (missing data)","title":"Batch uploader"},{"location":"projects/gee_asset_manager_addon/#parsing-metadata","text":"By metadata we understand here the properties associated with each image. Thanks to these, GEE user can easily filter collection based on specified criteria. The file with metadata should be organised as follows: filename (without extension) property1 header property2 header file1 value1 value2 file2 value3 value4 Note that header can contain only letters, digits and underscores. Example: id_no class category binomial system:time_start my_file_1 GASTROPODA EN Aaadonta constricta 1478943081000 my_file_2 GASTROPODA CR Aaadonta irregularis 1478943081000 The corresponding files are my_file_1.tif and my_file_2.tif. With each of the files five properties are associated: id_no, class, category, binomial and system:time_start. The latter is time in Unix epoch format, in milliseconds, as documented in GEE glosary. The program will match the file names from the upload directory with ones provided in the CSV and pass the metadata in JSON format: { id_no: my_file_1, class: GASTROPODA, category: EN, binomial: Aaadonta constricta, system:time_start: 1478943081000} The program will report any illegal fields, it will also complain if not all of the images passed for upload have metadata associated. User can opt to ignore it, in which case some assets will have no properties. Having metadata helps in organising your asstets, but is not mandatory - you can skip it.","title":"Parsing metadata"},{"location":"projects/gee_asset_manager_addon/#usage-examples","text":"","title":"Usage examples"},{"location":"projects/gee_asset_manager_addon/#ee-user","text":"This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back","title":"EE User"},{"location":"projects/gee_asset_manager_addon/#create","text":"This tool allows you to create a collection or folder in your earth engine root directory. The tool uses the system cli to achieve this and this has been included so as to reduce the need to switch between multiple tools and CLI. usage: ppipe.py create [-h] --typ TYP --path PATH optional arguments: -h, --help show this help message and exit --typ TYP Specify type: collection or folder --path PATH This is the path for the earth engine asset to be created full path is needsed eg: users/johndoe/collection","title":"Create"},{"location":"projects/gee_asset_manager_addon/#upload-a-directory-with-images-to-your-myfoldermycollection-and-associate-properties-with-each-image","text":"geeadd upload -u johndoe@gmail.com --source path_to_directory_with_tif -m path_to_metadata.csv --dest users/johndoe/myfolder/myponycollection The script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.","title":"Upload a directory with images to your myfolder/mycollection and associate properties with each image:"},{"location":"projects/gee_asset_manager_addon/#upload-a-directory-with-images-with-specific-nodata-value-to-a-selected-destination","text":"geeadd upload -u johndoe@gmail.com --source path_to_directory_with_tif --dest users/johndoe/myfolder/myponycollection --nodata 222 In this case we need to supply full path to the destination, which is helpful when we upload to a shared folder. In the provided example we also burn value 222 into all rasters for missing data (NoData).","title":"Upload a directory with images with specific NoData value to a selected destination"},{"location":"projects/gee_asset_manager_addon/#asset-list","text":"This tool is designed to either print or output asset lists within folders or collections using earthengine ls tool functions. usage: geeadd lst [-h] --location LOCATION --type TYPE [--items ITEMS] optional arguments: -h, --help show this help message and exit --location LOCATION This it the location of your folder/collection --type TYPE Whether you want the list to be printed or output as text(print/report) --items ITEMS Number of items to list","title":"Asset List"},{"location":"projects/gee_asset_manager_addon/#earth-engine-asset-report","text":"This tool recursively goes through all your assets(Includes Images, ImageCollection,Table,) and generates a report containing the following fields [Type,Asset Type, Path,Number of Assets,size(MB),unit,owner,readers,writers]. usage: geeadd.py ee_report [-h] --outfile OUTFILE optional arguments: -h, --help show this help message and exit --outfile OUTFILE This it the location of your report csv file A simple setup is the following geeadd --outfile \"C:\\johndoe\\report.csv\"","title":"Earth Engine Asset Report"},{"location":"projects/gee_asset_manager_addon/#task-query","text":"This script counts all currently running and ready tasks along with failed tasks. usage: geeadd.py tasks [-h] optional arguments: -h, --help show this help message and exit geeadd.py tasks","title":"Task Query"},{"location":"projects/gee_asset_manager_addon/#task-query-during-ingestion","text":"This script can be used intermittently to look at running, failed and ready(waiting) tasks during ingestion. This script is a special case using query tasks only when uploading assets to collection by providing collection pathway to see how collection size increases. usage: geeadd.py taskquery [-h] [--destination DESTINATION] optional arguments: -h, --help show this help message and exit --destination DESTINATION Full path to asset where you are uploading files geeadd.py taskquery users/johndoe/myfolder/myponycollection","title":"Task Query during ingestion"},{"location":"projects/gee_asset_manager_addon/#task-report","text":"Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type usage: geeadd.py report [-h] [--r R] [--e E] optional arguments: -h, --help show this help message and exit --r R Path CSV filename where the report will be saved --e E Path CSV filename where the errorlog will be saved geeadd.py report --r report.csv --e errorlog.csv","title":"Task Report"},{"location":"projects/gee_asset_manager_addon/#delete-a-collection-with-content","text":"The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution! geeadd delete users/johndoe/test Console output: 2016-07-17 16:14:09,212 :: oauth2client.client :: INFO :: Attempting refresh to obtain initial access_token 2016-07-17 16:14:09,213 :: oauth2client.client :: INFO :: Refreshing access_token 2016-07-17 16:14:10,842 :: root :: INFO :: Attempting to delete collection test 2016-07-17 16:14:16,898 :: root :: INFO :: Collection users/johndoe/test removed","title":"Delete a collection with content:"},{"location":"projects/gee_asset_manager_addon/#delete-all-directories-collections-based-on-a-unix-like-pattern","text":"geeadd delete users/johndoe/*weird[0-9]?name*","title":"Delete all directories / collections based on a Unix-like pattern"},{"location":"projects/gee_asset_manager_addon/#assets-move","text":"This script allows us to recursively move assets from one collection to the other. usage: geeadd.py mover [-h] [--assetpath ASSETPATH] [--finalpath FINALPATH] optional arguments: -h, --help show this help message and exit --assetpath ASSETPATH Existing path of assets --finalpath FINALPATH New path for assets geeadd.py mover --assetpath users/johndoe/myfolder/myponycollection --destination users/johndoe/myfolder/myotherponycollection","title":"Assets Move"},{"location":"projects/gee_asset_manager_addon/#assets-copy","text":"This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections. usage: geeadd.py copy [-h] [--initial INITIAL] [--final FINAL] optional arguments: -h, --help show this help message and exit --initial INITIAL Existing path of assets --final FINAL New path for assets geeadd.py mover --initial users/johndoe/myfolder/myponycollection --final users/johndoe/myfolder/myotherponycollection","title":"Assets Copy"},{"location":"projects/gee_asset_manager_addon/#assets-access","text":"This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time. usage: geeadd access [-h] --mode MODE --asset ASSET --user USER optional arguments: -h, --help show this help message and exit --mode MODE This lets you select if you want to change permission or folder/collection/image --asset ASSET This is the path to the earth engine asset whose permission you are changing folder/collection/image --user USER This is the email address to whom you want to give read or write permission Usage: john@doe.com:R or john@doe.com:W R/W refers to read or write permission geeadd.py access --mode folder --asset folder/collection/image --user john@doe.com:R","title":"Assets Access"},{"location":"projects/gee_asset_manager_addon/#set-collection-property","text":"This script is derived from the ee tool to set collection properties and will set overall properties for collection. usage: geeadd.py collprop [-h] [--coll COLL] [--p P] optional arguments: -h, --help show this help message and exit --coll COLL Path of Image Collection --p P system:description=Description / system:provider_url=url / sys tem:tags=tags / system:title=title","title":"Set Collection Property"},{"location":"projects/gee_asset_manager_addon/#cleanup-utility","text":"This script is used to clean folders once all processes have been completed. In short this is a function to clear folder on local machine. usage: geeadd.py cleanout [-h] [--dirpath DIRPATH] optional arguments: -h, --help show this help message and exit --dirpath DIRPATH Folder you want to delete after all processes have been completed geeadd.py cleanout --dirpath ./folder","title":"Cleanup Utility"},{"location":"projects/gee_asset_manager_addon/#cancel-all-tasks","text":"This is a simpler tool, can be called directly from the earthengine cli as well earthengine cli command earthengine task cancel all usage: geeadd.py cancel [-h] optional arguments: -h, --help show this help message and exit","title":"Cancel all tasks"},{"location":"projects/gee_asset_manager_addon/#changelog","text":"","title":"Changelog"},{"location":"projects/gee_asset_manager_addon/#v019","text":"Added Earth Engine Asset Report Tool General improvements","title":"v0.1.9"},{"location":"projects/gee_asset_manager_addon/#v018","text":"Fixed issues with install Dependcies now part of setup.py Updated Parser and general improvements","title":"v0.1.8"},{"location":"projects/gee_takeout/","text":"Google Takeout and Transfer: Tools and Guide for Code and Asset Transfer Command Line Interface Allows you to copy all codes and assets from one Google account to another Note: This is something that I have tested and have designed only for a windows machine with python 2.7.14 but can be easily ported into an different operating system. Use these tool and steps at your own risk and backup your scripts always just in case. If you still want to proceed which I assume you do in case you are still reading, I am including descriptions links to the tool I made and the steps I used to achieve the same. The tool is a single command line interface with three sections. Before you do this make sure of a few things Both your google accounts have an external password, since it requires that to download and perform a lot of the operations. Also enable __Let Less Secure App use your account_ on both these accounts._ Your system has native python available in terminal or command prompt depending on what kind of system you are using. You can check this by typing python --version _Git is installed on your system. For windows you can find __installation here_ _Earth Engine Command Line(earthengine cli) interface is installed, instructions are in the __developer page_ You have authenticated earthengine cli using earthengine authenticate Make sure you visit the __git source for your account_ within earth engine and allow access._ Check git is accessible via your system path type git help and check if the system can reach installed git command line tools. Now we setup and install the tool by running the following set of steps. The Github repository containing this tool and codes can be found here For windows download the zip here and after extraction go to the folder containing \"setup.py\" and open command prompt at that location and type python setup.py install pip install -r requirements.txt Now call the tool for the first time, by typing in geetakeout -h . This will only work if you have python in the system path which you can test for opening up terminal or command prompt and typing **python**. If you want, you don't have to install the tool to use it you can browse to the folder geetakeout inside the main zipped folder. You can then call the tool by simple python geetakeout.py -h The housekeeping and credential setup is optional since most of you have probably installed the earthengine cli and authenticated it using **earthengine authenticate**. Anatomy of the Process: How to transfer step by step Getting first things out of the way is to understand the three sections of this tool. To make life and this process simpler I designed the tool to have a flow so you can run these tools one after the other. The EE Setup and Housekeeping sections are optional , since I will generally updated the selenium driver for mozilla and it assumes you have authenticated your earthengine CLI . The tool might show an error if you have not authenticated using earthengine authenticate If you have installed the tool run geetakeout -h If you have migrated into the folder python geetakeout.py -h The GEE Takeout Tool CLI printout Setting up the case study For this blog I decided to make the transfer simple I have a university account but since my university if shifting umail services to a google app service it means my domain would change from @umail.iu.edu to @iu.edu which are separate accounts. I created the iu.edu GEE account recently. Code Editor comparison Left(my @umail account and right my @iu account) This also means that the root path for my home folder and repository are different. The idea is simple to be able to replicate the codes and assets from one account to the other. This includes every type if assets including collections but also making sure that the structure of the folders are same. That being said you will still have to change the home path in the codes but if the structure is same then only a single root-path changes. Comparing the root path and assets folder for both accounts So now that we have a setup, I am going to approach it step by step and have a walk through to explain the process better. Step 1: Getting your Repository Lists(gee_repo) This assumes that you have visited the Git Source for your codes in Google Earth Engine and authorized it. If not allow it here and then you are set to download your repo contents and perform git operations. The tool is setup for accessing all repositories that are shared with you. This downloads the list into an html file which can then be parsed for your repositories. Create GEE Repo List Step 2: Setting up your Git with Earth Engine Credentials(git_auth) You can do this using two methods The first simple includes you visiting your gitsource account page that we accessed earlier and click **Generate Password ** and follow instructions. I am going to talk more about the second method because this eliminates the need to get the password again and again since it is save as passkey. This will authenticate your git client with your git password using a browser less login and also store your gitkey GIT Auth (saves git key) We will use this again to setup our second account post authorization. This will print our gitkey location and make sure you copy that so you can swap in out as needed. Note the name of the key is in the format **git-\"username\"** in this case it is **git-roysam** Step 3: Authorize your Git Client with Git Key(git_swap) The next step is to use the saved gitkey to authorize the git client. We are setting everything up so that we can clone the repositories to which we have access. Authorize using the git key stored earlier Step 4: Clone your repositories(git_clone) This tool makes use of your earlier created GIT list, now that your git client has been authorized in step 2, you should be able to download your repos. This tool uses the account already linked to your terminal account. If you are not sure try **earthengine ls** to see your username. The export path is noted for the collection of repositories. Cloning your Git Repositories Step 5: Working with Assets: Generating Asset Report(ee_report) This includes all your assets , including tables, images, image collections and folders. We need to make sure we have this list to work on copying over your assets to the secondary account. Running this is simple and just requires a location for the csv file (the full path). Running Earth Engine Reports The output is a csv file consiting of the type of asset and the asset path to be replicated in the new account. And now that we have the list time to get permissions to copy these assets. Step 6: Setting Permissions to Assets(ee_permissions) We now use the report file generated to grant read access to all assets in your account. Once this is completed you will be able to copy your assets apart from being able to copy your codes. Getting permissions to assets Let us Begin to Copy : We change gears and switch over to the destination account Step 7: Setting up the Destination Account(ee_user and git_swap) Now we have to do two steps one after the other, do a quick earthengine authenticate and authenticate to your new account and perform Step 2 and Step 3 this time using your new account. The tool **ee_user** will also allow you to change your accounts. I already created Step 2 for my secondary account and now I will use that to authorize my git client with the new account. Change your earthengine authentication and also validate your git key Now we authorized the git client with the second key Step 8: Replicate Repositories (git_create) To setup our new account we need to build the outline of the earlier account, the repolists and folders inside these repos and then similary the folders and empty collections in the secondary account. Note: Git cannot push an empty repository so if you have an empty repository delete it before downloading and pushing to new account Git Create your folder based on your earlier account The repo lists now look similar Repo created on secondary account Step 9: Push to New Account(git_replicate) Now we push all codes from our earlier account to our new account, this way our repositories will now be populated with the most recent codes. \"Do not push to any repository that already has code because this will overwrite it\" git_replicate to new account Step 10: Replicate Asset Structure(asset_create) and Assets (asset_replicate) This is similar to git_create here we replicate the collection and folder structure so we can push our assets to them. You pass it the original report you created from your primary account and it sets up as needed. Creates asset structure (folders and collections) This has replicated the collection and Image folder structures. Asset Collections and Folders have been created(Left: asset home before asset_create Right: asset home after asset_create) However this is still empty and the last step makes sure that your assets are actually copied over to your new asset home. I have included a counter to measure transfers left incase it is a large collection. Asset Replication: Copying assets to your home folder The final results is your assets and codes all copied, you will still have to edit codes to change your path as needed but for now we have replicated an Earth Engine account into a new location. Replicated Assets on Both Accounts Copied from Left to Right There you go, over the last 10 steps we have managed to replicate and move an earth engine account from one place to another. Though I found this useful to move accounts within a university setting, I see some value in moving accounts and replicating when a project member leaves a project or for simply migrating at large. For now if an owner of an account deletes his/her account or looses access to his/her account and even if you are a writer to the repository and the collection, you will loose access to these codes and assets. So this can aid in maintaining continuity by moving codes to more persistent account. Though I have not tested this tools in a linux setting, these setup tools can be adapted and used easily in that framework, since I have tested the individual components in such setups.","title":"Google Earth Engine Takeout"},{"location":"projects/gee_takeout/#google-takeout-and-transfer-tools-and-guide-for-code-and-asset-transfer","text":"Command Line Interface Allows you to copy all codes and assets from one Google account to another Note: This is something that I have tested and have designed only for a windows machine with python 2.7.14 but can be easily ported into an different operating system. Use these tool and steps at your own risk and backup your scripts always just in case. If you still want to proceed which I assume you do in case you are still reading, I am including descriptions links to the tool I made and the steps I used to achieve the same. The tool is a single command line interface with three sections. Before you do this make sure of a few things Both your google accounts have an external password, since it requires that to download and perform a lot of the operations. Also enable __Let Less Secure App use your account_ on both these accounts._ Your system has native python available in terminal or command prompt depending on what kind of system you are using. You can check this by typing python --version _Git is installed on your system. For windows you can find __installation here_ _Earth Engine Command Line(earthengine cli) interface is installed, instructions are in the __developer page_ You have authenticated earthengine cli using earthengine authenticate Make sure you visit the __git source for your account_ within earth engine and allow access._ Check git is accessible via your system path type git help and check if the system can reach installed git command line tools. Now we setup and install the tool by running the following set of steps. The Github repository containing this tool and codes can be found here For windows download the zip here and after extraction go to the folder containing \"setup.py\" and open command prompt at that location and type python setup.py install pip install -r requirements.txt Now call the tool for the first time, by typing in geetakeout -h . This will only work if you have python in the system path which you can test for opening up terminal or command prompt and typing **python**. If you want, you don't have to install the tool to use it you can browse to the folder geetakeout inside the main zipped folder. You can then call the tool by simple python geetakeout.py -h The housekeeping and credential setup is optional since most of you have probably installed the earthengine cli and authenticated it using **earthengine authenticate**. Anatomy of the Process: How to transfer step by step Getting first things out of the way is to understand the three sections of this tool. To make life and this process simpler I designed the tool to have a flow so you can run these tools one after the other. The EE Setup and Housekeeping sections are optional , since I will generally updated the selenium driver for mozilla and it assumes you have authenticated your earthengine CLI . The tool might show an error if you have not authenticated using earthengine authenticate If you have installed the tool run geetakeout -h If you have migrated into the folder python geetakeout.py -h The GEE Takeout Tool CLI printout Setting up the case study For this blog I decided to make the transfer simple I have a university account but since my university if shifting umail services to a google app service it means my domain would change from @umail.iu.edu to @iu.edu which are separate accounts. I created the iu.edu GEE account recently. Code Editor comparison Left(my @umail account and right my @iu account) This also means that the root path for my home folder and repository are different. The idea is simple to be able to replicate the codes and assets from one account to the other. This includes every type if assets including collections but also making sure that the structure of the folders are same. That being said you will still have to change the home path in the codes but if the structure is same then only a single root-path changes. Comparing the root path and assets folder for both accounts So now that we have a setup, I am going to approach it step by step and have a walk through to explain the process better. Step 1: Getting your Repository Lists(gee_repo) This assumes that you have visited the Git Source for your codes in Google Earth Engine and authorized it. If not allow it here and then you are set to download your repo contents and perform git operations. The tool is setup for accessing all repositories that are shared with you. This downloads the list into an html file which can then be parsed for your repositories. Create GEE Repo List Step 2: Setting up your Git with Earth Engine Credentials(git_auth) You can do this using two methods The first simple includes you visiting your gitsource account page that we accessed earlier and click **Generate Password ** and follow instructions. I am going to talk more about the second method because this eliminates the need to get the password again and again since it is save as passkey. This will authenticate your git client with your git password using a browser less login and also store your gitkey GIT Auth (saves git key) We will use this again to setup our second account post authorization. This will print our gitkey location and make sure you copy that so you can swap in out as needed. Note the name of the key is in the format **git-\"username\"** in this case it is **git-roysam** Step 3: Authorize your Git Client with Git Key(git_swap) The next step is to use the saved gitkey to authorize the git client. We are setting everything up so that we can clone the repositories to which we have access. Authorize using the git key stored earlier Step 4: Clone your repositories(git_clone) This tool makes use of your earlier created GIT list, now that your git client has been authorized in step 2, you should be able to download your repos. This tool uses the account already linked to your terminal account. If you are not sure try **earthengine ls** to see your username. The export path is noted for the collection of repositories. Cloning your Git Repositories Step 5: Working with Assets: Generating Asset Report(ee_report) This includes all your assets , including tables, images, image collections and folders. We need to make sure we have this list to work on copying over your assets to the secondary account. Running this is simple and just requires a location for the csv file (the full path). Running Earth Engine Reports The output is a csv file consiting of the type of asset and the asset path to be replicated in the new account. And now that we have the list time to get permissions to copy these assets. Step 6: Setting Permissions to Assets(ee_permissions) We now use the report file generated to grant read access to all assets in your account. Once this is completed you will be able to copy your assets apart from being able to copy your codes. Getting permissions to assets Let us Begin to Copy : We change gears and switch over to the destination account Step 7: Setting up the Destination Account(ee_user and git_swap) Now we have to do two steps one after the other, do a quick earthengine authenticate and authenticate to your new account and perform Step 2 and Step 3 this time using your new account. The tool **ee_user** will also allow you to change your accounts. I already created Step 2 for my secondary account and now I will use that to authorize my git client with the new account. Change your earthengine authentication and also validate your git key Now we authorized the git client with the second key Step 8: Replicate Repositories (git_create) To setup our new account we need to build the outline of the earlier account, the repolists and folders inside these repos and then similary the folders and empty collections in the secondary account. Note: Git cannot push an empty repository so if you have an empty repository delete it before downloading and pushing to new account Git Create your folder based on your earlier account The repo lists now look similar Repo created on secondary account Step 9: Push to New Account(git_replicate) Now we push all codes from our earlier account to our new account, this way our repositories will now be populated with the most recent codes. \"Do not push to any repository that already has code because this will overwrite it\" git_replicate to new account Step 10: Replicate Asset Structure(asset_create) and Assets (asset_replicate) This is similar to git_create here we replicate the collection and folder structure so we can push our assets to them. You pass it the original report you created from your primary account and it sets up as needed. Creates asset structure (folders and collections) This has replicated the collection and Image folder structures. Asset Collections and Folders have been created(Left: asset home before asset_create Right: asset home after asset_create) However this is still empty and the last step makes sure that your assets are actually copied over to your new asset home. I have included a counter to measure transfers left incase it is a large collection. Asset Replication: Copying assets to your home folder The final results is your assets and codes all copied, you will still have to edit codes to change your path as needed but for now we have replicated an Earth Engine account into a new location. Replicated Assets on Both Accounts Copied from Left to Right There you go, over the last 10 steps we have managed to replicate and move an earth engine account from one place to another. Though I found this useful to move accounts within a university setting, I see some value in moving accounts and replicating when a project member leaves a project or for simply migrating at large. For now if an owner of an account deletes his/her account or looses access to his/her account and even if you are a writer to the repository and the collection, you will loose access to these codes and assets. So this can aid in maintaining continuity by moving codes to more persistent account. Though I have not tested this tools in a linux setting, these setup tools can be adapted and used easily in that framework, since I have tested the individual components in such setups.","title":"Google Takeout and Transfer: Tools and Guide for Code and Asset Transfer"},{"location":"projects/geeup/","text":"geeup: Simple CLI for Earth Engine Uploads This came of the simple need to handle batch uploads of both image assets to collections but also thanks to the new table feature the possibility of batch uploading shapefiles into a folder. Though a lot of these tools including batch image uploader is part of my other project geeadd which also includes additional features to add to the python CLI, this tool was designed to be minimal so as to allow the user to simply query thier quota, upload images or tables and also to query ongoing tasks and delete assets. I am hoping this tool with a simple objective proves useful to a few users of Google Earth Engine. Table of contents Installation Getting started geeup Simple CLI for Earth Engine Uploads gee Quota gee Zipshape gee upload gee table upload gee tasks gee delete Installation This assumes that you have native python pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have tested this only on python 2.7.15 but it should run on python 3. This also needs earthengine cli to be installed and authenticated on your system and earthengine to be callable in your command line or terminal To install geeup: Simple CLI for Earth Engine Uploads you can install using two methods pip install geeup or you can also try git clone https://github.com/samapriya/geeup.git cd geeup python setup.py install For linux use sudo. Installation is an optional step; the application can be also run directly by executing geeup.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. If you don't want to install, browse into the geeup folder and try python geeup.py to get to the same result. Getting started As usual, to print help: usage: geeup [-h] {quota,zipshape,upload,tabup,tasks,delete} ... Simple Client for Earth Engine Uploads positional arguments: {quota,zipshape,upload,tabup,tasks,delete} quota Print Earth Engine total quota and used quota zipshape Zips all shapefiles and subsidary files into individual zip files upload Batch Asset Uploader. tabup Batch Table Uploader. tasks Queries current task status [completed,running,ready,failed,cancelled] delete Deletes collection and all items inside. Supports Unix-like wildcards. optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: geeup dropinfo -h . If you didn't install geeup, then you can run it just by going to geeup directory and running python geeup.py [arguments go here] geeup Simple CLI for Earth Engine Uploads The tool is designed to handle batch uploading of images and tables(shapefiles). While there are image collection where you can batch upload imagery,for vector or shapefiles you have to batch upload them to a folder. gee Quota Just a simple tool to print your earth engine quota quickly. usage: geeup quota [-h] optional arguments: -h, --help show this help message and exit gee Zipshape So here's how table upload in Google Earth Engine works, you can either upload the component files shp, shx, prj and dbf or you can zip these files together and upload it as a single file. The pros for this is that it reduces the overall size of the shapefile after zipping them together, this tool looks for the shp file and finds the subsidary files and zips them ready for upload. It also helps when your have limited upload bandwith. Cons you have to create a replicate structure of the file system, but it saves on bandwidth and auto arranges your files so you don't have to look for each additional file. usage: geeup zipshape [-h] --input INPUT --output OUTPUT optional arguments: -h, --help show this help message and exit Required named arguments.: --input INPUT Path to the input directory with all shape files --output OUTPUT Destination folder Full path where shp, shx, prj and dbf files if present in input will be zipped and stored gee upload The script creates an Image Collection from GeoTIFFs in your local directory. By default, the collection name is the same as the local directory name; with optional parameter you can provide a different name. usage: geeup upload [-h] --source SOURCE --dest DEST [-m METADATA] [--large] [--nodata NODATA] [--bands BANDS] [-u USER] [-b BUCKET] optional arguments: -h, --help show this help message and exit Required named arguments.: --source SOURCE Path to the directory with images for upload. --dest DEST Destination. Full path for upload to Google Earth Engine, e.g. users/pinkiepie/myponycollection -u USER, --user USER Google account name (gmail address). Optional named arguments: -m METADATA, --metadata METADATA Path to CSV with metadata. --large (Advanced) Use multipart upload. Might help if upload of large files is failing on some systems. Might cause other issues. --nodata NODATA The value to burn into the raster as NoData (missing data) --bands BANDS Comma-separated list of names to use for the image bands. Spacesor other special characters are not allowed. -b BUCKET, --bucket BUCKET Google Cloud Storage bucket name. gee table upload This tool allows you to batch download tables/shapefiles to a folder. It uses a modified version of the image upload and a wrapper around the earthengine upload cli to achieve this while creating folders if they don't exist and reporting on assets and checking on uploads. This only requires a source, destination and your ee authenticated email address. usage: geeup tabup [-h] --source SOURCE --dest DEST [-u USER] optional arguments: -h, --help show this help message and exit Required named arguments.: --source SOURCE Path to the directory with zipped folder for upload. --dest DEST Destination. Full path for upload to Google Earth Engine, e.g. users/pinkiepie/myponycollection -u USER, --user USER Google account name (gmail address). gee tasks This script counts all currently running,ready,completed,failed and cancelled tasks along with failed tasks. This is linked to the account you initialized with your google earth engine account. This takes no argument. usage: geeup tasks [-h] optional arguments: -h, --help show this help message and exit gee delete The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution! usage: geeup delete [-h] id positional arguments: id Full path to asset for deletion. Recursively removes all folders, collections and images. optional arguments: -h, --help show this help message and exit","title":"Simple CLI for Google Earth Engine Uploads"},{"location":"projects/geeup/#geeup-simple-cli-for-earth-engine-uploads","text":"This came of the simple need to handle batch uploads of both image assets to collections but also thanks to the new table feature the possibility of batch uploading shapefiles into a folder. Though a lot of these tools including batch image uploader is part of my other project geeadd which also includes additional features to add to the python CLI, this tool was designed to be minimal so as to allow the user to simply query thier quota, upload images or tables and also to query ongoing tasks and delete assets. I am hoping this tool with a simple objective proves useful to a few users of Google Earth Engine.","title":"geeup: Simple CLI for Earth Engine Uploads"},{"location":"projects/geeup/#table-of-contents","text":"Installation Getting started geeup Simple CLI for Earth Engine Uploads gee Quota gee Zipshape gee upload gee table upload gee tasks gee delete","title":"Table of contents"},{"location":"projects/geeup/#installation","text":"This assumes that you have native python pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have tested this only on python 2.7.15 but it should run on python 3. This also needs earthengine cli to be installed and authenticated on your system and earthengine to be callable in your command line or terminal To install geeup: Simple CLI for Earth Engine Uploads you can install using two methods pip install geeup or you can also try git clone https://github.com/samapriya/geeup.git cd geeup python setup.py install For linux use sudo. Installation is an optional step; the application can be also run directly by executing geeup.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. If you don't want to install, browse into the geeup folder and try python geeup.py to get to the same result.","title":"Installation"},{"location":"projects/geeup/#getting-started","text":"As usual, to print help: usage: geeup [-h] {quota,zipshape,upload,tabup,tasks,delete} ... Simple Client for Earth Engine Uploads positional arguments: {quota,zipshape,upload,tabup,tasks,delete} quota Print Earth Engine total quota and used quota zipshape Zips all shapefiles and subsidary files into individual zip files upload Batch Asset Uploader. tabup Batch Table Uploader. tasks Queries current task status [completed,running,ready,failed,cancelled] delete Deletes collection and all items inside. Supports Unix-like wildcards. optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: geeup dropinfo -h . If you didn't install geeup, then you can run it just by going to geeup directory and running python geeup.py [arguments go here]","title":"Getting started"},{"location":"projects/geeup/#geeup-simple-cli-for-earth-engine-uploads_1","text":"The tool is designed to handle batch uploading of images and tables(shapefiles). While there are image collection where you can batch upload imagery,for vector or shapefiles you have to batch upload them to a folder.","title":"geeup Simple CLI for Earth Engine Uploads"},{"location":"projects/geeup/#gee-quota","text":"Just a simple tool to print your earth engine quota quickly. usage: geeup quota [-h] optional arguments: -h, --help show this help message and exit","title":"gee Quota"},{"location":"projects/geeup/#gee-zipshape","text":"So here's how table upload in Google Earth Engine works, you can either upload the component files shp, shx, prj and dbf or you can zip these files together and upload it as a single file. The pros for this is that it reduces the overall size of the shapefile after zipping them together, this tool looks for the shp file and finds the subsidary files and zips them ready for upload. It also helps when your have limited upload bandwith. Cons you have to create a replicate structure of the file system, but it saves on bandwidth and auto arranges your files so you don't have to look for each additional file. usage: geeup zipshape [-h] --input INPUT --output OUTPUT optional arguments: -h, --help show this help message and exit Required named arguments.: --input INPUT Path to the input directory with all shape files --output OUTPUT Destination folder Full path where shp, shx, prj and dbf files if present in input will be zipped and stored","title":"gee Zipshape"},{"location":"projects/geeup/#gee-upload","text":"The script creates an Image Collection from GeoTIFFs in your local directory. By default, the collection name is the same as the local directory name; with optional parameter you can provide a different name. usage: geeup upload [-h] --source SOURCE --dest DEST [-m METADATA] [--large] [--nodata NODATA] [--bands BANDS] [-u USER] [-b BUCKET] optional arguments: -h, --help show this help message and exit Required named arguments.: --source SOURCE Path to the directory with images for upload. --dest DEST Destination. Full path for upload to Google Earth Engine, e.g. users/pinkiepie/myponycollection -u USER, --user USER Google account name (gmail address). Optional named arguments: -m METADATA, --metadata METADATA Path to CSV with metadata. --large (Advanced) Use multipart upload. Might help if upload of large files is failing on some systems. Might cause other issues. --nodata NODATA The value to burn into the raster as NoData (missing data) --bands BANDS Comma-separated list of names to use for the image bands. Spacesor other special characters are not allowed. -b BUCKET, --bucket BUCKET Google Cloud Storage bucket name.","title":"gee upload"},{"location":"projects/geeup/#gee-table-upload","text":"This tool allows you to batch download tables/shapefiles to a folder. It uses a modified version of the image upload and a wrapper around the earthengine upload cli to achieve this while creating folders if they don't exist and reporting on assets and checking on uploads. This only requires a source, destination and your ee authenticated email address. usage: geeup tabup [-h] --source SOURCE --dest DEST [-u USER] optional arguments: -h, --help show this help message and exit Required named arguments.: --source SOURCE Path to the directory with zipped folder for upload. --dest DEST Destination. Full path for upload to Google Earth Engine, e.g. users/pinkiepie/myponycollection -u USER, --user USER Google account name (gmail address).","title":"gee table upload"},{"location":"projects/geeup/#gee-tasks","text":"This script counts all currently running,ready,completed,failed and cancelled tasks along with failed tasks. This is linked to the account you initialized with your google earth engine account. This takes no argument. usage: geeup tasks [-h] optional arguments: -h, --help show this help message and exit","title":"gee tasks"},{"location":"projects/geeup/#gee-delete","text":"The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution! usage: geeup delete [-h] id positional arguments: id Full path to asset for deletion. Recursively removes all folders, collections and images. optional arguments: -h, --help show this help message and exit","title":"gee delete"},{"location":"projects/jetstream_unofficial_cli/","text":"Jetstream Unofficial Addon for Atmosphere VM(s) Jetstream provides researchers and students with a unique approach to cloud computing and data analysis on demand. Moving away from a job submission model which is more pervasice on High Performance Cluster computing this allows users to create user-friendly installs of \u201cvirtual machines\u201d which can then be custom installed with softwares and packages and even imaged further to make science replicable. The system in theory also allows a BYoOS (Bring your own Operating System) designed to further allow more flexibility while sharing the backbone of high performance compute and cloud storage infrastructure. The idea of imaging a system allows the user to not only share their data or analysis but their entire setup that allows them to run the analysis making it a plug and play model in terms of research replication and transference. As the NSF project proceeds further it allows users to apply for grants or allocation time on the machines and indeed renew these free of cost for researchers and students. For my work looking at developing a Satellite Imagery Input Output (IO) pipeline I was allocated a Jetstream grant and while using a couple of these systems I always thought it would be easier to query number of instances and volumes running on my account, my burn rate and left over allocation over sometime. And something useful was the possibility to perform system actions such as shutdown(stop), restart, start, reboot among others programatically. It is with this idea that I approach the API backend at JetStream and built just a few tools to allows users to perform these actions without the need to log into their web browsers and allows user to call upon instance actions through a rough command line interface (CLI). I would love to thank Steve Gregory who is getting the Official API ready for release and Jeremy Fischer who have helped me in more ways and allowed me to keep asking questions and learn from them to build the unofficial api for Jetstream. The Jetstream's mission and funding is supported by NSF and any and all citations are useful so please make sure to cite and citation information can be found at the end of this readme or use the link Note: I have used this on the Jetstream-IU system but it should work on the TACC end as well and the information I decided to show per instance is based on what I used, there are much more information generated but the tool prints a subset Table of contents Getting started Save API Password as Credential Query Current Instances Query Current Volumes Perform Instance Actions Getting started To get started you need an XSEDE account of a Jetstream Rapid Access Account and you can create one using instructions here . Once into the system you will still need an allocation and the wiki page setup by the project explains these along with everything you can do step by step here . Once you have a project allocation and create some instances and volumes you can query and perform instance actions. Just browse to the folder and perform a python jetstream.py -h : usage: jetstream.py [-h] { ,jskey,instance,volume,action} ... JetStream API Unofficial positional arguments: { ,jskey,instance,volume,action} ------------------------------------------- -----Choose from JetStream Tools Below----- ------------------------------------------- jskey Allows you to save your JetStream API Password instance Allows users to print out all instance information volume Allows users to print out all volume information action Allows user to start, suspend,resume,reboot instance optional arguments: -h, --help show this help message and exit Save API Password as Credential This tool allows the user to save the credential or password file into users/.config/jetstream making sure that they are user specific and are not shared on a system resource or location. This password is not your XSEDE or Jetstream Rapid Access account password but a API password which has to be requested atleast for now. It uses a getpass implementation and write the password as a csv and the other tools first tries to read this and if not present asks for your password. usage: jetstream.py jskey [-h] optional arguments: -h, --help show this help message and exit Query Current Instances As the name stated this allows the user to query instances related to your account. This queries all instances for now and prints certain key parameters(these are a subset that I selected for my use) but can be easily modified in the code to print other information. usage: jetstream.py instance [-h] [--username USERNAME] [--password PASSWORD] optional arguments: -h, --help show this help message and exit --username USERNAME Jetstream API username --password PASSWORD Jetstream API password: Optional if you already saved jetstream key Incase you have already saved your password a setup would be simply python jetstream.py instance --username johndoe if not python jetstream.py instance --username johndoe --password pass Query Current Volumes The current volume options queries current volumes on your project. Note that the current info does not tell you which instance a volume is attached to which might be a nice endpoint readout to have. usage: jetstream.py volume [-h] [--username USERNAME] [--password PASSWORD] optional arguments: -h, --help show this help message and exit --username USERNAME Jetstream API username --password PASSWORD Jetstream API password: Optional if you already saved jetstream key Incase you have already saved your password a setup would be simply python jetstream.py volume --username johndoe if not python jetstream.py volume --username johndoe --password pass Perform Instance Actions This is perhaps one of the most important aspect of building this tool, the idea was to get a read out or a email message when a system has been idle for sometime and then auto shutdown or initiate a shutdown remotely. This command allows you to choose the instance you want to perform an instance action using the instance ID mentioned on your instance profile page. The next thing you need to know is what kind of action you want to perform. There are certain things that will create a conflict, for example: If you are trying to shutdown an already shutoff system or start an already active system. In that case it will print out a conflict message. usage: jetstream.py action [-h] [--username USERNAME] [--password PASSWORD] [--id ID] [--action ACTION] optional arguments: -h, --help show this help message and exit --username USERNAME Jetstream API username --password PASSWORD Jetstream API password: Optional if you already saved jetstream key --id ID Jetstream Instance ID on your Instance Detail Page --action ACTION Jetstream Instance Action, start|stop|suspend|resume|reboot Incase you have already saved your password a setup would be simply python jetstream.py action --username johndoe --id 00000 --action start if not python jetstream.py action --username johndoe --password pass --id 00000 --action start I would like to thank my grant from JetStream TG-GEO160014. You can cite this tool as Samapriya Roy. (2017, August 16). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon. Zenodo. http://doi.org/10.5281/zenodo.844018 And I would like to include Jetstream citations for others to use Stewart, C.A., Cockerill, T.M., Foster, I., Hancock, D., Merchant, N., Skidmore, E., Stanzione, D., Taylor, J., Tuecke, S., Turner, G., Vaughn, M., and Gaffney, N.I., Jetstream: a self-provisioned, scalable science and engineering cloud environment. 2015, In Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure. St. Louis, Missouri. ACM: 2792774. p. 1-8. http://dx.doi.org/10.1145/2792745.2792774 and John Towns, Timothy Cockerill, Maytal Dahan, Ian Foster, Kelly Gaither, Andrew Grimshaw, Victor Hazlewood, Scott Lathrop, Dave Lifka, Gregory D. Peterson, Ralph Roskies, J. Ray Scott, Nancy Wilkins-Diehr, \"XSEDE: Accelerating Scientific Discovery\", Computing in Science Engineering, vol.16, no. 5, pp. 62-74, Sept.-Oct. 2014, doi:10.1109/MCSE.2014.80","title":"Jetstream Unofficial Addon"},{"location":"projects/jetstream_unofficial_cli/#jetstream-unofficial-addon-for-atmosphere-vms","text":"Jetstream provides researchers and students with a unique approach to cloud computing and data analysis on demand. Moving away from a job submission model which is more pervasice on High Performance Cluster computing this allows users to create user-friendly installs of \u201cvirtual machines\u201d which can then be custom installed with softwares and packages and even imaged further to make science replicable. The system in theory also allows a BYoOS (Bring your own Operating System) designed to further allow more flexibility while sharing the backbone of high performance compute and cloud storage infrastructure. The idea of imaging a system allows the user to not only share their data or analysis but their entire setup that allows them to run the analysis making it a plug and play model in terms of research replication and transference. As the NSF project proceeds further it allows users to apply for grants or allocation time on the machines and indeed renew these free of cost for researchers and students. For my work looking at developing a Satellite Imagery Input Output (IO) pipeline I was allocated a Jetstream grant and while using a couple of these systems I always thought it would be easier to query number of instances and volumes running on my account, my burn rate and left over allocation over sometime. And something useful was the possibility to perform system actions such as shutdown(stop), restart, start, reboot among others programatically. It is with this idea that I approach the API backend at JetStream and built just a few tools to allows users to perform these actions without the need to log into their web browsers and allows user to call upon instance actions through a rough command line interface (CLI). I would love to thank Steve Gregory who is getting the Official API ready for release and Jeremy Fischer who have helped me in more ways and allowed me to keep asking questions and learn from them to build the unofficial api for Jetstream. The Jetstream's mission and funding is supported by NSF and any and all citations are useful so please make sure to cite and citation information can be found at the end of this readme or use the link Note: I have used this on the Jetstream-IU system but it should work on the TACC end as well and the information I decided to show per instance is based on what I used, there are much more information generated but the tool prints a subset","title":"Jetstream Unofficial Addon for Atmosphere VM(s)"},{"location":"projects/jetstream_unofficial_cli/#table-of-contents","text":"Getting started Save API Password as Credential Query Current Instances Query Current Volumes Perform Instance Actions","title":"Table of contents"},{"location":"projects/jetstream_unofficial_cli/#getting-started","text":"To get started you need an XSEDE account of a Jetstream Rapid Access Account and you can create one using instructions here . Once into the system you will still need an allocation and the wiki page setup by the project explains these along with everything you can do step by step here . Once you have a project allocation and create some instances and volumes you can query and perform instance actions. Just browse to the folder and perform a python jetstream.py -h : usage: jetstream.py [-h] { ,jskey,instance,volume,action} ... JetStream API Unofficial positional arguments: { ,jskey,instance,volume,action} ------------------------------------------- -----Choose from JetStream Tools Below----- ------------------------------------------- jskey Allows you to save your JetStream API Password instance Allows users to print out all instance information volume Allows users to print out all volume information action Allows user to start, suspend,resume,reboot instance optional arguments: -h, --help show this help message and exit","title":"Getting started"},{"location":"projects/jetstream_unofficial_cli/#save-api-password-as-credential","text":"This tool allows the user to save the credential or password file into users/.config/jetstream making sure that they are user specific and are not shared on a system resource or location. This password is not your XSEDE or Jetstream Rapid Access account password but a API password which has to be requested atleast for now. It uses a getpass implementation and write the password as a csv and the other tools first tries to read this and if not present asks for your password. usage: jetstream.py jskey [-h] optional arguments: -h, --help show this help message and exit","title":"Save API Password as Credential"},{"location":"projects/jetstream_unofficial_cli/#query-current-instances","text":"As the name stated this allows the user to query instances related to your account. This queries all instances for now and prints certain key parameters(these are a subset that I selected for my use) but can be easily modified in the code to print other information. usage: jetstream.py instance [-h] [--username USERNAME] [--password PASSWORD] optional arguments: -h, --help show this help message and exit --username USERNAME Jetstream API username --password PASSWORD Jetstream API password: Optional if you already saved jetstream key Incase you have already saved your password a setup would be simply python jetstream.py instance --username johndoe if not python jetstream.py instance --username johndoe --password pass","title":"Query Current Instances"},{"location":"projects/jetstream_unofficial_cli/#query-current-volumes","text":"The current volume options queries current volumes on your project. Note that the current info does not tell you which instance a volume is attached to which might be a nice endpoint readout to have. usage: jetstream.py volume [-h] [--username USERNAME] [--password PASSWORD] optional arguments: -h, --help show this help message and exit --username USERNAME Jetstream API username --password PASSWORD Jetstream API password: Optional if you already saved jetstream key Incase you have already saved your password a setup would be simply python jetstream.py volume --username johndoe if not python jetstream.py volume --username johndoe --password pass","title":"Query Current Volumes"},{"location":"projects/jetstream_unofficial_cli/#perform-instance-actions","text":"This is perhaps one of the most important aspect of building this tool, the idea was to get a read out or a email message when a system has been idle for sometime and then auto shutdown or initiate a shutdown remotely. This command allows you to choose the instance you want to perform an instance action using the instance ID mentioned on your instance profile page. The next thing you need to know is what kind of action you want to perform. There are certain things that will create a conflict, for example: If you are trying to shutdown an already shutoff system or start an already active system. In that case it will print out a conflict message. usage: jetstream.py action [-h] [--username USERNAME] [--password PASSWORD] [--id ID] [--action ACTION] optional arguments: -h, --help show this help message and exit --username USERNAME Jetstream API username --password PASSWORD Jetstream API password: Optional if you already saved jetstream key --id ID Jetstream Instance ID on your Instance Detail Page --action ACTION Jetstream Instance Action, start|stop|suspend|resume|reboot Incase you have already saved your password a setup would be simply python jetstream.py action --username johndoe --id 00000 --action start if not python jetstream.py action --username johndoe --password pass --id 00000 --action start I would like to thank my grant from JetStream TG-GEO160014. You can cite this tool as Samapriya Roy. (2017, August 16). samapriya/jetstream-unofficial-addon: jetstream-unofficial-addon. Zenodo. http://doi.org/10.5281/zenodo.844018 And I would like to include Jetstream citations for others to use Stewart, C.A., Cockerill, T.M., Foster, I., Hancock, D., Merchant, N., Skidmore, E., Stanzione, D., Taylor, J., Tuecke, S., Turner, G., Vaughn, M., and Gaffney, N.I., Jetstream: a self-provisioned, scalable science and engineering cloud environment. 2015, In Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure. St. Louis, Missouri. ACM: 2792774. p. 1-8. http://dx.doi.org/10.1145/2792745.2792774 and John Towns, Timothy Cockerill, Maytal Dahan, Ian Foster, Kelly Gaither, Andrew Grimshaw, Victor Hazlewood, Scott Lathrop, Dave Lifka, Gregory D. Peterson, Ralph Roskies, J. Ray Scott, Nancy Wilkins-Diehr, \"XSEDE: Accelerating Scientific Discovery\", Computing in Science Engineering, vol.16, no. 5, pp. 62-74, Sept.-Oct. 2014, doi:10.1109/MCSE.2014.80","title":"Perform Instance Actions"},{"location":"projects/open-impact/","text":"Open Impact Training and Hackathons Open Impact projects include hackathons, training to support developer interaction and engagement of Education and Research users. You can click on the headings to go their page SatSummit 2018: Hands-on Satellite Imagery Processing and Analysis Presentations and more of the joined CSDMS - SEN 2018 annual meeting Geoprocesses, geohazards - CSDMS 2018. Held May 22-24th 2018 in Boulder Colorado, USA. The presentation was Introduction to Google Earth Engine Terra 2018 Pointcloud And Remote Sensing Workshop Pointcloud And Remote Sensing Workshop Aimed at undergraduates, postgraduates and researchers in Earth / Ocean sciences, Computing, private companies (environmental consultancy, geomatics, topographic, geodesic services), NGOs (conservation, environment) From august 14 to 16 2018, at CICESE on Ensenada, B.C. The presentation was Planet API, data download and processing in Google Earth Engine NEON Data Institute 2018: Remote Sensing with Reproducible Workflows using Python Presentations and more of the joined CSDMS - SEN 2018 annual meeting Geoprocesses, geohazards - CSDMS 2018. Held May 22-24th 2018 in Boulder Colorado, USA. The presentation was Introduction to Google Earth Engine CSDMS 2018-Introduction to Google Earth Engine Presentations and more of the joined CSDMS - SEN 2018 annual meeting Geoprocesses, geohazards - CSDMS 2018. Held May 22-24th 2018 in Boulder Colorado, USA. The presentation was Introduction to Google Earth Engine Stanford Big Earth Hackathon Over 100 students gathered with industry and faculty mentors on April 14-15, 2018 to hack for planet earth. Congrats to the participants for an exciting weekend of planetary solutions.","title":"Open Impact"},{"location":"projects/open-impact/#open-impact-training-and-hackathons","text":"Open Impact projects include hackathons, training to support developer interaction and engagement of Education and Research users. You can click on the headings to go their page","title":"Open Impact Training and Hackathons"},{"location":"projects/open-impact/#satsummit-2018-hands-on-satellite-imagery-processing-and-analysis","text":"Presentations and more of the joined CSDMS - SEN 2018 annual meeting Geoprocesses, geohazards - CSDMS 2018. Held May 22-24th 2018 in Boulder Colorado, USA. The presentation was Introduction to Google Earth Engine","title":"SatSummit 2018: Hands-on Satellite Imagery Processing and Analysis"},{"location":"projects/open-impact/#terra-2018-pointcloud-and-remote-sensing-workshop","text":"Pointcloud And Remote Sensing Workshop Aimed at undergraduates, postgraduates and researchers in Earth / Ocean sciences, Computing, private companies (environmental consultancy, geomatics, topographic, geodesic services), NGOs (conservation, environment) From august 14 to 16 2018, at CICESE on Ensenada, B.C. The presentation was Planet API, data download and processing in Google Earth Engine","title":"Terra 2018 Pointcloud And Remote Sensing Workshop"},{"location":"projects/open-impact/#neon-data-institute-2018-remote-sensing-with-reproducible-workflows-using-python","text":"Presentations and more of the joined CSDMS - SEN 2018 annual meeting Geoprocesses, geohazards - CSDMS 2018. Held May 22-24th 2018 in Boulder Colorado, USA. The presentation was Introduction to Google Earth Engine","title":"NEON Data Institute 2018: Remote Sensing with Reproducible Workflows using Python"},{"location":"projects/open-impact/#csdms-2018-introduction-to-google-earth-engine","text":"Presentations and more of the joined CSDMS - SEN 2018 annual meeting Geoprocesses, geohazards - CSDMS 2018. Held May 22-24th 2018 in Boulder Colorado, USA. The presentation was Introduction to Google Earth Engine","title":"CSDMS 2018-Introduction to Google Earth Engine"},{"location":"projects/open-impact/#stanford-big-earth-hackathon","text":"Over 100 students gathered with industry and faculty mentors on April 14-15, 2018 to hack for planet earth. Congrats to the participants for an exciting weekend of planetary solutions.","title":"Stanford Big Earth Hackathon"},{"location":"projects/planet_batch_slack/","text":"Planet Batch Slack Pipeline CLI Planet Labs(Full line up of Satellites) and Planet Slack Technologies Logo For writing a readme file this time I have adapted a shared piece written for the medium article. The first part which is setting up the slack account, creating an application and a slack bot has been discussed in the article here . In the past I have written tools which act as pipelines for you to process single areas of interest at the time that could be chained, the need to write something that does a bit more heavy lifting arose. This command line interface(CLI) application was created to handle groups and teams that have multiple areas of interest and multiple input and output buckets and locations to function smoothly. I have integrated this to slack so you can be on the move while this task can be on a scheduler and update you when finished. Table of contents Installation Getting started Batch Approach to Structured JSON Batch Activation Batch Download and Balance Additional Tools Citation and Credits Changelog Installation The next step we will setup the Planet-Batch-Slack-Pipeline-CLI and integrate our previously built slack app for notifications. To setup the prerequisites you need to install the Planet Python API Client and Slack Python API Clients. * To install the tool you can go to the GitHub page at Planet-Batch-Slack-Pipeline-CLI . As always two of my favorite operating systems are Windows and Linux, and to install on Linux git clone https://github.com/samapriya/Planet-Batch-Slack-Pipeline-CLI.git cd Planet-Batch-Slack-Pipeline-CLI sudo python setup.py install pip install -r requirements.txt for windows download the zip and after extraction go to the folder containing \"setup.py\" and open command prompt at that location and type python setup.py install pip install -r requirements.txt Now call the tool for the first time, by typing in pbatch -h . This will only work if you have python in the system path which you can test for opening up terminal or command prompt and typing python. Getting started Once the requirements have been satisfied the first thing we would setup would be the OAuth Keys we created. The tools consists of a bunch of slack tools as well including capability to just use this tool to send slack messages, attachments and clean up channel as needed. Planet Batch Tools and Slack Addons Interface The two critical setup tools to make Slack ready and integrated are the smain and sbot tools where you will enter the OAuth for the application and OAuth for the bot that you generated earlier. These are then stored into your session for future use, you can call them using pbatch smain Use the \" OAuth Access Token \" generated earlier . You can find the tutorial here pbatch sbot Use \" Bot User OAuth Access Token\" generated earlier . You can find the tutorial here Once this is done your bot is now setup to message you when a task is completed. In our case these are tied into individual tools within the batch toolkit we just installed. To be clear these tools were designed based on what I thought was an effective way of looking at data, downloading them and chaining the processes together. They are still a set of individual tools to make sure that one operation is independent of the other and does not break in case of a problem. So a non-monolithic design in some sense to make sure the pieces work. We will go through each of them in the order of use pbatch planetkey is the obvious one which is your planet API key and will allow you to store this locally to a session. The aoijson tool is the same tool used in the Planet-EE CLI within the pbatch bundle allows you to bring any existing KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles to a structured geojson file, this is so that the Planet API can read the structured geojson which has additional filters such as cloud cover and range of dates. The tool can then allow you to convert the geojson file to include filters to be used with the Planet Data API. Let us say we want to convert this map.geojson to a structured aoi.json from June 1 2017 to June 30th 2017 with 15% cloud cover as our maximum . We would pass the following command pbatch.py aoijson --start 2017-06-01 --end 2017-06-30 --cloud 0.15 --inputfile GJSON --geo local path to map.geojson file --loc path where aoi.json output file will be created Batch Approach to Structured JSON This tool was then rewritten and included in the application to overcome two issues 1) Automatically detect the type of input file I am passing (For now it can automatically handle geojson, kml, shapefile and wkt files). The files are then saved in an output directory with the filename_aoi.json . The tool can also read from a csv file and parse different start dates, end dates and cloud cover for different files and create structured jsons to multiple locations making an multi path export easy. The csv headers should be The csv file needs to have following headers and setup pathways start end cloud outdir C:\\demo\\dallas.geojson 2017-01-01 2017-01-02 0.15 C:\\demo C:\\demo\\denver.geojson 2017-01-01 2017-03-02 0.15 C:\\demo C:\\demo\\sfo.geojson 2017-01-01 2017-05-02 0.15 C:\\demo C:\\demo\\indy.geojson 2017-01-01 2017-09-02 0.15 C:\\demo Below is a folder based batch execution to convert multiple geojson files to structured json files Batch Activation This tool was rewritten to provide users with two options to activate their assets. They can either point the tool at a folder and select the item and asset combination or they can specify a CSV file which contains each asset and item type and path to the structured JSON file. A setup would be as simple as pbatch activate --indir \"path to folder with structured json files\" --asset \"item asset type example: PSOrthoTile analytic\" The csv file need to have headers pathways asset C:\\demo\\dallas_aoi.json PSOrthoTile analytic C:\\demo\\denver_aoi.json REOrthoTile analytic C:\\demo\\sfo_aoi.json PSScene4Band analytic C:\\demo\\indy_aoi.json PSScene4Band analytic_sr The tool generates a slack readout which includes the filename , the asset and item type and the number of item and asset combinations that have been requested for activation Batch Download and Balance We run the downloader tool to batch download these assets and again you can choose to have either a folder or a csv file containing path to the json files, the item asset combination and the output location. A simple setup is thus pbatch downloader --indir Pathway to your json file --asset PSOrthoTile analytic --outdir C:\\output-folder The csv file needs to have following headers and setup pathways directory asset C:\\demo\\dallas_aoi.json C:\\demo\\t1 PSOrthoTile analytic C:\\demo\\denver_aoi.json C:\\demo\\t1 REOrthoTile analytic C:\\demo\\sfo_aoi.json C:\\demo\\t1 PSOrthoTile analytic_xml C:\\demo\\indy_aoi.json C:\\demo\\t1 REOrthoTile analytic_xml Batch downloading using folder This tool is unique for a few reasons This can using the CSV sort to identify different pathways to strctured jsons in different locations, but it can also download different assets for each input file and write to different location each time. Meaning this can be of production value to teams who have different source folders and output buckets where they would want their data to be written. The tool also prints information of Number of assets already active, number of assets that could not be activated and the total number of assets. Incase number of assets active do not match those that can be activated it will wait and show you a progressbar before trying again. This is the load balancing for each input file while making sure you don't have to estimate wait times for large requests. The tool generates a slack message posted on your channel letting you keep track of downloads. Additional Tools Two things that keep changing are space (The amount of space needed to store your data) and the time since you may want to look at different time windows . With this in mind an easy way to update you about the total space for the assets you activated I created a tool called pbatch space . A simple setup would be pbatch space --indir \"Input directory with structured json file\" --asset \"PSOrthoTile analytic\" And it can also consume a csv file where the csv file need to have headers CSV Setup to estimate size of assets in GB pathways asset C:\\demo\\dallas_aoi.json PSOrthoTile analytic C:\\demo\\denver_aoi.json REOrthoTile analytic C:\\demo\\sfo_aoi.json PSScene4Band analytic C:\\demo\\indy_aoi.json PSScene4Band analytic_sr Slack will record the last time you ran this tool because new assets may have activated since you last ran this or new assets may have become available for you to activate. This tool is useful only after you have activated your assets. To quickly change start times on structured JSON I created another tool pbatch aoiupdate . Most often all your data needs can be considered as x number of days from whatever you sent . Meaning I may want to look at 30 days of data from the end date and we don't want to recreate the structured json files. Turns out we can easily change that using a time delta function and simply rewrite the start date for our json files from which to start looking for data. Note: Your end date should be current date or later the way the date is now written is date greater than equal to 30 days from today and end date remains constant A good rule of them to be safe with this tool is to save the end date into the future so for example. The setup maybe pbatch aoiupdate --indir \"directory\" --days 30 start date \"**2017-01-01**\" end date \"**2017-12-31**\" new start date \"**2017-10-26**\" new end date \"**2017-12-31**\" You can also create a CSV setup for this as well with different days for each JSON file CSV Setup to update structured JSON. Note: the number of days is calculated from current date pathways days C:\\demo\\dallas_aoi.json 3 C:\\demo\\denver_aoi.json 5 C:\\demo\\sfo_aoi.json 14 C:\\demo\\indy_aoi.json 23 Another tool that was solely written witht he purpose of integration with Google Earth Engine is the pbatch metadata which allows you to tabulate metadata and this is for use in conjunction with Google Earth Engine atleast for my workflow. Citation and Credits You can cite the tool as Samapriya Roy. (2017, December 4). samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.1079887 Thanks to the Planet Ambassador Program Changelog v0.1.4 Now handles CSV formatting for all tools Added CSV Example setups folder for use v0.1.3 (Pre-release) Handles issues with subprocess module and long wait v0.1.2 (Pre-release) Updates to parsing CSV, optimized handling of filetypes v0.1.1 (Pre-release) Updates to Batch download handler","title":"Planet Batch Slack Pipeline"},{"location":"projects/planet_batch_slack/#planet-batch-slack-pipeline-cli","text":"Planet Labs(Full line up of Satellites) and Planet Slack Technologies Logo For writing a readme file this time I have adapted a shared piece written for the medium article. The first part which is setting up the slack account, creating an application and a slack bot has been discussed in the article here . In the past I have written tools which act as pipelines for you to process single areas of interest at the time that could be chained, the need to write something that does a bit more heavy lifting arose. This command line interface(CLI) application was created to handle groups and teams that have multiple areas of interest and multiple input and output buckets and locations to function smoothly. I have integrated this to slack so you can be on the move while this task can be on a scheduler and update you when finished.","title":"Planet Batch &amp; Slack Pipeline CLI"},{"location":"projects/planet_batch_slack/#table-of-contents","text":"Installation Getting started Batch Approach to Structured JSON Batch Activation Batch Download and Balance Additional Tools Citation and Credits Changelog","title":"Table of contents"},{"location":"projects/planet_batch_slack/#installation","text":"The next step we will setup the Planet-Batch-Slack-Pipeline-CLI and integrate our previously built slack app for notifications. To setup the prerequisites you need to install the Planet Python API Client and Slack Python API Clients. * To install the tool you can go to the GitHub page at Planet-Batch-Slack-Pipeline-CLI . As always two of my favorite operating systems are Windows and Linux, and to install on Linux git clone https://github.com/samapriya/Planet-Batch-Slack-Pipeline-CLI.git cd Planet-Batch-Slack-Pipeline-CLI sudo python setup.py install pip install -r requirements.txt for windows download the zip and after extraction go to the folder containing \"setup.py\" and open command prompt at that location and type python setup.py install pip install -r requirements.txt Now call the tool for the first time, by typing in pbatch -h . This will only work if you have python in the system path which you can test for opening up terminal or command prompt and typing python.","title":"Installation"},{"location":"projects/planet_batch_slack/#getting-started","text":"Once the requirements have been satisfied the first thing we would setup would be the OAuth Keys we created. The tools consists of a bunch of slack tools as well including capability to just use this tool to send slack messages, attachments and clean up channel as needed. Planet Batch Tools and Slack Addons Interface The two critical setup tools to make Slack ready and integrated are the smain and sbot tools where you will enter the OAuth for the application and OAuth for the bot that you generated earlier. These are then stored into your session for future use, you can call them using pbatch smain Use the \" OAuth Access Token \" generated earlier . You can find the tutorial here pbatch sbot Use \" Bot User OAuth Access Token\" generated earlier . You can find the tutorial here Once this is done your bot is now setup to message you when a task is completed. In our case these are tied into individual tools within the batch toolkit we just installed. To be clear these tools were designed based on what I thought was an effective way of looking at data, downloading them and chaining the processes together. They are still a set of individual tools to make sure that one operation is independent of the other and does not break in case of a problem. So a non-monolithic design in some sense to make sure the pieces work. We will go through each of them in the order of use pbatch planetkey is the obvious one which is your planet API key and will allow you to store this locally to a session. The aoijson tool is the same tool used in the Planet-EE CLI within the pbatch bundle allows you to bring any existing KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles to a structured geojson file, this is so that the Planet API can read the structured geojson which has additional filters such as cloud cover and range of dates. The tool can then allow you to convert the geojson file to include filters to be used with the Planet Data API. Let us say we want to convert this map.geojson to a structured aoi.json from June 1 2017 to June 30th 2017 with 15% cloud cover as our maximum . We would pass the following command pbatch.py aoijson --start 2017-06-01 --end 2017-06-30 --cloud 0.15 --inputfile GJSON --geo local path to map.geojson file --loc path where aoi.json output file will be created","title":"Getting started"},{"location":"projects/planet_batch_slack/#batch-approach-to-structured-json","text":"This tool was then rewritten and included in the application to overcome two issues 1) Automatically detect the type of input file I am passing (For now it can automatically handle geojson, kml, shapefile and wkt files). The files are then saved in an output directory with the filename_aoi.json . The tool can also read from a csv file and parse different start dates, end dates and cloud cover for different files and create structured jsons to multiple locations making an multi path export easy. The csv headers should be The csv file needs to have following headers and setup pathways start end cloud outdir C:\\demo\\dallas.geojson 2017-01-01 2017-01-02 0.15 C:\\demo C:\\demo\\denver.geojson 2017-01-01 2017-03-02 0.15 C:\\demo C:\\demo\\sfo.geojson 2017-01-01 2017-05-02 0.15 C:\\demo C:\\demo\\indy.geojson 2017-01-01 2017-09-02 0.15 C:\\demo Below is a folder based batch execution to convert multiple geojson files to structured json files","title":"Batch Approach to Structured JSON"},{"location":"projects/planet_batch_slack/#batch-activation","text":"This tool was rewritten to provide users with two options to activate their assets. They can either point the tool at a folder and select the item and asset combination or they can specify a CSV file which contains each asset and item type and path to the structured JSON file. A setup would be as simple as pbatch activate --indir \"path to folder with structured json files\" --asset \"item asset type example: PSOrthoTile analytic\" The csv file need to have headers pathways asset C:\\demo\\dallas_aoi.json PSOrthoTile analytic C:\\demo\\denver_aoi.json REOrthoTile analytic C:\\demo\\sfo_aoi.json PSScene4Band analytic C:\\demo\\indy_aoi.json PSScene4Band analytic_sr The tool generates a slack readout which includes the filename , the asset and item type and the number of item and asset combinations that have been requested for activation","title":"Batch Activation"},{"location":"projects/planet_batch_slack/#batch-download-and-balance","text":"We run the downloader tool to batch download these assets and again you can choose to have either a folder or a csv file containing path to the json files, the item asset combination and the output location. A simple setup is thus pbatch downloader --indir Pathway to your json file --asset PSOrthoTile analytic --outdir C:\\output-folder The csv file needs to have following headers and setup pathways directory asset C:\\demo\\dallas_aoi.json C:\\demo\\t1 PSOrthoTile analytic C:\\demo\\denver_aoi.json C:\\demo\\t1 REOrthoTile analytic C:\\demo\\sfo_aoi.json C:\\demo\\t1 PSOrthoTile analytic_xml C:\\demo\\indy_aoi.json C:\\demo\\t1 REOrthoTile analytic_xml Batch downloading using folder This tool is unique for a few reasons This can using the CSV sort to identify different pathways to strctured jsons in different locations, but it can also download different assets for each input file and write to different location each time. Meaning this can be of production value to teams who have different source folders and output buckets where they would want their data to be written. The tool also prints information of Number of assets already active, number of assets that could not be activated and the total number of assets. Incase number of assets active do not match those that can be activated it will wait and show you a progressbar before trying again. This is the load balancing for each input file while making sure you don't have to estimate wait times for large requests. The tool generates a slack message posted on your channel letting you keep track of downloads.","title":"Batch Download and Balance"},{"location":"projects/planet_batch_slack/#additional-tools","text":"Two things that keep changing are space (The amount of space needed to store your data) and the time since you may want to look at different time windows . With this in mind an easy way to update you about the total space for the assets you activated I created a tool called pbatch space . A simple setup would be pbatch space --indir \"Input directory with structured json file\" --asset \"PSOrthoTile analytic\" And it can also consume a csv file where the csv file need to have headers CSV Setup to estimate size of assets in GB pathways asset C:\\demo\\dallas_aoi.json PSOrthoTile analytic C:\\demo\\denver_aoi.json REOrthoTile analytic C:\\demo\\sfo_aoi.json PSScene4Band analytic C:\\demo\\indy_aoi.json PSScene4Band analytic_sr Slack will record the last time you ran this tool because new assets may have activated since you last ran this or new assets may have become available for you to activate. This tool is useful only after you have activated your assets. To quickly change start times on structured JSON I created another tool pbatch aoiupdate . Most often all your data needs can be considered as x number of days from whatever you sent . Meaning I may want to look at 30 days of data from the end date and we don't want to recreate the structured json files. Turns out we can easily change that using a time delta function and simply rewrite the start date for our json files from which to start looking for data. Note: Your end date should be current date or later the way the date is now written is date greater than equal to 30 days from today and end date remains constant A good rule of them to be safe with this tool is to save the end date into the future so for example. The setup maybe pbatch aoiupdate --indir \"directory\" --days 30 start date \"**2017-01-01**\" end date \"**2017-12-31**\" new start date \"**2017-10-26**\" new end date \"**2017-12-31**\" You can also create a CSV setup for this as well with different days for each JSON file CSV Setup to update structured JSON. Note: the number of days is calculated from current date pathways days C:\\demo\\dallas_aoi.json 3 C:\\demo\\denver_aoi.json 5 C:\\demo\\sfo_aoi.json 14 C:\\demo\\indy_aoi.json 23 Another tool that was solely written witht he purpose of integration with Google Earth Engine is the pbatch metadata which allows you to tabulate metadata and this is for use in conjunction with Google Earth Engine atleast for my workflow.","title":"Additional Tools"},{"location":"projects/planet_batch_slack/#citation-and-credits","text":"You can cite the tool as Samapriya Roy. (2017, December 4). samapriya/Planet-Batch-Slack-Pipeline-CLI: Planet-Batch-Slack-Pipeline-CLI (Version 0.1.4). Zenodo. http://doi.org/10.5281/zenodo.1079887 Thanks to the Planet Ambassador Program","title":"Citation and Credits"},{"location":"projects/planet_batch_slack/#changelog","text":"","title":"Changelog"},{"location":"projects/planet_batch_slack/#v014","text":"Now handles CSV formatting for all tools Added CSV Example setups folder for use","title":"v0.1.4"},{"location":"projects/planet_batch_slack/#v013-pre-release","text":"Handles issues with subprocess module and long wait","title":"v0.1.3 (Pre-release)"},{"location":"projects/planet_batch_slack/#v012-pre-release","text":"Updates to parsing CSV, optimized handling of filetypes","title":"v0.1.2 (Pre-release)"},{"location":"projects/planet_batch_slack/#v011-pre-release","text":"Updates to Batch download handler","title":"v0.1.1 (Pre-release)"},{"location":"projects/planet_gee_pipeline_cli/","text":"Planet GEE Pipeline CLI While moving between assets from Planet Inc and Google Earth Engine it was imperative to create a pipeline that allows for easy transitions between the two service end points and this tool is designed to act as a step by step process chain from Planet Assets to batch upload and modification within the Google Earth Engine environment. The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. tab. Table of contents Installation Getting started Batch uploader Parsing metadata Usage examples Planet Tools Planet Key AOI JSON Activate or Check Asset Check Total size of assets Download Asset Metadata Parser Earth Engine Tools EE User Create Upload a directory with images and associate properties with each image: Upload a directory with images with specific NoData value to a selected destination: Asset List Asset Size Earth Engine Asset Report Task Query Task Report Delete a collection with content: Assets Move Assets Copy Assets Access Set Collection Property Cancel all tasks Credits Installation We assume Earth Engine Python API is installed and EE authorised as desribed here . We also assume Planet Python API is installed you can install by simply running. pip install planet Further instructions can be found here This toolbox also uses some functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa sudo apt-get update sudo apt-get install gdal-bin For Windows I found this guide from UCLA To install Planet-GEE-Pipeline-CLI: git clone https://github.com/samapriya/Planet-GEE-Pipeline-CLI.git cd Planet-GEE-Pipeline-CLI pip install -r requirements.txt for linux use sudo pip install -r requirements.txt This release also contains a windows installer which bypasses the need for you to have admin permission, it does however require you to have python in the system path meaning when you open up command prompt you should be able to type python and start it within the command prompt window. Post installation using the installer you can just call ppipe using the command prompt similar to calling python. Give it a go post installation type ppipe -h Installation is an optional step; the application can be also run directly by executing ppipe.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. To install run python setup.py develop or python setup.py install In a linux distribution sudo python setup.py develop or sudo python setup.py install Getting started As usual, to print help: usage: ppipe.py [-h] { ,planetkey,aoijson,activatepl,space,downloadpl,metadata,ee_user,create,upload,lst,collsize,delete,tasks,taskquery,report,cancel,mover,copy,access,collprop,cleanout} ... Planet Pipeline with Google Earth Engine Batch Addons positional arguments: { ,planetkey,aoijson,activatepl,space,downloadpl,metadata,ee_user,create,upload,lst,collsize,delete,tasks,taskquery,report,cancel,mover,copy,access,collprop,cleanout} --------------------------------------- -----Choose from Planet Tools Below----- --------------------------------------- planetkey Enter your planet API Key aoijson Tool to convert KML, Shapefile,WKT,GeoJSON or Landsat WRS PathRow file to AreaOfInterest.JSON file with structured query for use with Planet API 1.0 activatepl Tool to query and/or activate Planet Assets space Tool to query total download size of activated assets local space left for download downloadpl Tool to download Planet Assets metadata Tool to tabulate and convert all metadata files from Planet or Digital Globe Assets ------------------------------------------- ----Choose from Earth Engine Tools Below---- ------------------------------------------- ee_user Get Earth Engine API Key Paste it back to Command line/shell to change user create Allows the user to create an asset collection or folder in Google Earth Engine upload Batch Asset Uploader. lst List assets in a folder/collection or write as text file collsize Collects collection size in Human Readable form Number of assets delete Deletes collection and all items inside. Supports Unix-like wildcards. tasks Queries currently running, enqued,failed taskquery Queries currently running, enqued,failed ingestions and uploaded assets report Create a report of all tasks and exports to a CSV file cancel Cancel all running tasks mover Moves all assets from one collection to another copy Copies all assets from one collection to another: Including copying from other users if you have read permission to their assets access Sets Permissions for Images, Collection or all assets in EE Folder Example: python ee_permissions.py --mode folder --asset users/john/doe --user jimmy@doe.com:R collprop Sets Overall Properties for Image Collection cleanout Clear folders with datasets from earlier downloaded optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: ppipe upload -h . If you didn't install ppipe, then you can run it just by going to ppipe directory and running python ppipe.py [arguments go here] Batch uploader The script creates an Image Collection from GeoTIFFs in your local directory. By default, the collection name is the same as the local directory name; with optional parameter you can provide a different name. Another optional parameter is a path to a CSV file with metadata for images, which is covered in the next section: Parsing metadata . usage: ppipe.py upload [-h] --source SOURCE --dest DEST [-m METADATA] [-mf MANIFEST] [--large] [--nodata NODATA] [-u USER] [-s SERVICE_ACCOUNT] [-k PRIVATE_KEY] [-b BUCKET] optional arguments: -h, --help show this help message and exit Required named arguments.: --source SOURCE Path to the directory with images for upload. --dest DEST Destination. Full path for upload to Google Earth Engine, e.g. users/pinkiepie/myponycollection -u USER, --user USER Google account name (gmail address). Optional named arguments: -m METADATA, --metadata METADATA Path to CSV with metadata. -mf MANIFEST, --manifest MANIFEST Manifest type to be used,for planetscope use planetscope --large (Advanced) Use multipart upload. Might help if upload of large files is failing on some systems. Might cause other issues. --nodata NODATA The value to burn into the raster as NoData (missing data) -s SERVICE_ACCOUNT, --service-account SERVICE_ACCOUNT Google Earth Engine service account. -k PRIVATE_KEY, --private-key PRIVATE_KEY Google Earth Engine private key file. -b BUCKET, --bucket BUCKET Google Cloud Storage bucket name. Parsing metadata By metadata we understand here the properties associated with each image. Thanks to these, GEE user can easily filter collection based on specified criteria. The file with metadata should be organised as follows: filename (without extension) property1 header property2 header file1 value1 value2 file2 value3 value4 Note that header can contain only letters, digits and underscores. Example: id_no class category binomial system:time_start my_file_1 GASTROPODA EN Aaadonta constricta 1478943081000 my_file_2 GASTROPODA CR Aaadonta irregularis 1478943081000 The corresponding files are my_file_1.tif and my_file_2.tif. With each of the files five properties are associated: id_no, class, category, binomial and system:time_start. The latter is time in Unix epoch format, in milliseconds, as documented in GEE glosary. The program will match the file names from the upload directory with ones provided in the CSV and pass the metadata in JSON format: { id_no: my_file_1, class: GASTROPODA, category: EN, binomial: Aaadonta constricta, system:time_start: 1478943081000} The program will report any illegal fields, it will also complain if not all of the images passed for upload have metadata associated. User can opt to ignore it, in which case some assets will have no properties. Having metadata helps in organising your asstets, but is not mandatory - you can skip it. Usage examples Usage examples have been segmented into two parts focusing on both planet tools as well as earth engine tools, earth engine tools include additional developments in CLI which allows you to recursively interact with their python API Planet Tools The Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications. Planet Key This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools usage: ppipe.py planetkey [-h] optional arguments: -h, --help show this help message and exit If using on a private machine the Key is saved as a csv file for all future runs of the tool. AOI JSON The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you. usage: ppipe.py aoijson [-h] [--start START] [--end END] [--cloud CLOUD] [--inputfile INPUTFILE] [--geo GEO] [--loc LOC] optional arguments: -h, --help show this help message and exit --start START Start date in YYYY-MM-DD? --end END End date in YYYY-MM-DD? --cloud CLOUD Maximum Cloud Cover(0-1) representing 0-100 --inputfile INPUTFILE Choose a kml/shapefile/geojson or WKT file for AOI(KML/SHP/GJSON/WKT) or WRS (6 digit RowPath Example: 023042) --geo GEO map.geojson/aoi.kml/aoi.shp/aoi.wkt file --loc LOC Location where aoi.json file is to be stored Activate or Check Asset The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier usage: ppipe.py activatepl [-h] [--aoi AOI] [--action ACTION] [--asst ASST] optional arguments: -h, --help show this help message and exit --aoi AOI Choose aoi.json file created earlier --action ACTION choose between check/activate --asst ASST Choose between planet asset types (PSOrthoTile analytic/REOrthoTile analytic/PSOrthoTile analytic_xml/REOrthoTile analytic_xml Check Total size of assets It is important to sometimes estimate the overall size of download before you can actually download activated assets. This tool allows you to estimate local storage available at any location and overall size of download in MB or GB. This tool makes use of an existing url get request to look at content size and estimate overall download size of download for the activated assets. usage: ppipe.py space [-h] [--aoi AOI] [--local LOCAL] [--asset ASSET] optional arguments: -h, --help show this help message and exit --aoi AOI Choose aoi.json file created earlier --local LOCAL local path where you are downloading assets --asset ASSET Choose between planet asset types (PSOrthoTile analytic/PSOrthoTile analytic_dn/PSOrthoTile visual/PSScene4Band analytic/PSScene4Band analytic_dn/PSScene3Band analytic/PSScene3Band analytic_dn/PSScene3Band visual/REOrthoTile analytic/REOrthoTile visual Download Asset Having metadata helps in organising your asstets, but is not mandatory - you can skip it. The downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types. usage: ppipe.py downloadpl [-h] [--aoi AOI] [--action ACTION] [--asst ASST] [--pathway PATHWAY] optional arguments: -h, --help show this help message and exit --aoi AOI Choose aoi.json file created earlier --action ACTION choose download --asst ASST Choose between planet asset types (PSOrthoTile analytic/REOrthoTile analytic/PSOrthoTile analytic_xml/REOrthoTile analytic_xml --pathway PATHWAY Folder Pathways where PlanetAssets are saved exampled ./PlanetScope ./RapidEye Metadata Parser The metadata tab is a more powerful tool and consists of metadata parsing for All PlanetScope and RapiEye Assets along with Digital Globe MultiSpectral and DigitalGlobe PanChromatic datasets. This was developed as a standalone to process xml metadata files from multiple sources and is important step is the user plans to upload these assets to Google Earth Engine. usage: ppipe.py metadata [-h] [--asset ASSET] [--mf MF] [--mfile MFILE] [--errorlog ERRORLOG] optional arguments: -h, --help show this help message and exit --asset ASSET Choose PS OrthoTile(PSO)|PS OrthoTile DN(PSO_DN)|PS OrthoTile Visual(PSO_V)|PS4Band Analytic(PS4B)|PS4Band DN(PS4B_DN)|PS3Band Analytic(PS3B)|PS3Band DN(PS3B_DN)|PS3Band Visual(PS3B_V)|RE OrthoTile (REO)|RE OrthoTile Visual(REO_V)|DigitalGlobe MultiSpectral(DGMS)|DigitalGlobe Panchromatic(DGP)? --mf MF Metadata folder? --mfile MFILE Metadata filename to be exported along with Path.csv --errorlog ERRORLOG Errorlog to be exported along with Path.csv Earth Engine Tools The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. This is also a seperate package for earth engine users to use and can be downloaded here EE User This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back Create This tool allows you to create a collection or folder in your earth engine root directory. The tool uses the system cli to achieve this and this has been included so as to reduce the need to switch between multiple tools and CLI. usage: ppipe.py create [-h] --typ TYP --path PATH optional arguments: -h, --help show this help message and exit --typ TYP Specify type: collection or folder --path PATH This is the path for the earth engine asset to be created full path is needsed eg: users/johndoe/collection Upload a directory with images to your myfolder/mycollection and associate properties with each image: ppipe upload -u johndoe@gmail.com --source path_to_directory_with_tif -m path_to_metadata.csv -mf maifest_type(ex:planetscope) --dest users/johndoe/myfolder/myponycollection The script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.You can also skip manifest for RapidEye imagery or any other source that does not require metadata field type handling. Upload a directory with images with specific NoData value to a selected destination ppipe upload -u johndoe@gmail.com --source path_to_directory_with_tif --dest users/johndoe/myfolder/myponycollection --nodata 222 In this case we need to supply full path to the destination, which is helpful when we upload to a shared folder. In the provided example we also burn value 222 into all rasters for missing data (NoData). Asset List This tool is designed to either print or output asset lists within folders or collections using earthengine ls tool functions. usage: geeadd.py lst [-h] --location LOCATION --typ TYP [--items ITEMS] [--output OUTPUT] optional arguments: -h, --help show this help message and exit Required named arguments.: --location LOCATION This it the location of your folder/collection --typ TYP Whether you want the list to be printed or output as text[print/report] Optional named arguments: --items ITEMS Number of items to list --output OUTPUT Folder location for report to be exported Asset Size This tool allows you to query the size of any Earth Engine asset[Images, Image Collections, Tables and Folders] and prints out the number of assets and total asset size in non-byte encoding meaning KB, MB, GB, TB depending on size. usage: geeadd assetsize [-h] --asset ASSET optional arguments: -h, --help show this help message and exit --asset ASSET Earth Engine Asset for which to get size properties Earth Engine Asset Report This tool recursively goes through all your assets(Includes Images, ImageCollection,Table,) and generates a report containing the following fields [Type,Asset Type, Path,Number of Assets,size(MB),unit,owner,readers,writers]. usage: geeadd.py ee_report [-h] --outfile OUTFILE optional arguments: -h, --help show this help message and exit --outfile OUTFILE This it the location of your report csv file A simple setup is the following geeadd --outfile \"C:\\johndoe\\report.csv\" Task Query This script counts all currently running and ready tasks along with failed tasks. usage: geeadd.py tasks [-h] optional arguments: -h, --help show this help message and exit geeadd.py tasks Task Report Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type usage: geeadd taskreport [-h] [--r R] optional arguments: -h, --help show this help message and exit --r R Folder Path where the reports will be saved Delete a collection with content: The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution! geeadd delete users/johndoe/test Console output: 2016-07-17 16:14:09,212 :: oauth2client.client :: INFO :: Attempting refresh to obtain initial access_token 2016-07-17 16:14:09,213 :: oauth2client.client :: INFO :: Refreshing access_token 2016-07-17 16:14:10,842 :: root :: INFO :: Attempting to delete collection test 2016-07-17 16:14:16,898 :: root :: INFO :: Collection users/johndoe/test removed Delete all directories / collections based on a Unix-like pattern geeadd delete users/johndoe/*weird[0-9]?name* Assets Move This script allows us to recursively move assets from one collection to the other. usage: geeadd.py mover [-h] [--assetpath ASSETPATH] [--finalpath FINALPATH] optional arguments: -h, --help show this help message and exit --assetpath ASSETPATH Existing path of assets --finalpath FINALPATH New path for assets geeadd.py mover --assetpath users/johndoe/myfolder/myponycollection --destination users/johndoe/myfolder/myotherponycollection Assets Copy This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections. usage: geeadd.py copy [-h] [--initial INITIAL] [--final FINAL] optional arguments: -h, --help show this help message and exit --initial INITIAL Existing path of assets --final FINAL New path for assets geeadd.py mover --initial users/johndoe/myfolder/myponycollection --final users/johndoe/myfolder/myotherponycollection Assets Access This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time. usage: geeadd access [-h] --mode MODE --asset ASSET --user USER optional arguments: -h, --help show this help message and exit --mode MODE This lets you select if you want to change permission or folder/collection/image --asset ASSET This is the path to the earth engine asset whose permission you are changing folder/collection/image --user USER This is the email address to whom you want to give read or write permission Usage: john@doe.com:R or john@doe.com:W R/W refers to read or write permission geeadd.py access --mode folder --asset folder/collection/image --user john@doe.com:R Set Collection Property This script is derived from the ee tool to set collection properties and will set overall properties for collection. usage: geeadd.py collprop [-h] [--coll COLL] [--p P] optional arguments: -h, --help show this help message and exit --coll COLL Path of Image Collection --p P system:description=Description / system:provider_url=url / sys tem:tags=tags / system:title=title Cancel all tasks This is a simpler tool, can be called directly from the earthengine cli as well earthengine cli command earthengine task cancel all usage: geeadd.py cancel [-h] optional arguments: -h, --help show this help message and exit Credits JetStream A portion of the work is suported by JetStream Grant TG-GEO160014. Also supported by Planet Labs Ambassador Program Original upload function adapted from Lukasz's asset manager tool Changelog v0.3.0 Allows for quiet authentication for use in Google Colab or non interactive environments Improved planet key entry and authentication protocols v0.2.91 Fixed issue with Surface Reflectance metadata and manifest lib Improved ingestion support for (PSScene4Band analytic_Sr)[PS4B_SR] v0.2.9 Fixed issues with generating id list Improved overall security of command calls v0.2.2 Major improvements to ingestion using manifest ingest in Google Earth Engine Contains manifest for all commonly used Planet Data item and asset combinations Added additional tool to Earth Engine Enhancement including quota check before upload to GEE v0.2.1 Fixed initialization loop issue v0.2.0 Metadata parser and Uploader Can now handle PlanetScope 4 Band Surface Reflectance Datasets General Improvements v0.1.9 Changes made to reflect updated GEE Addon tools general improvements v0.1.8 Minor fixes to parser and general improvements Planet Key is now stored in a configuration folder which is safer \"C:\\users.config\\planet\" Earth Engine now requires you to assign a field type for metadata meaning an alphanumeric column like satID cannot also have numeric values unless specified explicitly . Manifest option has been added to handle this (just use -mf \"planetscope\") Added capability to query download size and local disk capacity before downloading planet assets. Added the list function to generate list of collections or folders including reports Added the collection size tool which allows you to estimate total size or quota used from your allocated quota. ogr2ft feature is removed since Earth Engine now allows vector and table uploading.","title":"Planet-Google Earth Engine Pipeline CLI"},{"location":"projects/planet_gee_pipeline_cli/#planet-gee-pipeline-cli","text":"While moving between assets from Planet Inc and Google Earth Engine it was imperative to create a pipeline that allows for easy transitions between the two service end points and this tool is designed to act as a step by step process chain from Planet Assets to batch upload and modification within the Google Earth Engine environment. The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. tab.","title":"Planet GEE Pipeline CLI"},{"location":"projects/planet_gee_pipeline_cli/#table-of-contents","text":"Installation Getting started Batch uploader Parsing metadata Usage examples Planet Tools Planet Key AOI JSON Activate or Check Asset Check Total size of assets Download Asset Metadata Parser Earth Engine Tools EE User Create Upload a directory with images and associate properties with each image: Upload a directory with images with specific NoData value to a selected destination: Asset List Asset Size Earth Engine Asset Report Task Query Task Report Delete a collection with content: Assets Move Assets Copy Assets Access Set Collection Property Cancel all tasks Credits","title":"Table of contents"},{"location":"projects/planet_gee_pipeline_cli/#installation","text":"We assume Earth Engine Python API is installed and EE authorised as desribed here . We also assume Planet Python API is installed you can install by simply running. pip install planet Further instructions can be found here This toolbox also uses some functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa sudo apt-get update sudo apt-get install gdal-bin For Windows I found this guide from UCLA To install Planet-GEE-Pipeline-CLI: git clone https://github.com/samapriya/Planet-GEE-Pipeline-CLI.git cd Planet-GEE-Pipeline-CLI pip install -r requirements.txt for linux use sudo pip install -r requirements.txt This release also contains a windows installer which bypasses the need for you to have admin permission, it does however require you to have python in the system path meaning when you open up command prompt you should be able to type python and start it within the command prompt window. Post installation using the installer you can just call ppipe using the command prompt similar to calling python. Give it a go post installation type ppipe -h Installation is an optional step; the application can be also run directly by executing ppipe.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. To install run python setup.py develop or python setup.py install In a linux distribution sudo python setup.py develop or sudo python setup.py install","title":"Installation"},{"location":"projects/planet_gee_pipeline_cli/#getting-started","text":"As usual, to print help: usage: ppipe.py [-h] { ,planetkey,aoijson,activatepl,space,downloadpl,metadata,ee_user,create,upload,lst,collsize,delete,tasks,taskquery,report,cancel,mover,copy,access,collprop,cleanout} ... Planet Pipeline with Google Earth Engine Batch Addons positional arguments: { ,planetkey,aoijson,activatepl,space,downloadpl,metadata,ee_user,create,upload,lst,collsize,delete,tasks,taskquery,report,cancel,mover,copy,access,collprop,cleanout} --------------------------------------- -----Choose from Planet Tools Below----- --------------------------------------- planetkey Enter your planet API Key aoijson Tool to convert KML, Shapefile,WKT,GeoJSON or Landsat WRS PathRow file to AreaOfInterest.JSON file with structured query for use with Planet API 1.0 activatepl Tool to query and/or activate Planet Assets space Tool to query total download size of activated assets local space left for download downloadpl Tool to download Planet Assets metadata Tool to tabulate and convert all metadata files from Planet or Digital Globe Assets ------------------------------------------- ----Choose from Earth Engine Tools Below---- ------------------------------------------- ee_user Get Earth Engine API Key Paste it back to Command line/shell to change user create Allows the user to create an asset collection or folder in Google Earth Engine upload Batch Asset Uploader. lst List assets in a folder/collection or write as text file collsize Collects collection size in Human Readable form Number of assets delete Deletes collection and all items inside. Supports Unix-like wildcards. tasks Queries currently running, enqued,failed taskquery Queries currently running, enqued,failed ingestions and uploaded assets report Create a report of all tasks and exports to a CSV file cancel Cancel all running tasks mover Moves all assets from one collection to another copy Copies all assets from one collection to another: Including copying from other users if you have read permission to their assets access Sets Permissions for Images, Collection or all assets in EE Folder Example: python ee_permissions.py --mode folder --asset users/john/doe --user jimmy@doe.com:R collprop Sets Overall Properties for Image Collection cleanout Clear folders with datasets from earlier downloaded optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: ppipe upload -h . If you didn't install ppipe, then you can run it just by going to ppipe directory and running python ppipe.py [arguments go here]","title":"Getting started"},{"location":"projects/planet_gee_pipeline_cli/#batch-uploader","text":"The script creates an Image Collection from GeoTIFFs in your local directory. By default, the collection name is the same as the local directory name; with optional parameter you can provide a different name. Another optional parameter is a path to a CSV file with metadata for images, which is covered in the next section: Parsing metadata . usage: ppipe.py upload [-h] --source SOURCE --dest DEST [-m METADATA] [-mf MANIFEST] [--large] [--nodata NODATA] [-u USER] [-s SERVICE_ACCOUNT] [-k PRIVATE_KEY] [-b BUCKET] optional arguments: -h, --help show this help message and exit Required named arguments.: --source SOURCE Path to the directory with images for upload. --dest DEST Destination. Full path for upload to Google Earth Engine, e.g. users/pinkiepie/myponycollection -u USER, --user USER Google account name (gmail address). Optional named arguments: -m METADATA, --metadata METADATA Path to CSV with metadata. -mf MANIFEST, --manifest MANIFEST Manifest type to be used,for planetscope use planetscope --large (Advanced) Use multipart upload. Might help if upload of large files is failing on some systems. Might cause other issues. --nodata NODATA The value to burn into the raster as NoData (missing data) -s SERVICE_ACCOUNT, --service-account SERVICE_ACCOUNT Google Earth Engine service account. -k PRIVATE_KEY, --private-key PRIVATE_KEY Google Earth Engine private key file. -b BUCKET, --bucket BUCKET Google Cloud Storage bucket name.","title":"Batch uploader"},{"location":"projects/planet_gee_pipeline_cli/#parsing-metadata","text":"By metadata we understand here the properties associated with each image. Thanks to these, GEE user can easily filter collection based on specified criteria. The file with metadata should be organised as follows: filename (without extension) property1 header property2 header file1 value1 value2 file2 value3 value4 Note that header can contain only letters, digits and underscores. Example: id_no class category binomial system:time_start my_file_1 GASTROPODA EN Aaadonta constricta 1478943081000 my_file_2 GASTROPODA CR Aaadonta irregularis 1478943081000 The corresponding files are my_file_1.tif and my_file_2.tif. With each of the files five properties are associated: id_no, class, category, binomial and system:time_start. The latter is time in Unix epoch format, in milliseconds, as documented in GEE glosary. The program will match the file names from the upload directory with ones provided in the CSV and pass the metadata in JSON format: { id_no: my_file_1, class: GASTROPODA, category: EN, binomial: Aaadonta constricta, system:time_start: 1478943081000} The program will report any illegal fields, it will also complain if not all of the images passed for upload have metadata associated. User can opt to ignore it, in which case some assets will have no properties. Having metadata helps in organising your asstets, but is not mandatory - you can skip it.","title":"Parsing metadata"},{"location":"projects/planet_gee_pipeline_cli/#usage-examples","text":"Usage examples have been segmented into two parts focusing on both planet tools as well as earth engine tools, earth engine tools include additional developments in CLI which allows you to recursively interact with their python API","title":"Usage examples"},{"location":"projects/planet_gee_pipeline_cli/#planet-tools","text":"The Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.","title":"Planet Tools"},{"location":"projects/planet_gee_pipeline_cli/#planet-key","text":"This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools usage: ppipe.py planetkey [-h] optional arguments: -h, --help show this help message and exit If using on a private machine the Key is saved as a csv file for all future runs of the tool.","title":"Planet Key"},{"location":"projects/planet_gee_pipeline_cli/#aoi-json","text":"The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you. usage: ppipe.py aoijson [-h] [--start START] [--end END] [--cloud CLOUD] [--inputfile INPUTFILE] [--geo GEO] [--loc LOC] optional arguments: -h, --help show this help message and exit --start START Start date in YYYY-MM-DD? --end END End date in YYYY-MM-DD? --cloud CLOUD Maximum Cloud Cover(0-1) representing 0-100 --inputfile INPUTFILE Choose a kml/shapefile/geojson or WKT file for AOI(KML/SHP/GJSON/WKT) or WRS (6 digit RowPath Example: 023042) --geo GEO map.geojson/aoi.kml/aoi.shp/aoi.wkt file --loc LOC Location where aoi.json file is to be stored","title":"AOI JSON"},{"location":"projects/planet_gee_pipeline_cli/#activate-or-check-asset","text":"The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier usage: ppipe.py activatepl [-h] [--aoi AOI] [--action ACTION] [--asst ASST] optional arguments: -h, --help show this help message and exit --aoi AOI Choose aoi.json file created earlier --action ACTION choose between check/activate --asst ASST Choose between planet asset types (PSOrthoTile analytic/REOrthoTile analytic/PSOrthoTile analytic_xml/REOrthoTile analytic_xml","title":"Activate or Check Asset"},{"location":"projects/planet_gee_pipeline_cli/#check-total-size-of-assets","text":"It is important to sometimes estimate the overall size of download before you can actually download activated assets. This tool allows you to estimate local storage available at any location and overall size of download in MB or GB. This tool makes use of an existing url get request to look at content size and estimate overall download size of download for the activated assets. usage: ppipe.py space [-h] [--aoi AOI] [--local LOCAL] [--asset ASSET] optional arguments: -h, --help show this help message and exit --aoi AOI Choose aoi.json file created earlier --local LOCAL local path where you are downloading assets --asset ASSET Choose between planet asset types (PSOrthoTile analytic/PSOrthoTile analytic_dn/PSOrthoTile visual/PSScene4Band analytic/PSScene4Band analytic_dn/PSScene3Band analytic/PSScene3Band analytic_dn/PSScene3Band visual/REOrthoTile analytic/REOrthoTile visual","title":"Check Total size of assets"},{"location":"projects/planet_gee_pipeline_cli/#download-asset","text":"Having metadata helps in organising your asstets, but is not mandatory - you can skip it. The downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types. usage: ppipe.py downloadpl [-h] [--aoi AOI] [--action ACTION] [--asst ASST] [--pathway PATHWAY] optional arguments: -h, --help show this help message and exit --aoi AOI Choose aoi.json file created earlier --action ACTION choose download --asst ASST Choose between planet asset types (PSOrthoTile analytic/REOrthoTile analytic/PSOrthoTile analytic_xml/REOrthoTile analytic_xml --pathway PATHWAY Folder Pathways where PlanetAssets are saved exampled ./PlanetScope ./RapidEye","title":"Download Asset"},{"location":"projects/planet_gee_pipeline_cli/#metadata-parser","text":"The metadata tab is a more powerful tool and consists of metadata parsing for All PlanetScope and RapiEye Assets along with Digital Globe MultiSpectral and DigitalGlobe PanChromatic datasets. This was developed as a standalone to process xml metadata files from multiple sources and is important step is the user plans to upload these assets to Google Earth Engine. usage: ppipe.py metadata [-h] [--asset ASSET] [--mf MF] [--mfile MFILE] [--errorlog ERRORLOG] optional arguments: -h, --help show this help message and exit --asset ASSET Choose PS OrthoTile(PSO)|PS OrthoTile DN(PSO_DN)|PS OrthoTile Visual(PSO_V)|PS4Band Analytic(PS4B)|PS4Band DN(PS4B_DN)|PS3Band Analytic(PS3B)|PS3Band DN(PS3B_DN)|PS3Band Visual(PS3B_V)|RE OrthoTile (REO)|RE OrthoTile Visual(REO_V)|DigitalGlobe MultiSpectral(DGMS)|DigitalGlobe Panchromatic(DGP)? --mf MF Metadata folder? --mfile MFILE Metadata filename to be exported along with Path.csv --errorlog ERRORLOG Errorlog to be exported along with Path.csv","title":"Metadata Parser"},{"location":"projects/planet_gee_pipeline_cli/#earth-engine-tools","text":"The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. This is also a seperate package for earth engine users to use and can be downloaded here","title":"Earth Engine Tools"},{"location":"projects/planet_gee_pipeline_cli/#ee-user","text":"This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back","title":"EE User"},{"location":"projects/planet_gee_pipeline_cli/#create","text":"This tool allows you to create a collection or folder in your earth engine root directory. The tool uses the system cli to achieve this and this has been included so as to reduce the need to switch between multiple tools and CLI. usage: ppipe.py create [-h] --typ TYP --path PATH optional arguments: -h, --help show this help message and exit --typ TYP Specify type: collection or folder --path PATH This is the path for the earth engine asset to be created full path is needsed eg: users/johndoe/collection","title":"Create"},{"location":"projects/planet_gee_pipeline_cli/#upload-a-directory-with-images-to-your-myfoldermycollection-and-associate-properties-with-each-image","text":"ppipe upload -u johndoe@gmail.com --source path_to_directory_with_tif -m path_to_metadata.csv -mf maifest_type(ex:planetscope) --dest users/johndoe/myfolder/myponycollection The script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.You can also skip manifest for RapidEye imagery or any other source that does not require metadata field type handling.","title":"Upload a directory with images to your myfolder/mycollection and associate properties with each image:"},{"location":"projects/planet_gee_pipeline_cli/#upload-a-directory-with-images-with-specific-nodata-value-to-a-selected-destination","text":"ppipe upload -u johndoe@gmail.com --source path_to_directory_with_tif --dest users/johndoe/myfolder/myponycollection --nodata 222 In this case we need to supply full path to the destination, which is helpful when we upload to a shared folder. In the provided example we also burn value 222 into all rasters for missing data (NoData).","title":"Upload a directory with images with specific NoData value to a selected destination"},{"location":"projects/planet_gee_pipeline_cli/#asset-list","text":"This tool is designed to either print or output asset lists within folders or collections using earthengine ls tool functions. usage: geeadd.py lst [-h] --location LOCATION --typ TYP [--items ITEMS] [--output OUTPUT] optional arguments: -h, --help show this help message and exit Required named arguments.: --location LOCATION This it the location of your folder/collection --typ TYP Whether you want the list to be printed or output as text[print/report] Optional named arguments: --items ITEMS Number of items to list --output OUTPUT Folder location for report to be exported","title":"Asset List"},{"location":"projects/planet_gee_pipeline_cli/#asset-size","text":"This tool allows you to query the size of any Earth Engine asset[Images, Image Collections, Tables and Folders] and prints out the number of assets and total asset size in non-byte encoding meaning KB, MB, GB, TB depending on size. usage: geeadd assetsize [-h] --asset ASSET optional arguments: -h, --help show this help message and exit --asset ASSET Earth Engine Asset for which to get size properties","title":"Asset Size"},{"location":"projects/planet_gee_pipeline_cli/#earth-engine-asset-report","text":"This tool recursively goes through all your assets(Includes Images, ImageCollection,Table,) and generates a report containing the following fields [Type,Asset Type, Path,Number of Assets,size(MB),unit,owner,readers,writers]. usage: geeadd.py ee_report [-h] --outfile OUTFILE optional arguments: -h, --help show this help message and exit --outfile OUTFILE This it the location of your report csv file A simple setup is the following geeadd --outfile \"C:\\johndoe\\report.csv\"","title":"Earth Engine Asset Report"},{"location":"projects/planet_gee_pipeline_cli/#task-query","text":"This script counts all currently running and ready tasks along with failed tasks. usage: geeadd.py tasks [-h] optional arguments: -h, --help show this help message and exit geeadd.py tasks","title":"Task Query"},{"location":"projects/planet_gee_pipeline_cli/#task-report","text":"Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type usage: geeadd taskreport [-h] [--r R] optional arguments: -h, --help show this help message and exit --r R Folder Path where the reports will be saved","title":"Task Report"},{"location":"projects/planet_gee_pipeline_cli/#delete-a-collection-with-content","text":"The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution! geeadd delete users/johndoe/test Console output: 2016-07-17 16:14:09,212 :: oauth2client.client :: INFO :: Attempting refresh to obtain initial access_token 2016-07-17 16:14:09,213 :: oauth2client.client :: INFO :: Refreshing access_token 2016-07-17 16:14:10,842 :: root :: INFO :: Attempting to delete collection test 2016-07-17 16:14:16,898 :: root :: INFO :: Collection users/johndoe/test removed","title":"Delete a collection with content:"},{"location":"projects/planet_gee_pipeline_cli/#delete-all-directories-collections-based-on-a-unix-like-pattern","text":"geeadd delete users/johndoe/*weird[0-9]?name*","title":"Delete all directories / collections based on a Unix-like pattern"},{"location":"projects/planet_gee_pipeline_cli/#assets-move","text":"This script allows us to recursively move assets from one collection to the other. usage: geeadd.py mover [-h] [--assetpath ASSETPATH] [--finalpath FINALPATH] optional arguments: -h, --help show this help message and exit --assetpath ASSETPATH Existing path of assets --finalpath FINALPATH New path for assets geeadd.py mover --assetpath users/johndoe/myfolder/myponycollection --destination users/johndoe/myfolder/myotherponycollection","title":"Assets Move"},{"location":"projects/planet_gee_pipeline_cli/#assets-copy","text":"This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections. usage: geeadd.py copy [-h] [--initial INITIAL] [--final FINAL] optional arguments: -h, --help show this help message and exit --initial INITIAL Existing path of assets --final FINAL New path for assets geeadd.py mover --initial users/johndoe/myfolder/myponycollection --final users/johndoe/myfolder/myotherponycollection","title":"Assets Copy"},{"location":"projects/planet_gee_pipeline_cli/#assets-access","text":"This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time. usage: geeadd access [-h] --mode MODE --asset ASSET --user USER optional arguments: -h, --help show this help message and exit --mode MODE This lets you select if you want to change permission or folder/collection/image --asset ASSET This is the path to the earth engine asset whose permission you are changing folder/collection/image --user USER This is the email address to whom you want to give read or write permission Usage: john@doe.com:R or john@doe.com:W R/W refers to read or write permission geeadd.py access --mode folder --asset folder/collection/image --user john@doe.com:R","title":"Assets Access"},{"location":"projects/planet_gee_pipeline_cli/#set-collection-property","text":"This script is derived from the ee tool to set collection properties and will set overall properties for collection. usage: geeadd.py collprop [-h] [--coll COLL] [--p P] optional arguments: -h, --help show this help message and exit --coll COLL Path of Image Collection --p P system:description=Description / system:provider_url=url / sys tem:tags=tags / system:title=title","title":"Set Collection Property"},{"location":"projects/planet_gee_pipeline_cli/#cancel-all-tasks","text":"This is a simpler tool, can be called directly from the earthengine cli as well earthengine cli command earthengine task cancel all usage: geeadd.py cancel [-h] optional arguments: -h, --help show this help message and exit","title":"Cancel all tasks"},{"location":"projects/planet_gee_pipeline_cli/#credits","text":"JetStream A portion of the work is suported by JetStream Grant TG-GEO160014. Also supported by Planet Labs Ambassador Program Original upload function adapted from Lukasz's asset manager tool","title":"Credits"},{"location":"projects/planet_gee_pipeline_cli/#changelog","text":"","title":"Changelog"},{"location":"projects/planet_gee_pipeline_cli/#v030","text":"Allows for quiet authentication for use in Google Colab or non interactive environments Improved planet key entry and authentication protocols","title":"v0.3.0"},{"location":"projects/planet_gee_pipeline_cli/#v0291","text":"Fixed issue with Surface Reflectance metadata and manifest lib Improved ingestion support for (PSScene4Band analytic_Sr)[PS4B_SR]","title":"v0.2.91"},{"location":"projects/planet_gee_pipeline_cli/#v029","text":"Fixed issues with generating id list Improved overall security of command calls","title":"v0.2.9"},{"location":"projects/planet_gee_pipeline_cli/#v022","text":"Major improvements to ingestion using manifest ingest in Google Earth Engine Contains manifest for all commonly used Planet Data item and asset combinations Added additional tool to Earth Engine Enhancement including quota check before upload to GEE","title":"v0.2.2"},{"location":"projects/planet_gee_pipeline_cli/#v021","text":"Fixed initialization loop issue","title":"v0.2.1"},{"location":"projects/planet_gee_pipeline_cli/#v020","text":"Metadata parser and Uploader Can now handle PlanetScope 4 Band Surface Reflectance Datasets General Improvements","title":"v0.2.0"},{"location":"projects/planet_gee_pipeline_cli/#v019","text":"Changes made to reflect updated GEE Addon tools general improvements","title":"v0.1.9"},{"location":"projects/planet_gee_pipeline_cli/#v018","text":"Minor fixes to parser and general improvements Planet Key is now stored in a configuration folder which is safer \"C:\\users.config\\planet\" Earth Engine now requires you to assign a field type for metadata meaning an alphanumeric column like satID cannot also have numeric values unless specified explicitly . Manifest option has been added to handle this (just use -mf \"planetscope\") Added capability to query download size and local disk capacity before downloading planet assets. Added the list function to generate list of collections or folders including reports Added the collection size tool which allows you to estimate total size or quota used from your allocated quota. ogr2ft feature is removed since Earth Engine now allows vector and table uploading.","title":"v0.1.8"},{"location":"projects/planet_gee_pipeline_gui/","text":"Planet GEE Pipeline GUI The Planet Pipeline GUI came from the actual CLI (command line interface tools) to enable the use of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications. Table of contents Installation Usage examples Planet Tools Planet Key AOI JSON Activate or Check Asset Download Asset Metadata Parser Earth Engine Tools EE User Upload a directory with images and associate properties with each image: Upload a directory with images with specific NoData value to a selected destination: Task Query Task Query during ingestion Task Report Delete a collection with content: Assets Move Assets Copy Assets Access Set Collection Property Convert to Fusion Table Cleanup Utility Cancel all tasks Credits Installation We assume Earth Engine Python API is installed and EE authorised as desribed here . We also assume Planet Python API is installed you can install by simply running pip install planet Further instructions can be found here You require two important packages for this to run WxPython(which is what the GUI is built on) for windows(Tested in Windows 10) https://wxpython.org/download.php pip install wxPython for linux(Tested in Ubuntu 16) sudo add-apt-repository deb http://archive.ubuntu.com/ubuntu utopic main restricted universe sudo apt-get update apt-cache search python-wxgtk3.0 sudo apt-get install python-wxgtk3.0 This toolbox also uses some functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa sudo apt-get update sudo apt-get install gdal-bin For Windows I found this guide from UCLA Usage examples Usage examples have been segmented into two parts focusing on both planet tools as well as earth engine tools, earth engine tools include additional developments in CLI which allows you to recursively interact with their python API. To run the tool open a command prompt window or terminal and type python ee_ppipe.pyc Planet Tools The Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications. Planet Key This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools If using on a private machine the Key is saved as a csv file for all future runs of the tool. AOI JSON The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you. Activate or Check Asset The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier Download Asset Having metadata helps in organising your asstets, but is not mandatory - you can skip it. The downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types. Metadata Parser The metadata tab is a more powerful tool and consists of metadata parsing for PlanetScope OrthoTile RapiEye OrthoTile along with Digital Globe MultiSpectral and DigitalGlobe PanChromatic datasets. This was developed as a standalone to process xml metadata files from multiple sources and is important step is the user plans to upload these assets to Google Earth Engine. The combine Planet-GEE Pipeline tool will be made available soon for testing. Earth Engine Tools The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. This is also a seperate package for earth engine users to use and can be downloaded here EE User This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back Upload a directory with images to your myfolder/mycollection and associate properties with each image: The script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option. Task Query This script counts all currently running and ready tasks along with failed tasks. Task Query during ingestion This script can be used intermittently to look at running, failed and ready(waiting) tasks during ingestion. This script is a special case using query tasks only when uploading assets to collection by providing collection pathway to see how collection size increases. Task Report Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type Delete a collection with content: The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution! Assets Move This script allows us to recursively move assets from one collection to the other. Assets Copy This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections. Assets Access This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time. Set Collection Property This script is derived from the ee tool to set collection properties and will set overall properties for collection. Convert to Fusion Table Once validated with gdal and google fusion table it can be used to convert any geoObject to google fusion table. Forked and contributed by Gennadii here . The scripts can be used only with a specific google account Cleanup Utility This script is used to clean folders once all processes have been completed. In short this is a function to clear folder on local machine. Cancel all tasks This is a simpler tool, can be called directly from the earthengine cli as well Credits JetStream A portion of the work is suported by JetStream Grant TG-GEO160014. Also supported by Planet Labs Ambassador Program","title":"Planet-Google Earth Engine Pipeline GUI"},{"location":"projects/planet_gee_pipeline_gui/#planet-gee-pipeline-gui","text":"The Planet Pipeline GUI came from the actual CLI (command line interface tools) to enable the use of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.","title":"Planet GEE Pipeline GUI"},{"location":"projects/planet_gee_pipeline_gui/#table-of-contents","text":"Installation Usage examples Planet Tools Planet Key AOI JSON Activate or Check Asset Download Asset Metadata Parser Earth Engine Tools EE User Upload a directory with images and associate properties with each image: Upload a directory with images with specific NoData value to a selected destination: Task Query Task Query during ingestion Task Report Delete a collection with content: Assets Move Assets Copy Assets Access Set Collection Property Convert to Fusion Table Cleanup Utility Cancel all tasks Credits","title":"Table of contents"},{"location":"projects/planet_gee_pipeline_gui/#installation","text":"We assume Earth Engine Python API is installed and EE authorised as desribed here . We also assume Planet Python API is installed you can install by simply running pip install planet Further instructions can be found here You require two important packages for this to run WxPython(which is what the GUI is built on) for windows(Tested in Windows 10) https://wxpython.org/download.php pip install wxPython for linux(Tested in Ubuntu 16) sudo add-apt-repository deb http://archive.ubuntu.com/ubuntu utopic main restricted universe sudo apt-get update apt-cache search python-wxgtk3.0 sudo apt-get install python-wxgtk3.0 This toolbox also uses some functionality from GDAL For installing GDAL in Ubuntu sudo add-apt-repository ppa:ubuntugis/ppa sudo apt-get update sudo apt-get install gdal-bin For Windows I found this guide from UCLA","title":"Installation"},{"location":"projects/planet_gee_pipeline_gui/#usage-examples","text":"Usage examples have been segmented into two parts focusing on both planet tools as well as earth engine tools, earth engine tools include additional developments in CLI which allows you to recursively interact with their python API. To run the tool open a command prompt window or terminal and type python ee_ppipe.pyc","title":"Usage examples"},{"location":"projects/planet_gee_pipeline_gui/#planet-tools","text":"The Planet Toolsets consists of tools required to access control and download planet labs assets (PlanetScope and RapidEye OrthoTiles) as well as parse metadata in a tabular form which maybe required by other applications.","title":"Planet Tools"},{"location":"projects/planet_gee_pipeline_gui/#planet-key","text":"This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools If using on a private machine the Key is saved as a csv file for all future runs of the tool.","title":"Planet Key"},{"location":"projects/planet_gee_pipeline_gui/#aoi-json","text":"The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.","title":"AOI JSON"},{"location":"projects/planet_gee_pipeline_gui/#activate-or-check-asset","text":"The activatepl tab allows the users to either check or activate planet assets, in this case only PSOrthoTile and REOrthoTile are supported because I was only interested in these two asset types for my work but can be easily extended to other asset types. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier","title":"Activate or Check Asset"},{"location":"projects/planet_gee_pipeline_gui/#download-asset","text":"Having metadata helps in organising your asstets, but is not mandatory - you can skip it. The downloadpl tab allows the users to download assets. The platform can download Asset or Asset_XML which is the metadata file to desired folders.One again I was only interested in these two asset types(PSOrthoTile and REOrthoTile) for my work but can be easily extended to other asset types.","title":"Download Asset"},{"location":"projects/planet_gee_pipeline_gui/#metadata-parser","text":"The metadata tab is a more powerful tool and consists of metadata parsing for PlanetScope OrthoTile RapiEye OrthoTile along with Digital Globe MultiSpectral and DigitalGlobe PanChromatic datasets. This was developed as a standalone to process xml metadata files from multiple sources and is important step is the user plans to upload these assets to Google Earth Engine. The combine Planet-GEE Pipeline tool will be made available soon for testing.","title":"Metadata Parser"},{"location":"projects/planet_gee_pipeline_gui/#earth-engine-tools","text":"The ambition is apart from helping user with batch actions on assets along with interacting and extending capabilities of existing GEE CLI. It is developed case by case basis to include more features in the future as it becomes available or as need arises. This is also a seperate package for earth engine users to use and can be downloaded here","title":"Earth Engine Tools"},{"location":"projects/planet_gee_pipeline_gui/#ee-user","text":"This tool is designed to allow different users to change earth engine authentication credentials. The tool invokes the authentication call and copies the authentication key verification website to the clipboard which can then be pasted onto a browser and the generated key can be pasted back","title":"EE User"},{"location":"projects/planet_gee_pipeline_gui/#upload-a-directory-with-images-to-your-myfoldermycollection-and-associate-properties-with-each-image","text":"The script will prompt the user for Google account password. The program will also check that all properties in path_to_metadata.csv do not contain any illegal characters for GEE. Don't need metadata? Simply skip this option.","title":"Upload a directory with images to your myfolder/mycollection and associate properties with each image:"},{"location":"projects/planet_gee_pipeline_gui/#task-query","text":"This script counts all currently running and ready tasks along with failed tasks.","title":"Task Query"},{"location":"projects/planet_gee_pipeline_gui/#task-query-during-ingestion","text":"This script can be used intermittently to look at running, failed and ready(waiting) tasks during ingestion. This script is a special case using query tasks only when uploading assets to collection by providing collection pathway to see how collection size increases.","title":"Task Query during ingestion"},{"location":"projects/planet_gee_pipeline_gui/#task-report","text":"Sometimes it is important to generate a report based on all tasks that is running or has finished. Generated report includes taskId, data time, task status and type","title":"Task Report"},{"location":"projects/planet_gee_pipeline_gui/#delete-a-collection-with-content","text":"The delete is recursive, meaning it will delete also all children assets: images, collections and folders. Use with caution!","title":"Delete a collection with content:"},{"location":"projects/planet_gee_pipeline_gui/#assets-move","text":"This script allows us to recursively move assets from one collection to the other.","title":"Assets Move"},{"location":"projects/planet_gee_pipeline_gui/#assets-copy","text":"This script allows us to recursively copy assets from one collection to the other. If you have read acess to assets from another user this will also allow you to copy assets from their collections.","title":"Assets Copy"},{"location":"projects/planet_gee_pipeline_gui/#assets-access","text":"This tool allows you to set asset acess for either folder , collection or image recursively meaning you can add collection access properties for multiple assets at the same time.","title":"Assets Access"},{"location":"projects/planet_gee_pipeline_gui/#set-collection-property","text":"This script is derived from the ee tool to set collection properties and will set overall properties for collection.","title":"Set Collection Property"},{"location":"projects/planet_gee_pipeline_gui/#convert-to-fusion-table","text":"Once validated with gdal and google fusion table it can be used to convert any geoObject to google fusion table. Forked and contributed by Gennadii here . The scripts can be used only with a specific google account","title":"Convert to Fusion Table"},{"location":"projects/planet_gee_pipeline_gui/#cleanup-utility","text":"This script is used to clean folders once all processes have been completed. In short this is a function to clear folder on local machine.","title":"Cleanup Utility"},{"location":"projects/planet_gee_pipeline_gui/#cancel-all-tasks","text":"This is a simpler tool, can be called directly from the earthengine cli as well","title":"Cancel all tasks"},{"location":"projects/planet_gee_pipeline_gui/#credits","text":"JetStream A portion of the work is suported by JetStream Grant TG-GEO160014. Also supported by Planet Labs Ambassador Program","title":"Credits"},{"location":"projects/planet_pipeline_gui/","text":"Planet Pipeline GUI The Planet Pipeline GUI came from the actual CLI (command line interface tools) to enable the use of tools required to access control and download planet labs assets as well as parse metadata in a tabular form which maybe required by other applications. Table of contents Installation Getting started Planet Key AOI JSON Activate or Check Asset Download Size Download Asset Metadata Parser Credits Installation We assume Planet Python API is installed you can install by simply running pip install planet Further instructions can be found here To install the tool: git clone https://github.com/samapriya/Planet-Pipeline-GUI.git cd Planet-Pipeline-GUI pip install . The application can be also run directly by executing PlanetPipe_GUI.pyc script. You require two important packages for this to run WxPython(which is what the GUI is built on) for windows(Tested in Windows 10) https://wxpython.org/download.php for linux(Tested in Ubuntu 16) sudo add-apt-repository deb http://archive.ubuntu.com/ubuntu utopic main restricted universe sudo apt-get update apt-cache search python-wxgtk3.0 sudo apt-get install python-wxgtk3.0 Getting started This should be pretty simple on windows systems with python =2.7.13 you can just double click the PlanetPipe_GUI.pyc for linux user python PlanetPipe_GUI.pyc Planet Key This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools If using on a private machine the Key is saved as a csv file for all future runs of the tool. AOI JSON The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you. Activate or Check Asset The activatepl tab allows the users to either check or activate planet assets. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier Download Size The space tool allows the user to check the size of the activated assets before downloading while also estimating the available space on the local drive. The overall size and space is printed in MB and GB Download Asset The downloadpl tab allows the users to download assets that have been activated. Credits JetStream A portion of the work is suported by JetStream Grant TG-GEO160014. Also supported by Planet Labs Ambassador Program","title":"Planet Bulk Data Downloader GUI"},{"location":"projects/planet_pipeline_gui/#planet-pipeline-gui","text":"The Planet Pipeline GUI came from the actual CLI (command line interface tools) to enable the use of tools required to access control and download planet labs assets as well as parse metadata in a tabular form which maybe required by other applications.","title":"Planet Pipeline GUI"},{"location":"projects/planet_pipeline_gui/#table-of-contents","text":"Installation Getting started Planet Key AOI JSON Activate or Check Asset Download Size Download Asset Metadata Parser Credits","title":"Table of contents"},{"location":"projects/planet_pipeline_gui/#installation","text":"We assume Planet Python API is installed you can install by simply running pip install planet Further instructions can be found here To install the tool: git clone https://github.com/samapriya/Planet-Pipeline-GUI.git cd Planet-Pipeline-GUI pip install . The application can be also run directly by executing PlanetPipe_GUI.pyc script. You require two important packages for this to run WxPython(which is what the GUI is built on) for windows(Tested in Windows 10) https://wxpython.org/download.php for linux(Tested in Ubuntu 16) sudo add-apt-repository deb http://archive.ubuntu.com/ubuntu utopic main restricted universe sudo apt-get update apt-cache search python-wxgtk3.0 sudo apt-get install python-wxgtk3.0","title":"Installation"},{"location":"projects/planet_pipeline_gui/#getting-started","text":"This should be pretty simple on windows systems with python =2.7.13 you can just double click the PlanetPipe_GUI.pyc for linux user python PlanetPipe_GUI.pyc","title":"Getting started"},{"location":"projects/planet_pipeline_gui/#planet-key","text":"This tool basically asks you to input your Planet API Key using a password prompt this is then used for all subsequent tools If using on a private machine the Key is saved as a csv file for all future runs of the tool.","title":"Planet Key"},{"location":"projects/planet_pipeline_gui/#aoi-json","text":"The aoijson tab within the toolset allows you to create filters and structure your existing input file to that which can be used with Planet's API. The tool requires inputs with start and end date, along with cloud cover. You can choose from multiple input files types such as KML, Zipped Shapefile, GeoJSON, WKT or even Landsat Tiles based on PathRow numbers. The geo option asks you to select existing files which will be converted into formatted JSON file called aoi.json. If using WRS as an option just type in the 6 digit PathRow combination and it will create a json file for you.","title":"AOI JSON"},{"location":"projects/planet_pipeline_gui/#activate-or-check-asset","text":"The activatepl tab allows the users to either check or activate planet assets. This tool makes use of an existing json file sturctured for use within Planet API or the aoi.json file created earlier","title":"Activate or Check Asset"},{"location":"projects/planet_pipeline_gui/#download-size","text":"The space tool allows the user to check the size of the activated assets before downloading while also estimating the available space on the local drive. The overall size and space is printed in MB and GB","title":"Download Size"},{"location":"projects/planet_pipeline_gui/#download-asset","text":"The downloadpl tab allows the users to download assets that have been activated.","title":"Download Asset"},{"location":"projects/planet_pipeline_gui/#credits","text":"JetStream A portion of the work is suported by JetStream Grant TG-GEO160014. Also supported by Planet Labs Ambassador Program","title":"Credits"},{"location":"projects/pydrop/","text":"pydrop: Minimal Python Client for Digital Ocean Droplets This is a minimal tool designed to interact with the Digital Ocean API. This does not translate all functionalities of the API but is a template I created for some of the most common operations I could perform. New tools will be added in the future as I familiarize myself further with the API structure and use as a student. For now this tool allows you to summarize all your droplets running, including and necessarily a price summary to keep tabs on your droplets monthly and hourly rates. The tool also allows you to seach by tags, delete a drop or perfrom actions such as start, stop or shutdown a droplet. Table of contents Installation Getting started Digital Ocean Python CLI Tools Digital Ocean Key Droplets Info Droplets Delete Droplets Reset Droplets Action Installation This assumes that you have native python pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have tested this only on python 2.7.15 but can be easily modified for python 3. To install Python CLI for Digital Ocean you can install using two methods pip install pydrop or you can also try git clone https://github.com/samapriya/pydrop.git cd pydrop python setup.py install For linux use sudo. Installation is an optional step; the application can be also run directly by executing pydrop.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. If you don't want to install, browse into the pydrop folder and try python pydrop.py to get to the same result. Getting started As usual, to print help: usage: pydrop [-h] {dokey,dropinfo,dropdelete,dropaction} ... Digital Ocean API Python CLI positional arguments: {dokey,dropinfo,dropdelete,dropaction} dokey Enter your Digital Ocean API Key dropinfo Prints information about all your droplets dropdelete Permanently deletes the droplet dropaction Performs an action on your droplets optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: pydrop dropinfo -h . If you didn't install pydrop, then you can run it just by going to pydrop directory and running python pydrop.py [arguments go here] Digital Ocean Python CLI Tools The Digital Ocean Python CLI and tools setup contains minimal CLI in python to perform basic actions on droplets along with query and analyze your DO enviroment quickly. Digital Ocean Key This tool basically asks you to input your Digital Ocean API Key using a password prompt this is then used for all subsequent tools. This tool now includes an option for a quiet authentication using the API key incase it is unable to invoke an interactive environment such as in Google colaboratory. usage: pydrop dokey [-h] [--key KEY] optional arguments: -h, --help show this help message and exit Optional named arguments: --key KEY Your Digital Ocean API Key If using on a private machine the Key is saved as a csv file for all future runs of the tool. Droplets Info The droplets info tool prints summary info about all your droplets. You can choose to narrow it down further using a droplet tag so only those droplets with speific tags will be printed. Since I wanted the ability of including price summaries, I have included prices summaries. usage: pydrop dropinfo [-h] [--tag TAG] optional arguments: -h, --help show this help message and exit Optional named arguments: --tag TAG Use a tag to refine your search Droplets Delete This deletes a droplet and you can specify either the droplet name or id. Incase you don't remember the name or id, just run the tool without any arguments and it will list out all droplet id(s) and names. usage: pydrop dropdelete [-h] [--id ID] [--name NAME] optional arguments: -h, --help show this help message and exit Optional named arguments: --id ID Use an image ID to delete droplet --name NAME Use an image name to delete droplet Droplets Reset This resets the password of a droplet and you can specify either the droplet name or id. Incase you don't remember the name or id, just run the tool without any arguments and it will list out all droplet id(s) and names. usage: pydrop dropreset [-h] [--id ID] [--name NAME] optional arguments: -h, --help show this help message and exit Optional named arguments: --id ID Use an image ID to delete droplet --name NAME Use an image name to delete droplet Droplets Action The droplet action tool was designed to achieve and have more control over individual droplet actions and I included actions such as shutdown, power off, power on and rename. Just like the droplet delete tool, this tool will print the name and id of all droplets if no arguments are passed and you can then choose the one on which to perform the action. usage: pydrop dropaction [-h] [--id ID] [--name NAME] [--action ACTION] [--rename RENAME] optional arguments: -h, --help show this help message and exit Optional named arguments: --id ID Use an image ID to perform action --name NAME Use an image name to perform action --action ACTION Action type |shutdown= graceful shutdown |power_off= hard shutdown |power_on= power on |rename= rename --rename RENAME Incase you are renaming droplet you can provide new name Changelog v0.0.3 Now checks for keys and auto initializes if missing Includes password reset tool","title":"pydrop Minimal Digital Ocean CLI"},{"location":"projects/pydrop/#pydrop-minimal-python-client-for-digital-ocean-droplets","text":"This is a minimal tool designed to interact with the Digital Ocean API. This does not translate all functionalities of the API but is a template I created for some of the most common operations I could perform. New tools will be added in the future as I familiarize myself further with the API structure and use as a student. For now this tool allows you to summarize all your droplets running, including and necessarily a price summary to keep tabs on your droplets monthly and hourly rates. The tool also allows you to seach by tags, delete a drop or perfrom actions such as start, stop or shutdown a droplet.","title":"pydrop: Minimal Python Client for Digital Ocean Droplets"},{"location":"projects/pydrop/#table-of-contents","text":"Installation Getting started Digital Ocean Python CLI Tools Digital Ocean Key Droplets Info Droplets Delete Droplets Reset Droplets Action","title":"Table of contents"},{"location":"projects/pydrop/#installation","text":"This assumes that you have native python pip installed in your system, you can test this by going to the terminal (or windows command prompt) and trying python and then pip list If you get no errors and you have python 2.7.14 or higher you should be good to go. Please note that I have tested this only on python 2.7.15 but can be easily modified for python 3. To install Python CLI for Digital Ocean you can install using two methods pip install pydrop or you can also try git clone https://github.com/samapriya/pydrop.git cd pydrop python setup.py install For linux use sudo. Installation is an optional step; the application can be also run directly by executing pydrop.py script. The advantage of having it installed is being able to execute ppipe as any command line tool. I recommend installation within virtual environment. If you don't want to install, browse into the pydrop folder and try python pydrop.py to get to the same result.","title":"Installation"},{"location":"projects/pydrop/#getting-started","text":"As usual, to print help: usage: pydrop [-h] {dokey,dropinfo,dropdelete,dropaction} ... Digital Ocean API Python CLI positional arguments: {dokey,dropinfo,dropdelete,dropaction} dokey Enter your Digital Ocean API Key dropinfo Prints information about all your droplets dropdelete Permanently deletes the droplet dropaction Performs an action on your droplets optional arguments: -h, --help show this help message and exit To obtain help for a specific functionality, simply call it with help switch, e.g.: pydrop dropinfo -h . If you didn't install pydrop, then you can run it just by going to pydrop directory and running python pydrop.py [arguments go here]","title":"Getting started"},{"location":"projects/pydrop/#digital-ocean-python-cli-tools","text":"The Digital Ocean Python CLI and tools setup contains minimal CLI in python to perform basic actions on droplets along with query and analyze your DO enviroment quickly.","title":"Digital Ocean Python CLI Tools"},{"location":"projects/pydrop/#digital-ocean-key","text":"This tool basically asks you to input your Digital Ocean API Key using a password prompt this is then used for all subsequent tools. This tool now includes an option for a quiet authentication using the API key incase it is unable to invoke an interactive environment such as in Google colaboratory. usage: pydrop dokey [-h] [--key KEY] optional arguments: -h, --help show this help message and exit Optional named arguments: --key KEY Your Digital Ocean API Key If using on a private machine the Key is saved as a csv file for all future runs of the tool.","title":"Digital Ocean Key"},{"location":"projects/pydrop/#droplets-info","text":"The droplets info tool prints summary info about all your droplets. You can choose to narrow it down further using a droplet tag so only those droplets with speific tags will be printed. Since I wanted the ability of including price summaries, I have included prices summaries. usage: pydrop dropinfo [-h] [--tag TAG] optional arguments: -h, --help show this help message and exit Optional named arguments: --tag TAG Use a tag to refine your search","title":"Droplets Info"},{"location":"projects/pydrop/#droplets-delete","text":"This deletes a droplet and you can specify either the droplet name or id. Incase you don't remember the name or id, just run the tool without any arguments and it will list out all droplet id(s) and names. usage: pydrop dropdelete [-h] [--id ID] [--name NAME] optional arguments: -h, --help show this help message and exit Optional named arguments: --id ID Use an image ID to delete droplet --name NAME Use an image name to delete droplet","title":"Droplets Delete"},{"location":"projects/pydrop/#droplets-reset","text":"This resets the password of a droplet and you can specify either the droplet name or id. Incase you don't remember the name or id, just run the tool without any arguments and it will list out all droplet id(s) and names. usage: pydrop dropreset [-h] [--id ID] [--name NAME] optional arguments: -h, --help show this help message and exit Optional named arguments: --id ID Use an image ID to delete droplet --name NAME Use an image name to delete droplet","title":"Droplets Reset"},{"location":"projects/pydrop/#droplets-action","text":"The droplet action tool was designed to achieve and have more control over individual droplet actions and I included actions such as shutdown, power off, power on and rename. Just like the droplet delete tool, this tool will print the name and id of all droplets if no arguments are passed and you can then choose the one on which to perform the action. usage: pydrop dropaction [-h] [--id ID] [--name NAME] [--action ACTION] [--rename RENAME] optional arguments: -h, --help show this help message and exit Optional named arguments: --id ID Use an image ID to perform action --name NAME Use an image name to perform action --action ACTION Action type |shutdown= graceful shutdown |power_off= hard shutdown |power_on= power on |rename= rename --rename RENAME Incase you are renaming droplet you can provide new name","title":"Droplets Action"},{"location":"projects/pydrop/#changelog","text":"v0.0.3 Now checks for keys and auto initializes if missing Includes password reset tool","title":"Changelog"},{"location":"projects/slack_notifier_cli_addon/","text":"Slack Notifier-CLI Addon For those working with team collaborations and notifications slack is a quick alternative to group emails and chats. The need for a notification tool was also met with the use of the API which could be neatly tied up in clients. This CLI add-on was developed simply to function as an additional tool which can reside in an application folder and which can be called upon within a program and act as a notifier for events and updates. The tool combines simple methods in building channels and application bots and uses backends to send messages, files and to handle message history. The notifier was based on a comparison between available methods in tools such as pushbullet, pushover among a few for being able to have cross platform compatibility. Though this was designed as a means for getting process update for specific tools this CLI is essentially a plug and play into any system which can talk and pass arguments to this tool. In time additional and more refined implementation control might be included to handle specific functions. Table of contents Getting started Slack Credential Slack-Bot Credential Slack Messages Slack Message with Attachment Slack Delete All Getting started To get started you need a Slack account and you can create one here . Once you create the slack team you can further create an application and a bot within your team. This will allow you to get two API keys that you need for your tool. To access both these API tokens go to . On the features, tab should be an option called OAuth Permissions and should provide you with OAuth Access Token and Bot User OAuth Access Token . Note that the bot can only post in those channels where you have given it permission. If you add the bot to multiple channels you can specify the channel when posting messages or files. Just browse to the folder and perform python slack_addon.py -h : usage: slack_addon.py [-h] { ,smain,sbot,botupdate,botfile,slackdelete} ... Slack API Addon positional arguments: { ,smain,sbot,botupdate,botfile,slackdelete} ------------------------------------------- -----Choose from Slack Tools Below----- ------------------------------------------- smain Allows you to save your Slack Main API Token sbot Allows you to save your Slack Bot API Token botupdate Allows your bot to post messages on slack channel botfile Allows you to post a file along with comments slackdelete Allows users to delete all messages and files posted by bots optional arguments: -h, --help show this help message and exit Slack Credential This tool allows the user to save slack credential(OAuth Access Token) into users/.config/slackkey making sure that they are user specific and are not shared on a system resource or location. It uses a getpass implementation and writes the password as a CSV and the other tools first try to read this and if not present asks for your password. usage: slack_addon.py smain [-h] optional arguments: -h, --help show this help message and exit Slack-Bot Credential This tool allows the user to save slack bot credential(Bot User OAuth Access Token) into users/.config/slackkey making sure that they are user specific and are not shared on a system resource or location. It uses a getpass implementation and writes the password as a CSV and the other tools first try to read this and if not present asks for your password. usage: slack_addon.py sbot [-h] optional arguments: -h, --help show this help message and exit Slack Messages The slack messaging application is the primary tool which uses the slacker backend and allows the user to send messages as a bot to specific channel(s). The messaging service reads your Bot User OAuth Access Token and allows you to send messages to all channels where the bot has been added or has permission to post. If you do not specify the channel the bot posts to the general channel. usage: slack_addon.py botupdate [-h] [--channel CHANNEL] [--msg MSG] optional arguments: -h, --help show this help message and exit --channel CHANNEL Slack Bot update channel --msg MSG Slack Bot update message Incase you have already saved your password sending a message is as simple as python slack_addon.py --channel #general --message Hello world The application can simple be added by a call command with any process running as a system and the bot can update you about system processes, about usage, about application status and file sizes. The possibilities are endless. Slack Message with Attachment One of the most interesting applications for me was to check that not only can I sent system and application updates but I could send snapshots or process outputs such as excel files and zip files and even error logs as needed. This tool allows the slack bot to not only send a message but only to include a file with the message. The filepath points to the location of the file, the fname allows you to name the file accordingly and the cmmt option is used to add a coment of message along with the file. The channel option allows you to choose a specific channel you want to post the message and as earlier it will post to general channel. usage: slack_addon.py botfile [-h] [--channel CHANNEL] [--filepath FILEPATH] [--cmmt CMMT] [--fname FNAME] optional arguments: -h, --help show this help message and exit --channel CHANNEL Slack Bot channel --filepath FILEPATH Slack Bot file path to upload --cmmt CMMT Slack Bot file comment --fname FNAME Slack Bot filename Incase you have already saved your password a setup would be simply python slack_addon.py --channel #general --filepath /users/myfilepath.csv --cmmt Check the error logs --fname errorlog Slack Delete All One of the current non existent methods within Slack is the capability to delete all messages. This is built using a backend cli tool to delete all messages and files if needed and I integrated that in the current CLI. The current tool is primarily related to deleting all messages and files for cleaning up a channel as needed. The tool uses your main slack channel API token and uses that to delete all messages from all users but can be modified to delete messages from specific bots if needed. For now the tool deletes all messages and files in the general channel. usage: slack_addon.py slackdelete [-h] optional arguments: -h, --help show this help message and exit to use this tool simple type python slack_addon.py slackdelete","title":"Slack Notifier CLI Addon"},{"location":"projects/slack_notifier_cli_addon/#slack-notifier-cli-addon","text":"For those working with team collaborations and notifications slack is a quick alternative to group emails and chats. The need for a notification tool was also met with the use of the API which could be neatly tied up in clients. This CLI add-on was developed simply to function as an additional tool which can reside in an application folder and which can be called upon within a program and act as a notifier for events and updates. The tool combines simple methods in building channels and application bots and uses backends to send messages, files and to handle message history. The notifier was based on a comparison between available methods in tools such as pushbullet, pushover among a few for being able to have cross platform compatibility. Though this was designed as a means for getting process update for specific tools this CLI is essentially a plug and play into any system which can talk and pass arguments to this tool. In time additional and more refined implementation control might be included to handle specific functions.","title":"Slack Notifier-CLI Addon"},{"location":"projects/slack_notifier_cli_addon/#table-of-contents","text":"Getting started Slack Credential Slack-Bot Credential Slack Messages Slack Message with Attachment Slack Delete All","title":"Table of contents"},{"location":"projects/slack_notifier_cli_addon/#getting-started","text":"To get started you need a Slack account and you can create one here . Once you create the slack team you can further create an application and a bot within your team. This will allow you to get two API keys that you need for your tool. To access both these API tokens go to . On the features, tab should be an option called OAuth Permissions and should provide you with OAuth Access Token and Bot User OAuth Access Token . Note that the bot can only post in those channels where you have given it permission. If you add the bot to multiple channels you can specify the channel when posting messages or files. Just browse to the folder and perform python slack_addon.py -h : usage: slack_addon.py [-h] { ,smain,sbot,botupdate,botfile,slackdelete} ... Slack API Addon positional arguments: { ,smain,sbot,botupdate,botfile,slackdelete} ------------------------------------------- -----Choose from Slack Tools Below----- ------------------------------------------- smain Allows you to save your Slack Main API Token sbot Allows you to save your Slack Bot API Token botupdate Allows your bot to post messages on slack channel botfile Allows you to post a file along with comments slackdelete Allows users to delete all messages and files posted by bots optional arguments: -h, --help show this help message and exit","title":"Getting started"},{"location":"projects/slack_notifier_cli_addon/#slack-credential","text":"This tool allows the user to save slack credential(OAuth Access Token) into users/.config/slackkey making sure that they are user specific and are not shared on a system resource or location. It uses a getpass implementation and writes the password as a CSV and the other tools first try to read this and if not present asks for your password. usage: slack_addon.py smain [-h] optional arguments: -h, --help show this help message and exit","title":"Slack Credential"},{"location":"projects/slack_notifier_cli_addon/#slack-bot-credential","text":"This tool allows the user to save slack bot credential(Bot User OAuth Access Token) into users/.config/slackkey making sure that they are user specific and are not shared on a system resource or location. It uses a getpass implementation and writes the password as a CSV and the other tools first try to read this and if not present asks for your password. usage: slack_addon.py sbot [-h] optional arguments: -h, --help show this help message and exit","title":"Slack-Bot Credential"},{"location":"projects/slack_notifier_cli_addon/#slack-messages","text":"The slack messaging application is the primary tool which uses the slacker backend and allows the user to send messages as a bot to specific channel(s). The messaging service reads your Bot User OAuth Access Token and allows you to send messages to all channels where the bot has been added or has permission to post. If you do not specify the channel the bot posts to the general channel. usage: slack_addon.py botupdate [-h] [--channel CHANNEL] [--msg MSG] optional arguments: -h, --help show this help message and exit --channel CHANNEL Slack Bot update channel --msg MSG Slack Bot update message Incase you have already saved your password sending a message is as simple as python slack_addon.py --channel #general --message Hello world The application can simple be added by a call command with any process running as a system and the bot can update you about system processes, about usage, about application status and file sizes. The possibilities are endless.","title":"Slack Messages"},{"location":"projects/slack_notifier_cli_addon/#slack-message-with-attachment","text":"One of the most interesting applications for me was to check that not only can I sent system and application updates but I could send snapshots or process outputs such as excel files and zip files and even error logs as needed. This tool allows the slack bot to not only send a message but only to include a file with the message. The filepath points to the location of the file, the fname allows you to name the file accordingly and the cmmt option is used to add a coment of message along with the file. The channel option allows you to choose a specific channel you want to post the message and as earlier it will post to general channel. usage: slack_addon.py botfile [-h] [--channel CHANNEL] [--filepath FILEPATH] [--cmmt CMMT] [--fname FNAME] optional arguments: -h, --help show this help message and exit --channel CHANNEL Slack Bot channel --filepath FILEPATH Slack Bot file path to upload --cmmt CMMT Slack Bot file comment --fname FNAME Slack Bot filename Incase you have already saved your password a setup would be simply python slack_addon.py --channel #general --filepath /users/myfilepath.csv --cmmt Check the error logs --fname errorlog","title":"Slack Message with Attachment"},{"location":"projects/slack_notifier_cli_addon/#slack-delete-all","text":"One of the current non existent methods within Slack is the capability to delete all messages. This is built using a backend cli tool to delete all messages and files if needed and I integrated that in the current CLI. The current tool is primarily related to deleting all messages and files for cleaning up a channel as needed. The tool uses your main slack channel API token and uses that to delete all messages from all users but can be modified to delete messages from specific bots if needed. For now the tool deletes all messages and files in the general channel. usage: slack_addon.py slackdelete [-h] optional arguments: -h, --help show this help message and exit to use this tool simple type python slack_addon.py slackdelete","title":"Slack Delete All"},{"location":"projects/synthetic_models/","text":"Synthetic LandScape Generation While working on synthetic landscape generation or neutral landscape generation, it it interesting to try out different randomizing and clustering algorithms. The ones included here were part of the same experiments. The outputs are generally preceded by a header so that the ASCII files can be read easily in a GIS software such as ArcMap. The output file is ASCII type since this is commonly accepted by multiple tools. The number of rows and columns can be changed as can the no data value which for now is set to a 16 bit signed integer output. There are codes included to generate a video using landscapes as frames within the output file. Further work can include the nlmpy module Table of contents Installation Packages Clumped matrix algorithms Fragmentation Aggregation Algorithms NlmPy applications Noise Function Terrain Generation Random matrix algorithms Random matrix to video Credits Installation We assume that the user already has a copy of Matlab and has installed necessary libraries in python such as nlmpy and numpy+mlk and scipy. The toolbox can be modified to accomodate varying sizes of images to be produced and varying number of classes. The codes are mostly written in matlab and some implementation in NlmPy produced by Etherington et al are included as well. Packages Each folder has a variety of packages and tools available to run different kinds of analysis. Tools and methods include and are not limited to the following Clumped matrix algorithms This allows for clumping to occur in random matrix algorithms such that there is an element of non random allocation of class filename Description clumped_randi_land.m clumping algorithm applied to Uniformly distributed pseudorandom integers clumped_sprand_land clumping algorithm applied to Sparse uniformly distributed random matrix Fragmentation Aggregation Algorithms This one is more specific looking at different fragmentation and aggregation algorithms and allows the user to set convergence time for solution.It includes Gibbs model and Metropolis-Hastings algorithm. NlmPy applications NlmPy was created as a python library which allows the user to user different algorithms Etherington et al . The output files as ASCII to allow for easy read. Noise Function Terrain Generation These Matlab functions allows you to test noise functions to generate artificial terrains using noise function executables.Source codes for the c++ noise functions files are included in the adjoining folder and were provided kindly by Travis Archer for open use.The codes create a bmp output which are then modified to create binned responses and landscape classes as needed. Noise types include Cell Noise Diamond Square Erosion Midpoint Displacement Perline Noise Simplex Noise * Value Noise Random matrix algorithms This uses the random matrix functions within matlab to extract different landscape types. Once again they are exported as ASCII. filename Description randi_land.m Uniformly distributed pseudorandom integers sprand_land.m Sparse uniformly distributed random matrix rand_land.m Uniformly distributed random numbers rng_seed_land Random Seed Generation with Random Number Generator Random matrix to video These are experiments within matlab to use the landscapes generated as frames in a video output file from the landscapes. The video output is generally in avi format and includes slices of the landscapes produced. Credits I would like to thank Dr. Scott Robeson my mentor among others who got me interested in some topics on Landscape ecology.","title":"Synthetic Landscape Generation Models"},{"location":"projects/synthetic_models/#synthetic-landscape-generation","text":"While working on synthetic landscape generation or neutral landscape generation, it it interesting to try out different randomizing and clustering algorithms. The ones included here were part of the same experiments. The outputs are generally preceded by a header so that the ASCII files can be read easily in a GIS software such as ArcMap. The output file is ASCII type since this is commonly accepted by multiple tools. The number of rows and columns can be changed as can the no data value which for now is set to a 16 bit signed integer output. There are codes included to generate a video using landscapes as frames within the output file. Further work can include the nlmpy module","title":"Synthetic LandScape Generation"},{"location":"projects/synthetic_models/#table-of-contents","text":"Installation Packages Clumped matrix algorithms Fragmentation Aggregation Algorithms NlmPy applications Noise Function Terrain Generation Random matrix algorithms Random matrix to video Credits","title":"Table of contents"},{"location":"projects/synthetic_models/#installation","text":"We assume that the user already has a copy of Matlab and has installed necessary libraries in python such as nlmpy and numpy+mlk and scipy. The toolbox can be modified to accomodate varying sizes of images to be produced and varying number of classes. The codes are mostly written in matlab and some implementation in NlmPy produced by Etherington et al are included as well.","title":"Installation"},{"location":"projects/synthetic_models/#packages","text":"Each folder has a variety of packages and tools available to run different kinds of analysis. Tools and methods include and are not limited to the following","title":"Packages"},{"location":"projects/synthetic_models/#clumped-matrix-algorithms","text":"This allows for clumping to occur in random matrix algorithms such that there is an element of non random allocation of class filename Description clumped_randi_land.m clumping algorithm applied to Uniformly distributed pseudorandom integers clumped_sprand_land clumping algorithm applied to Sparse uniformly distributed random matrix","title":"Clumped matrix algorithms"},{"location":"projects/synthetic_models/#fragmentation-aggregation-algorithms","text":"This one is more specific looking at different fragmentation and aggregation algorithms and allows the user to set convergence time for solution.It includes Gibbs model and Metropolis-Hastings algorithm.","title":"Fragmentation Aggregation Algorithms"},{"location":"projects/synthetic_models/#nlmpy-applications","text":"NlmPy was created as a python library which allows the user to user different algorithms Etherington et al . The output files as ASCII to allow for easy read.","title":"NlmPy applications"},{"location":"projects/synthetic_models/#noise-function-terrain-generation","text":"These Matlab functions allows you to test noise functions to generate artificial terrains using noise function executables.Source codes for the c++ noise functions files are included in the adjoining folder and were provided kindly by Travis Archer for open use.The codes create a bmp output which are then modified to create binned responses and landscape classes as needed. Noise types include Cell Noise Diamond Square Erosion Midpoint Displacement Perline Noise Simplex Noise * Value Noise","title":"Noise Function Terrain Generation"},{"location":"projects/synthetic_models/#random-matrix-algorithms","text":"This uses the random matrix functions within matlab to extract different landscape types. Once again they are exported as ASCII. filename Description randi_land.m Uniformly distributed pseudorandom integers sprand_land.m Sparse uniformly distributed random matrix rand_land.m Uniformly distributed random numbers rng_seed_land Random Seed Generation with Random Number Generator","title":"Random matrix algorithms"},{"location":"projects/synthetic_models/#random-matrix-to-video","text":"These are experiments within matlab to use the landscapes generated as frames in a video output file from the landscapes. The video output is generally in avi format and includes slices of the landscapes produced.","title":"Random matrix to video"},{"location":"projects/synthetic_models/#credits","text":"I would like to thank Dr. Scott Robeson my mentor among others who got me interested in some topics on Landscape ecology.","title":"Credits"}]}